<!DOCTYPE html>

<html class="itcauecng" data-apple="true" data-hairline="true" data-theme="light" lang="zh"><head><meta charset="utf-8"/><title data-rh="true">大模型中的强化学习 - 知乎</title><meta content="width=device-width,initial-scale=1,maximum-scale=1" name="viewport"/><meta content="webkit" name="renderer"/><meta content="webkit" name="force-rendering"/><meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/><meta content='"FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"' name="google-site-verification"/><meta content='{"pageId":"10178"}' data-rh="true" name="za-config"/><meta content="强化学习 (Reinforcement Learning),大模型,多模态大模型" data-rh="true" name="keywords"/><meta content="本文主要介绍强化学习（RL）的关键背景、RL和LLM的结合，以及各种RLHF算法。背景什么是强化学习？ 强化学习（Reinforcement Learning, RL） 是机器学习的一个分支，目标是让智能体（agent）通过与环境（environmen…" data-rh="true" name="description"/><meta content="大模型中的强化学习" data-rh="true" property="og:title"/><meta content="https://zhuanlan.zhihu.com/p/693582342" data-rh="true" property="og:url"/><meta content="本文主要介绍强化学习（RL）的关键背景、RL和LLM的结合，以及各种RLHF算法。背景什么是强化学习？ 强化学习（Reinforcement Learning, RL） 是机器学习的一个分支，目标是让智能体（agent）通过与环境（environmen…" data-rh="true" property="og:description"/><meta content="" data-rh="true" property="og:image"/><meta content="article" data-rh="true" property="og:type"/><meta content="知乎专栏" data-rh="true" property="og:site_name"/><link data-rh="true" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png" rel="apple-touch-icon"/><link data-rh="true" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png" rel="apple-touch-icon" sizes="152x152"/><link data-rh="true" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.d5793cac.png" rel="apple-touch-icon" sizes="120x120"/><link data-rh="true" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7abf3393.png" rel="apple-touch-icon" sizes="76x76"/><link data-rh="true" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.362a8eac.png" rel="apple-touch-icon" sizes="60x60"/><link crossorigin="" href="https://static.zhihu.com/heifetz/favicon.ico" rel="shortcut icon" type="image/x-icon"/><link crossorigin="" href="https://static.zhihu.com/heifetz/search.xml" rel="search" title="知乎" type="application/opensearchdescription+xml"/><link href="//static.zhimg.com" rel="dns-prefetch"/><link href="//pica.zhimg.com" rel="dns-prefetch"/><link href="//picx.zhimg.com" rel="dns-prefetch"/><link href="//pic1.zhimg.com" rel="dns-prefetch"/><link href="//pic2.zhimg.com" rel="dns-prefetch"/><link href="//pic3.zhimg.com" rel="dns-prefetch"/><link href="//pic4.zhimg.com" rel="dns-prefetch"/><link href="//static.zhihu.com" rel="dns-prefetch"/><style data-emotion-css="9mvwt1">:root{--zhc-padding-horizontal:20px;--zhc-padding-vertical:16px;--zhc-notification-top:75px;--app-padding:16px;--app-header-height:52px;--app-max-width:640px;--app-width:1000px;--app-font-size:15px;}</style><script data-web-reporter-config='{"platform":"web","project":"heifetz"}' nonce="1fd6587f-56e3-430f-bfdf-8bac0df9e6f9">!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports):"function"==typeof define&&define.amd?define(["exports"],t):t((e=e||self).webReporter={})}(this,function(e){"use strict";var t={},n=!1,o=function(){var e,o,r,a,i;return n||(e=document.querySelector("script[data-web-reporter-config]"),o=e&&e.dataset.webReporterConfig||"{}",r=JSON.parse(o),a=r.platform,i=r.project,t={platform:a,project:i},n=!0),t};function r(e){return a(function(){return localStorage.getItem(e)})()}function a(e){return function(){try{return e.apply(void 0,arguments)}catch(e){}}}var i=a(function(e,t){var n={platform:"web",project:o().project,clientTimestamp:+new Date};!function(e,t,n){"1"===r("weber:logenabled")&&console.log("[web-reporter]%o",{type:e,base:t,data:n})}(e,n,t),function(e,t){var n=btoa(JSON.stringify(t));if("undefined"!=typeof Blob&&window.navigator&&window.navigator.sendBeacon){var o=new Blob([n],{type:"text/plain"});navigator.sendBeacon(e,o)}else{var r=new XMLHttpRequest;r.open("POST",e),r.withCredentials=!1,r.setRequestHeader("Content-Type","text/plain;charset=UTF-8"),r.send(n)}}(r("weber:api")||"https://apm.zhihu.com/collector/web_json",{type:e,base:n,data:t})});e.report=i,Object.defineProperty(e,"__esModule",{value:!0})});
</script><link crossorigin="" href="https://static.zhihu.com/heifetz/8911.216a26f4.5b3b2d4c94f43f16c18d.css" rel="stylesheet"/><link crossorigin="" href="https://static.zhihu.com/heifetz/column.216a26f4.0ce7e2bb57fbf078f3c9.css" rel="stylesheet"/><script nonce="1fd6587f-56e3-430f-bfdf-8bac0df9e6f9">!function(){"use strict";!function(e,n){var r=[];function t(e){return function(){r.push([e,arguments])}}n.Raven={captureException:t("captureException"),captureMessage:t("captureMessage"),captureBreadcrumb:t("captureBreadcrumb")};var a,o,c,i,s,u="undefined"!=typeof DOMError;function d(e){var n=e instanceof Error||e instanceof ErrorEvent||u&&e instanceof DOMError||e instanceof DOMException;Raven.captureException(n?e:new Error(e.message||e.reason))}n.addEventListener("unhandledrejection",d),n.addEventListener("error",d,!0),a=e.src,o=e,c=function(){r.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",d),n.removeEventListener("error",d,!0)},i=document.head||document.getElementsByTagName("head")[0],(s=document.createElement("script")).crossOrigin=o.crossOrigin,s.dataset.sentryConfig=o["data-sentry-config"],s.onload=c,s.src=a,i.appendChild(s)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"2970-4caf5ee5\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\",\"tags\":{\"app_name\":\"heifetz\"}}"},window)}();
</script></head><body class="PostIndex-body Body--isAppleDevice"><div id="root"><div class="App"><style data-emotion-css="55n9hh">.css-55n9hh{position:fixed;top:0;right:0;left:0;z-index:101;display:none;height:2px;pointer-events:none;background:#1772F6;-webkit-transform:translateX(-100%);-ms-transform:translateX(-100%);transform:translateX(-100%);}</style><div class="LoadingBar css-55n9hh"></div><style data-emotion-css="1posb5p">:root{--app-header-height:62px;}</style><style data-emotion-css="s8xum0">.css-s8xum0{position:fixed;z-index:100;width:100%;top:0;}</style><div class="css-s8xum0"><style data-emotion-css="iilrph">.css-iilrph{box-sizing:border-box;margin:0;min-width:0;position:relative;min-width:1175px;overflow:hidden;background-clip:content-box;box-shadow:0 1px 3px rgba(0,0,0,0.1);-webkit-transition-property:background-color,box-shadow;transition-property:background-color,box-shadow;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in-out;transition-timing-function:ease-in-out;--logo-width:64px;--profile-width:34px;background-color:#ffffff;}</style><header class="AppHeader css-iilrph" role="banner"><style data-emotion-css="lgijre">.css-lgijre{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;gap:16px;padding:0 20px;margin:0 auto;height:62px;max-width:1338px;}</style><div class="css-lgijre"><a aria-label="知乎" href="https://www.zhihu.com"><svg fill="#1772F6" height="30" viewbox="0 0 64 30" width="64"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><style data-emotion-css="51utkw">.css-51utkw{width:100%;height:100%;position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:16px;-webkit-transition:-webkit-transform 0.3s ease-out;-webkit-transition:transform 0.3s ease-out;transition:transform 0.3s ease-out;}</style><div class="css-51utkw"><div class="css-51utkw"><style data-emotion-css="72pd91">.css-72pd91{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;}</style><nav class="css-72pd91"><style data-emotion-css="44m30e">.css-44m30e{box-sizing:border-box;margin:0;min-width:0;color:#09408e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;color:#373a40;font-weight:normal;padding:0 12px;font-size:16px;-webkit-transition:color 0.15s ease-out;transition:color 0.15s ease-out;-webkit-text-decoration:none;text-decoration:none;}.css-44m30e:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}.css-44m30e:focus-visible{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}.css-44m30e:hover{font-weight:500;color:#191B1F;}</style><a class="css-44m30e" href="https://www.zhihu.com/follow"><style data-emotion-css="135a9x5">.css-135a9x5{position:relative;gap:2px;}</style><style data-emotion-css="c400lu">.css-c400lu{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;position:relative;gap:2px;}</style><div class="css-c400lu">关注</div></a><a class="css-44m30e" href="https://www.zhihu.com/"><div class="css-c400lu">推荐</div></a><a class="css-44m30e" href="https://www.zhihu.com/hot"><div class="css-c400lu">热榜</div></a><a class="css-44m30e" href="https://www.zhihu.com/column-square"><div class="css-c400lu">专栏</div></a><a class="css-44m30e" href="https://www.zhihu.com/ring-feeds"><div class="css-c400lu">圈子<style data-emotion-css="xd86it">.css-xd86it{box-sizing:border-box;margin:0;min-width:0;position:absolute;right:5px;top:0;font-size:11px;-webkit-transform:scale(0.8) translate(100%,-50%);-ms-transform:scale(0.8) translate(100%,-50%);transform:scale(0.8) translate(100%,-50%);border:2px solid;border-color:#ffffff;border-radius:9999px;font-weight:normal;background:linear-gradient(270deg,#1CBEFE 0%,#00CA9E 100%);color:#ffffff;padding:.5px 4px;}</style><div class="css-xd86it">New</div></div></a><style data-emotion-css="53paqb">.css-53paqb{box-sizing:border-box;margin:0;min-width:0;background-color:#c4c7ce;margin-left:10px;margin-right:10px;width:1px;height:15px;}</style><div class="css-53paqb"></div><a class="css-44m30e" href="https://www.zhihu.com/consult" target="_blank"><div class="css-c400lu">付费咨询</div></a><a class="css-44m30e" href="https://www.zhihu.com/education/learning" target="_blank"><div class="css-c400lu">知学堂</div></a></nav><style data-emotion-css="11bp94m">.css-11bp94m{-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}</style><style data-emotion-css="x84wzl">.css-x84wzl{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}</style><div class="css-x84wzl"><style data-emotion-css="v0eidh">.css-v0eidh{height:38px;}.css-v0eidh .SearchBar-input{margin-left:auto;max-width:365px;border-radius:9999px;height:100%;border:1px solid var(--MapUIFrame08B);}.css-v0eidh .SearchBar-input:focus-within{border:1px solid var(--MapText05A);}.css-v0eidh .SearchBar-askButton{margin-left:16px;height:100%;}.css-v0eidh .SearchBar-askDropdownButton{width:34px;height:34px;}.css-v0eidh .SearchBar-searchButton{height:38px;border-bottom-right-radius:9999px;border-top-right-radius:9999px;position:relative;right:-1px;}</style><div class="SearchBar css-v0eidh" data-za-module="PresetWordItem" role="search"><form class="SearchBar-tool"><style data-emotion-css="e4pgg3">.css-e4pgg3{--inputPadding:16px;}</style><div><div class="Popover"><style data-emotion-css="1telfwe">.css-1telfwe{--inputPadding:16px;border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1telfwe{--inputPadding:16px;border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1telfwe{--inputPadding:16px;border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1telfwe{--inputPadding:16px;border-color:undefined !important;-webkit-transition-property:background-color,border,color;transition-property:background-color,border,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><label class="SearchBar-input css-1telfwe Input-wrapper QZcfWkCJoarhIYxlM_sG"><input aria-activedescendant="null--1" aria-autocomplete="list" aria-expanded="false" aria-haspopup="true" autocomplete="off" class="Input i7cW1UcwT6ThdhTakqFm" id="null-toggle" maxlength="100" placeholder="" role="combobox" type="text" value=""/><button aria-label="搜索" class="Button SearchBar-searchButton FEfUrdfMIKpQDJDqkjte Button--primary epMJl0lFQuYbC7jrwr_o" type="button"><style data-emotion-css="1kuhax6">.css-1kuhax6{color:var(--MapBrand);opacity:1;-webkit-transition-property:color;transition-property:color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}.css-1kuhax6:hover{opacity:1;}</style><span style="display:inline-flex;align-items:center">​<svg class="ZDI ZDI--SearchFill16 SearchBar-searchIcon css-1kuhax6" fill="currentColor" height="20" viewbox="0 0 16 16" width="20"><path clip-rule="evenodd" d="M10.218 11.632a5.5 5.5 0 1 1 1.414-1.414l2.075 2.075a1 1 0 0 1-1.414 1.414l-2.075-2.075ZM10.6 7.1a3.5 3.5 0 1 1-7 0 3.5 3.5 0 0 1 7 0Z" fill-rule="evenodd"></path></svg></span></button></label></div></div></form><style data-emotion-css="hnfpd2">.css-hnfpd2{gap:2px;white-space:nowrap;border-radius:9999px;-webkit-transition:background .15s ease-out;transition:background .15s ease-out;background:linear-gradient(270deg,rgba(43,64,253,0.16) 0%,rgba(150,106,255,0.16) 100%);}.css-hnfpd2:hover{-webkit-filter:brightness(0.85);filter:brightness(0.85);}.css-hnfpd2:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}.css-hnfpd2:focus-visible{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}</style><style data-emotion-css="1plefh4">.css-1plefh4{box-sizing:border-box;margin:0;min-width:0;color:#09408e;padding-left:13px;padding-right:13px;color:#5a4df8;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:14px;font-weight:500;margin-left:15px;margin-right:15px;gap:2px;white-space:nowrap;border-radius:9999px;-webkit-transition:background .15s ease-out;transition:background .15s ease-out;background:linear-gradient(270deg,rgba(43,64,253,0.16) 0%,rgba(150,106,255,0.16) 100%);}.css-1plefh4:hover{-webkit-filter:brightness(0.85);filter:brightness(0.85);}.css-1plefh4:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}.css-1plefh4:focus-visible{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}</style><a class="css-1plefh4" href="https://zhida.zhihu.com/" target="_blank"><svg class="ZDI ZDI--ZhidaLogo24" fill="currentColor" height="16" viewbox="0 0 24 24" width="16"><path d="M21.37.96a.245.245 0 0 1 .46 0l.413 1.115c.082.223.259.4.483.483l1.114.412a.245.245 0 0 1 0 .46l-1.114.412a.817.817 0 0 0-.483.483L21.83 5.44a.245.245 0 0 1-.46 0l-.412-1.115a.818.818 0 0 0-.483-.483L19.36 3.43a.245.245 0 0 1 0-.46l1.115-.412c.223-.083.4-.26.483-.483L21.37.96ZM14.675 14.738a2.02 2.02 0 0 0 .05-.127l1.584-4.46a1.59 1.59 0 0 0-1.499-2.122h-3.05c-.715 0-1.363.381-1.715.981l.01-.021c.392-.887 1.223-2.766 2.625-4.07 1.648-1.531 4.286-1.28 6.035.136 2.507 2.029 3.717 5.515 2.525 8.895-1.434 4.068-5.478 6.79-9.792 6.79h-.143c1.805-1.424 2.818-4.207 3.37-6.002Z"></path><path d="M.752 7.655.167 9.317.2 9.312C1.104 8.017 2.6 8.02 5.302 8.024H5.534L1.05 20.74H7.275c.264-.027.607-.043 1.041-.043 3.86 0 5.464-3.838 6.138-5.452.063-.15.118-.282.166-.39a1.988 1.988 0 0 1-1.768 1.08H11.56c-.592 0-1.171.176-1.663.506l-.776.521a.124.124 0 0 1-.185-.143l.25-.716a.127.127 0 0 0-.12-.169 1.07 1.07 0 0 1-1.01-1.428l1.83-5.153c.017-.047.035-.093.055-.138.061-.184.122-.376.185-.572.591-1.851 1.333-4.173 3.756-5.354H6.726c-3.923 0-5.018 1.767-5.584 3.256l-.237.673c-.052.155-.102.302-.153.438Z"></path></svg>直答</a><div class="SearchBar-askContainer"><style data-emotion-css="1c2r2hg edqjxp">.css-1c2r2hg{line-height:40px;white-space:nowrap;color:var(--MapText03A);font-size:13px;font-weight:500;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:24px;padding-right:24px;gap:12px;}.css-1c2r2hg:hover{color:var(--MapText03A) !important;background:var(--MapUIFrame10A) !important;}.css-edqjxp.Popover-content--bottom.Popover-content--arrowed{margin-top:8px;}</style><div class="Popover"><style data-emotion-css="1liyluz">.css-1liyluz{background-color:undefined !important;color:undefined !important;border:undefined !important;-webkit-transition-property:background-color,color;transition-property:background-color,color;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;border-radius:9999px;width:38px;height:38px;padding:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.css-1liyluz:hover{opacity:1;}</style><button aria-expanded="false" aria-haspopup="true" aria-label="创作" class="Button SearchBar-askDropdownButton css-1liyluz FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG" id="null-toggle" type="button"><svg class="ZDI ZDI--PlusFill24" fill="currentColor" height="16" viewbox="0 0 24 24" width="16"><path clip-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" fill-rule="evenodd"></path></svg></button></div></div></div></div><style data-emotion-css="1vbrp2j">.css-1vbrp2j{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;gap:6px;}</style><div class="css-1vbrp2j"><style data-emotion-css="16zsfw9">.css-16zsfw9{box-sizing:border-box;margin:0;min-width:0;color:#09408e;position:relative;gap:2px;-webkit-transition:color 0.15s ease-out;transition:color 0.15s ease-out;-webkit-text-decoration:none;text-decoration:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;min-width:48px;color:#535861;font-size:12px;line-height:14px;}.css-16zsfw9:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}.css-16zsfw9:focus-visible{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}.css-16zsfw9:hover{color:#191B1F;}</style><button class="css-16zsfw9" href="https://www.zhihu.com/notifications"><svg class="ZDI ZDI--BellFill24" fill="currentColor" height="18" viewbox="0 0 24 24" width="18"><path clip-rule="evenodd" d="M9.723 21.271c0-.42.34-.76.76-.76h3.043a.76.76 0 0 1 0 1.521h-3.043a.76.76 0 0 1-.76-.76Z" fill-rule="evenodd"></path><path d="M11.153 3.115c0-.618.376-1.115.844-1.115.469 0 .845.499.845 1.115v.183c3.997.369 7.012 4.117 7.024 8.515V17.468h.253a.76.76 0 1 1 0 1.521H3.891a.76.76 0 0 1 0-1.521h.253V11.813c.011-4.392 3.02-8.137 7.009-8.514v-.184Z"></path></svg><style data-emotion-css="vurnku">.css-vurnku{box-sizing:border-box;margin:0;min-width:0;}</style><div class="css-vurnku">消息</div></button><button class="css-16zsfw9" href="https://www.zhihu.com/messages"><svg class="ZDI ZDI--ChatBubbleTwoFill24" fill="currentColor" height="18" viewbox="0 0 24 24" width="18"><path clip-rule="evenodd" d="M2 11c0 1.79.553 3.45 1.498 4.82L2.6 18.667a.6.6 0 0 0 .751.753l3.07-.96A8.5 8.5 0 1 0 2 11Zm11.46 9.414c-.457.16-.506.794-.034.904A6.96 6.96 0 0 0 15 21.5c1.148 0 2.422-.31 3.444-.912.357-.217.658-.378 1.043-.252l1.414.42c.357.112.679-.168.574-.546l-.47-1.57a.736.736 0 0 1 .05-.632c.602-1.108.945-2.32.945-3.498 0-1.07-.248-2.11-.7-3.046-.21-.435-.815-.25-.872.23-.47 3.954-3.211 7.394-6.968 8.72Z" fill-rule="evenodd"></path></svg><div class="css-vurnku">私信</div></button></div></div><style data-emotion-css="1ndlr1n">.css-1ndlr1n{pointer-events:none;position:absolute;top:0;left:calc(50% - (var(--logo-width) - var(--profile-width)) / 2);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);width:100%;height:100%;max-width:1032px;}</style><div class="css-1ndlr1n"><div></div></div></div><style data-emotion-css="ruapjk">.css-ruapjk{position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}</style><div class="css-ruapjk"><button class="Button AppHeader-profileEntry FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP" type="button"><style data-emotion-css="hmral">.css-hmral{border-radius:100%;}</style><style data-emotion-css="1nix5eo">.css-1nix5eo{dynamic-range-limit:standard;border-radius:2px;border-radius:100%;}</style><style data-emotion-css="d9tvwx">.css-d9tvwx{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:34px;height:34px;dynamic-range-limit:standard;border-radius:2px;border-radius:100%;}</style><img alt="点击打开undefined的主页" class="Avatar AppHeader-profileAvatar css-d9tvwx" src="images/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpeg" srcset="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpeg 2x"/></button></div></div></header></div><style data-emotion-css="1g41cri">.css-1g41cri{width:100%;height:62px;}</style><div class="css-1g41cri"></div><div><span aria-live="assertive" role="log" style="position:absolute;top:-10000px;left:-10000px"></span></div><main class="App-main" role="main"><div class="Post-content" data-zop='{"authorName":"大家好我是爱因","itemId":"693582342","title":"大模型中的强化学习","type":"article"}' data-zop-usertoken='{"userToken":""}'><style data-emotion-css="1oxku7z">html{-webkit-scroll-padding-top:calc(52px + 2em);-moz-scroll-padding-top:calc(52px + 2em);-ms-scroll-padding-top:calc(52px + 2em);scroll-padding-top:calc(52px + 2em);-webkit-scroll-padding-bottom:56px;-moz-scroll-padding-bottom:56px;-ms-scroll-padding-bottom:56px;scroll-padding-bottom:56px;}</style><style data-emotion-css="1uovyp5">.css-1uovyp5{position:relative;top:-53px;}</style><div class="css-1uovyp5"></div><style data-emotion-css="1rpg5c6">.css-1rpg5c6{position:-webkit-sticky;position:sticky;top:52px;}</style><div class="css-1rpg5c6"><style data-emotion-css="moxmo5">.css-moxmo5{position:absolute;height:1px;bottom:-2px;}</style><div class="css-moxmo5"></div></div><div class="Post-Row-Content"><div class="Post-Row-Content-left"><div class="Post-Row-Content-left-article"><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">大模型中的强化学习</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta content="大家好我是爱因" itemprop="name"/><meta content="https://picx.zhimg.com/v2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b" itemprop="image"/><meta content="https://www.zhihu.com/people/iamein" itemprop="url"/><meta itemprop="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><a class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/iamein" target="_blank"><style data-emotion-css="83b4ar">.css-83b4ar{dynamic-range-limit:standard;border-radius:50%;}</style><style data-emotion-css="mc624r">.css-mc624r{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:38px;height:38px;dynamic-range-limit:standard;border-radius:50%;}</style><img alt="大家好我是爱因" class="Avatar AuthorInfo-avatar css-mc624r" src="images/v2-c5d9627d9e98fd98830eeced564275cb_l.jpg" srcset="https://picx.zhimg.com/v2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b 2x"/></a></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><a class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/iamein" target="_blank">大家好我是爱因</a><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-0">LLM/MLLM</div></div></div></div></div></div><button class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Plus FollowButton-icon" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path clip-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" fill-rule="evenodd"></path></svg></span>关注他</button></div><style data-emotion-css="9x8rdd">.css-9x8rdd{margin:16px 0 8px;}</style><div class="css-9x8rdd"><style data-emotion-css="1jkp9i7">.css-1jkp9i7{-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion-css="q2wk8b">.css-q2wk8b{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><div class="css-q2wk8b"><style data-emotion-css="sz3kpl">.css-sz3kpl{margin:0 10px 10px 0;}</style><style data-emotion-css="1ovoyzc">.css-1ovoyzc{-webkit-flex:0 1 auto;-ms-flex:0 1 auto;flex:0 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;background:rgba(23,114,246,0.08);padding:0 8px;height:30px;border-radius:15px;-webkit-text-decoration:none;text-decoration:none;font-size:14px;line-height:18px;color:#1772F6;overflow:hidden;margin:0 10px 10px 0;}</style><style data-emotion-css="n4rzfz">.css-n4rzfz{box-sizing:border-box;margin:0;min-width:0;color:#09408e;-webkit-flex:0 1 auto;-ms-flex:0 1 auto;flex:0 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;background:rgba(23,114,246,0.08);padding:0 8px;height:30px;border-radius:15px;-webkit-text-decoration:none;text-decoration:none;font-size:14px;line-height:18px;color:#1772F6;overflow:hidden;margin:0 10px 10px 0;}</style><a class="css-n4rzfz" href="https://www.zhihu.com/column/jackstark" style="cursor:pointer"><style data-emotion-css="1tu59u4">.css-1tu59u4{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}</style><svg class="ZDI ZDI--ColumnFill24 css-1tu59u4" fill="#1772F6" height="20" viewbox="0 0 24 24" width="20"><path clip-rule="evenodd" d="M7.1 2.625A3.475 3.475 0 0 0 3.625 6.1v13.83a1.875 1.875 0 0 0 2.668 1.699l5.22-2.437c.302-.14.65-.14.95 0l5.246 2.44a1.875 1.875 0 0 0 2.666-1.7V6.1A3.475 3.475 0 0 0 16.9 2.625H7.1ZM16.375 9a.875.875 0 0 1-.875.875h-7a.875.875 0 1 1 0-1.75h7c.483 0 .875.392.875.875ZM8.5 13.875h7a.875.875 0 0 0 0-1.75h-7a.875.875 0 0 0 0 1.75Z" fill-rule="evenodd"></path></svg><style data-emotion-css="4rl99m">.css-4rl99m{box-sizing:border-box;margin:0;min-width:0;overflow:hidden;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:4px;margin-right:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-4rl99m"><style data-emotion-css="tbn57f">.css-tbn57f{box-sizing:border-box;margin:0;min-width:0;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-weight:500;margin-top:0;}</style><div class="css-tbn57f">收录于 · 机器学习小王子</div></div><style data-emotion-css="77ab9r">.css-77ab9r{margin:0;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}</style><svg class="ZDI ZDI--ArrowRight16 css-77ab9r" fill="#1772F6" height="12" viewbox="0 0 16 16" width="12"><path clip-rule="evenodd" d="M6.252 2.328a.75.75 0 0 1 1.04.213l3.334 5.05a.75.75 0 0 1 0 .826L7.292 13.46a.75.75 0 0 1-1.251-.828L9.1 8.004 6.04 3.367a.75.75 0 0 1 .212-1.039Z" fill-rule="evenodd"></path></svg></a></div></div><style data-emotion-css="z4ujak">.css-z4ujak{color:#8491a5;}</style><span class="css-z4ujak"></span></header><div class="Post-RichTextContainer"><style data-emotion-css="1od93p9">.css-1od93p9{margin-top:16px;position:relative;}</style><div class="css-1od93p9"><style data-emotion-css="376mun">.css-376mun{position:relative;display:inline;}</style><div class="css-376mun"><style data-emotion-css="1dlndns">.css-1dlndns{position:absolute;left:NaNpx;top:0;}</style><style data-emotion-css="ldd79s">.css-ldd79s{box-sizing:border-box;margin:0;min-width:0;position:absolute;left:NaNpx;top:0;}</style><div class="css-ldd79s"></div><span id="content"><style data-emotion-css="dg64xe">.css-dg64xe .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-dg64xe .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-dg64xe .FileLinkCard-info{margin-left:12px;}.css-dg64xe .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-dg64xe .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-dg64xe .FileLinkCard-source{white-space:pre;}.css-dg64xe img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-dg64xe img.content_image[data-size="normal"],.css-dg64xe img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-dg64xe img.content_image[data-size="small"],.css-dg64xe img.origin_image[data-size="small"]{width:320px;max-width:100%;}</style><style data-emotion-css="11pkt0i">.css-11pkt0i .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:90px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px;background-color:#f8f8fa;}.css-11pkt0i .LinkCard.new,.css-11pkt0i .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-11pkt0i .LinkCard.new.LinkCard--customStyle{display:block;padding:0;background:linear-gradient(180deg,#ffffff 0%,rgba(248,248,250,0.4) 100%);}.css-11pkt0i .LinkCard.new.LinkCard--customStyle .LinkCard-wrapper{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:90px;padding:12px;}.css-11pkt0i .LinkCard.new.LinkCard--customStyle .LinkCard-image{width:66px;height:66px;border-radius:6px;margin-right:12px;margin-left:0;}.css-11pkt0i .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-11pkt0i .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-11pkt0i .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-11pkt0i .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:4px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-11pkt0i .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-11pkt0i .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-11pkt0i .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-11pkt0i .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-11pkt0i .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-11pkt0i .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-11pkt0i .LinkCard.new .LinkCard-desc{display:block;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;}.css-11pkt0i .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-11pkt0i .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:6px;padding:0 4px;border-radius:3px;line-height:16px;background:#ebeced;}.css-11pkt0i .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-11pkt0i .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-11pkt0i .LinkCard.new .LinkCard-desc-wrapper{overflow:visible;white-space:nowrap;}.css-11pkt0i .LinkCard.new .LinkCard-desc-wrapper-line-clamp{display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-11pkt0i .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:66px;height:66px;margin-right:12px;margin-left:0;object-fit:cover;border-radius:6px;overflow:hidden;}.css-11pkt0i .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-11pkt0i .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-11pkt0i .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-11pkt0i .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-11pkt0i .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-11pkt0i .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-11pkt0i .LinkCard.new .LinkCard-richText .bold{font-weight:500;}.css-11pkt0i .LinkCard.new .LinkCard-richText .tag{margin-left:8px;}.css-11pkt0i .LinkCard.new .LinkCard-richText object{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-desc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-duration{font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:15px;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-collect{height:18px;width:20px;-webkit-flex:0 0 20px;-ms-flex:0 0 20px;flex:0 0 20px;margin-left:auto;color:#81858f;position:relative;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-collect-icon{position:absolute;bottom:0;right:0;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-play{height:34px;width:34px;-webkit-flex:0 0 34px;-ms-flex:0 0 34px;flex:0 0 34px;margin-left:12px;color:#1772F6;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:100%;background:rgba(23,114,246,0.08);-webkit-align-self:flex-end;-ms-flex-item-align:end;align-self:flex-end;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard-play-icon{margin-left:2px;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}.css-11pkt0i .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main .LinkCard-title{margin-bottom:6px;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:390px;border-radius:8px;overflow:hidden;border:1px solid #ebeced;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-content{display:block;padding:12px 12px 10px;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-header{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:8px;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-avatar-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-avatar{width:16px;height:16px;border-radius:100%;object-fit:cover;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-source{margin-left:6px;font-size:12px;line-height:16px;color:#9196a1;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-answer{display:-webkit-box;font-size:13px;line-height:19px;color:#535861;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-footer{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:10px 12px;background-color:#f8f8fa;cursor:pointer;line-height:19px;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-logo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;fill:#5a4df8;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-footer-text{display:block;font-size:13px;font-weight:500;line-height:19px;color:#191B1F;margin-left:6px;}.css-11pkt0i .LinkCard.new .ZhidaLinkCard-arrow{color:#5a4df8;margin-left:auto;}.css-11pkt0i .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-11pkt0i .LinkCard.old,.css-11pkt0i .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-11pkt0i .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-11pkt0i .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-11pkt0i .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-11pkt0i .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-11pkt0i .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-11pkt0i .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-11pkt0i .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}</style><style data-emotion-css="1t433vd">.css-1t433vd .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1t433vd .LinkCard.old,.css-1t433vd .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1t433vd .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-1t433vd .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1t433vd .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1t433vd .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1t433vd .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-1t433vd .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1t433vd .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1t433vd .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:90px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px;background-color:#f8f8fa;}.css-1t433vd .LinkCard.new,.css-1t433vd .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1t433vd .LinkCard.new.LinkCard--customStyle{display:block;padding:0;background:linear-gradient(180deg,#ffffff 0%,rgba(248,248,250,0.4) 100%);}.css-1t433vd .LinkCard.new.LinkCard--customStyle .LinkCard-wrapper{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:90px;padding:12px;}.css-1t433vd .LinkCard.new.LinkCard--customStyle .LinkCard-image{width:66px;height:66px;border-radius:6px;margin-right:12px;margin-left:0;}.css-1t433vd .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1t433vd .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-1t433vd .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1t433vd .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:4px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1t433vd .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1t433vd .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1t433vd .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1t433vd .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1t433vd .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1t433vd .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-1t433vd .LinkCard.new .LinkCard-desc{display:block;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;}.css-1t433vd .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1t433vd .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:6px;padding:0 4px;border-radius:3px;line-height:16px;background:#ebeced;}.css-1t433vd .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1t433vd .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1t433vd .LinkCard.new .LinkCard-desc-wrapper{overflow:visible;white-space:nowrap;}.css-1t433vd .LinkCard.new .LinkCard-desc-wrapper-line-clamp{display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1t433vd .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:66px;height:66px;margin-right:12px;margin-left:0;object-fit:cover;border-radius:6px;overflow:hidden;}.css-1t433vd .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-1t433vd .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-1t433vd .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1t433vd .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1t433vd .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-1t433vd .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-1t433vd .LinkCard.new .LinkCard-richText .bold{font-weight:500;}.css-1t433vd .LinkCard.new .LinkCard-richText .tag{margin-left:8px;}.css-1t433vd .LinkCard.new .LinkCard-richText object{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-desc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-duration{font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:15px;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-collect{height:18px;width:20px;-webkit-flex:0 0 20px;-ms-flex:0 0 20px;flex:0 0 20px;margin-left:auto;color:#81858f;position:relative;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-collect-icon{position:absolute;bottom:0;right:0;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-play{height:34px;width:34px;-webkit-flex:0 0 34px;-ms-flex:0 0 34px;flex:0 0 34px;margin-left:12px;color:#1772F6;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:100%;background:rgba(23,114,246,0.08);-webkit-align-self:flex-end;-ms-flex-item-align:end;align-self:flex-end;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard-play-icon{margin-left:2px;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}.css-1t433vd .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main .LinkCard-title{margin-bottom:6px;}.css-1t433vd .LinkCard.new .ZhidaLinkCard{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:390px;border-radius:8px;overflow:hidden;border:1px solid #ebeced;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-content{display:block;padding:12px 12px 10px;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-header{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:8px;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-avatar-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-avatar{width:16px;height:16px;border-radius:100%;object-fit:cover;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-source{margin-left:6px;font-size:12px;line-height:16px;color:#9196a1;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-answer{display:-webkit-box;font-size:13px;line-height:19px;color:#535861;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-footer{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:10px 12px;background-color:#f8f8fa;cursor:pointer;line-height:19px;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-logo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;fill:#5a4df8;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-footer-text{display:block;font-size:13px;font-weight:500;line-height:19px;color:#191B1F;margin-left:6px;}.css-1t433vd .LinkCard.new .ZhidaLinkCard-arrow{color:#5a4df8;margin-left:auto;}.css-1t433vd .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1t433vd .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1t433vd .FileLinkCard-info{margin-left:12px;}.css-1t433vd .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1t433vd .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1t433vd .FileLinkCard-source{white-space:pre;}.css-1t433vd img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-1t433vd img.content_image[data-size="normal"],.css-1t433vd img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-1t433vd img.content_image[data-size="small"],.css-1t433vd img.origin_image[data-size="small"]{width:320px;max-width:100%;}</style><style data-emotion-css="1q1c6ox animation-1yvu044">.css-1q1c6ox{word-break:break-word;line-height:1.6;}.css-1q1c6ox > [data-first-child]{margin-top:0;}.css-1q1c6ox > :last-child{margin-bottom:0;}.css-1q1c6ox h1,.css-1q1c6ox h2{clear:left;margin-top:calc((1.4em * 2) / 1.2);margin-bottom:calc(1.4em / 1.2);font-size:1.2em;line-height:1.5;font-weight:500;}.css-1q1c6ox h3,.css-1q1c6ox h4,.css-1q1c6ox h5,.css-1q1c6ox h6{clear:left;margin-top:calc((1.4em * 1.5) / 1.1);margin-bottom:calc(1.4em / 1.1);font-size:1.1em;line-height:1.5;font-weight:500;}.css-1q1c6ox u{-webkit-text-decoration:none;text-decoration:none;border-bottom:1px solid #373a40;}.css-1q1c6ox b{font-weight:500;}.css-1q1c6ox sup{font-size:0.8em;}.css-1q1c6ox sup[data-draft-type='reference']{color:#09408e;}.css-1q1c6ox a:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}html[data-focus-visible] .css-1q1c6ox a:focus{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}.css-1q1c6ox a.ztext-link,.css-1q1c6ox a.internal,.css-1q1c6ox a.external{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;color:#09408e;}.css-1q1c6ox a.ztext-link:hover,.css-1q1c6ox a.internal:hover,.css-1q1c6ox a.external:hover{color:#09408e;}.css-1q1c6ox a.ztext-link > .ellipsis::after,.css-1q1c6ox a.internal > .ellipsis::after,.css-1q1c6ox a.external > .ellipsis::after{content:'...';}.css-1q1c6ox a.ztext-link > .invisible,.css-1q1c6ox a.internal > .invisible,.css-1q1c6ox a.external > .invisible{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;}.css-1q1c6ox a.ztext-link u,.css-1q1c6ox a.internal u,.css-1q1c6ox a.external u{border:none;}.css-1q1c6ox a.member_mention{color:#09408e;}.css-1q1c6ox a.member_mention:hover{border-bottom:1px solid #09408e;}.css-1q1c6ox a.UserLink-link{color:#09408e;}.css-1q1c6ox a.UserLink-link:hover{border-bottom:1px solid #09408e;}.css-1q1c6ox p{margin:1.4em 0;}.css-1q1c6ox p.ztext-empty-paragraph{margin:calc((2.8em- (1.4em * 2 + 1.6em)) / 2) 0;}.css-1q1c6ox p.ztext-empty-paragraph + .ztext-empty-paragraph{margin:1.4em 0;}.css-1q1c6ox hr{margin:4em auto;width:240px;max-width:100%;border:none;border-top:1px solid #c4c7ce;}.css-1q1c6ox img[eeimg]{max-width:100%;vertical-align:middle;}.css-1q1c6ox img[eeimg="1"]{margin:0 3px;max-width:calc(100% - 6px);display:inline-block;}.css-1q1c6ox img[eeimg="2"]{margin:1.4em auto;display:block;}.css-1q1c6ox blockquote{margin:1.4em 0;padding-left:1em;color:#535861;border-left:3px solid #c4c7ce;}.css-1q1c6ox ol,.css-1q1c6ox ul{margin:1.4em 0;padding:0;width:100%;}.css-1q1c6ox ol ol,.css-1q1c6ox ul ol,.css-1q1c6ox ol ul,.css-1q1c6ox ul ul{margin:0;}.css-1q1c6ox ol li::before,.css-1q1c6ox ul li::before{width:1em;}.css-1q1c6ox ol > ol,.css-1q1c6ox ul > ol,.css-1q1c6ox ol > ul,.css-1q1c6ox ul > ul{padding-left:1em;box-sizing:border-box;}.css-1q1c6ox ul>li{display:table;width:100%;list-style:none;}.css-1q1c6ox ul>li::before{display:table-cell;content:'•  ';white-space:pre;}.css-1q1c6ox ol{counter-reset:ol;}.css-1q1c6ox ol > li{display:table;width:100%;list-style:none;}.css-1q1c6ox ol > li::before{display:table-cell;text-align:right;counter-increment:ol;content:counter(ol) '. ';white-space:pre;}.css-1q1c6ox ol ol{counter-reset:ol2;}.css-1q1c6ox ol ol li::before{counter-increment:ol2;content:counter(ol2) '. ';}.css-1q1c6ox ol ol ol{counter-reset:ol3;}.css-1q1c6ox ol ol ol li::before{counter-increment:ol3;content:counter(ol3) '. ';}.css-1q1c6ox ol ol ol ol{counter-reset:ol4;}.css-1q1c6ox ol ol ol ol li::before{counter-increment:ol4;content:counter(ol4) '. ';}.css-1q1c6ox figure{margin:1.4em 0;}.css-1q1c6ox figure .content_image,.css-1q1c6ox figure .origin_image{margin:0 auto;}.css-1q1c6ox figure figcaption{margin-top:calc(0.6em / 0.9);padding:0 1em;font-size:0.9em;line-height:1.5;text-align:center;color:#9196a1;}.css-1q1c6ox figure + figure{margin-top:calc(1.4em * 1.6);}.css-1q1c6ox figure[data-size='small'],.css-1q1c6ox figure:not([data-size]) > [data-size='small']{clear:both;}.css-1q1c6ox figure[data-size='left'],.css-1q1c6ox figure:not([data-size]) > [data-size='left']{float:left;margin:0 20px 20px 0;max-width:33%;}.css-1q1c6ox figure[data-size='right'],.css-1q1c6ox figure:not([data-size]) > [data-size='right']{float:right;margin:0 0 20px 20px;max-width:33%;}.css-1q1c6ox figure[data-size='collapse']{margin-bottom:0;}.css-1q1c6ox figure[data-size='collapse'] + figure{margin-top:0;}.css-1q1c6ox .content_image,.css-1q1c6ox .origin_image{display:block;max-width:100%;height:auto;margin:1.4em auto;background-color:#fff;}.css-1q1c6ox .content_image[data-size='small'],.css-1q1c6ox .origin_image[data-size='small']{max-width:40%;}.css-1q1c6ox .content_image.zh-lightbox-thumb,.css-1q1c6ox .origin_image.zh-lightbox-thumb{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}.css-1q1c6ox code{margin:0 2px;padding:3px 4px;border-radius:3px;font-family:Menlo,Monaco,Consolas,'Andale Mono','lucida console','Courier New',monospace;font-size:0.9em;background-color:#f8f8fa;}.css-1q1c6ox pre{margin:1.4em 0;padding:calc(0.8em / 0.9);font-size:0.9em;word-break:initial;word-wrap:initial;white-space:pre;overflow:auto;-webkit-overflow-scrolling:touch;background:#f8f8fa;border-radius:4px;}.css-1q1c6ox pre code{margin:0;padding:0;font-size:inherit;border-radius:0;background-color:inherit;}.css-1q1c6ox li pre{white-space:pre-wrap;}.css-1q1c6ox table[data-draft-type='table']{border-collapse:collapse;font-size:15px;margin:1.4em auto;max-width:100%;table-layout:fixed;text-align:left;width:100%;}.css-1q1c6ox table[data-draft-type='table'][data-size='small']{min-width:260px;width:40%;}.css-1q1c6ox table[data-draft-type='table'][data-row-style='striped'] tr:nth-of-type(2n + 1){background:#f8f8fa;}.css-1q1c6ox table[data-draft-type='table'] td,.css-1q1c6ox table[data-draft-type='table'] th{border:1px solid #c4c7ce;line-height:24px;height:24px;padding:3px 12px;}.css-1q1c6ox table[data-draft-type='table'] th{background:#ebeced;color:#191B1F;font-weight:500;}.css-1q1c6ox .video-box,.css-1q1c6ox .link-box{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin:1.4em 0;overflow:auto;white-space:normal;cursor:pointer;border:solid 1px #ebeced;border-radius:4px;}.css-1q1c6ox .lazy[data-lazy-status]{background-color:#f8f8fa;}.css-1q1c6ox .lazy[data-lazy-status="ok"]{background-color:#fff;-webkit-animation:animation-1yvu044 0.5s ease-in;animation:animation-1yvu044 0.5s ease-in;}.css-1q1c6ox .highlight{margin:1em 0;}.css-1q1c6ox .highlight pre{margin:0;}.css-1q1c6ox .highlight .hll{background-color:#f8f8fa;}.css-1q1c6ox .highlight .c{font-style:italic;color:#9196a1;}.css-1q1c6ox .highlight .err{color:#D95350;}.css-1q1c6ox .highlight .k{font-weight:600;}.css-1q1c6ox .highlight .o{font-weight:600;}.css-1q1c6ox .highlight .cm{font-style:italic;color:#9196a1;}.css-1q1c6ox .highlight .cp{font-weight:600;color:#9196a1;}.css-1q1c6ox .highlight .c1{font-style:italic;color:#9196a1;}.css-1q1c6ox .highlight .cs{font-style:italic;font-weight:600;color:#9196a1;}.css-1q1c6ox .highlight .gd{color:#F05159;}.css-1q1c6ox .highlight .ge{font-style:italic;}.css-1q1c6ox .highlight .gr{color:#D95350;}.css-1q1c6ox .highlight .gh{color:#9196a1;}.css-1q1c6ox .highlight .gi{color:#12b370;}.css-1q1c6ox .highlight .go{color:#81858f;}.css-1q1c6ox .highlight .gp{color:#535861;}.css-1q1c6ox .highlight .gs{font-weight:600;}.css-1q1c6ox .highlight .gu{color:#9196a1;}.css-1q1c6ox .highlight .gt{color:#D95350;}.css-1q1c6ox .highlight .kc{font-weight:600;}.css-1q1c6ox .highlight .kd{font-weight:600;}.css-1q1c6ox .highlight .kn{font-weight:600;}.css-1q1c6ox .highlight .kp{font-weight:600;}.css-1q1c6ox .highlight .kr{font-weight:600;}.css-1q1c6ox .highlight .kt{font-weight:600;color:#09408e;}.css-1q1c6ox .highlight .m{color:#1772F6;}.css-1q1c6ox .highlight .s{color:#D95350;}.css-1q1c6ox .highlight .na{color:#1772F6;}.css-1q1c6ox .highlight .nb{color:#1772F6;}.css-1q1c6ox .highlight .nc{font-weight:600;color:#09408e;}.css-1q1c6ox .highlight .no{color:#1772F6;}.css-1q1c6ox .highlight .ni{color:#6A5FF3;}.css-1q1c6ox .highlight .ne{font-weight:600;color:#D95350;}.css-1q1c6ox .highlight .nf{font-weight:600;color:#D95350;}.css-1q1c6ox .highlight .nn{color:#535861;}.css-1q1c6ox .highlight .nt{color:#09408e;}.css-1q1c6ox .highlight .nv{color:#1772F6;}.css-1q1c6ox .highlight .ow{font-weight:600;}.css-1q1c6ox .highlight .w{color:#adb0b7;}.css-1q1c6ox .highlight .mf{color:#1772F6;}.css-1q1c6ox .highlight .mh{color:#1772F6;}.css-1q1c6ox .highlight .mi{color:#1772F6;}.css-1q1c6ox .highlight .mo{color:#1772F6;}.css-1q1c6ox .highlight .sb{color:#D95350;}.css-1q1c6ox .highlight .sc{color:#D95350;}.css-1q1c6ox .highlight .sd{color:#D95350;}.css-1q1c6ox .highlight .s2{color:#D95350;}.css-1q1c6ox .highlight .se{color:#D95350;}.css-1q1c6ox .highlight .sh{color:#D95350;}.css-1q1c6ox .highlight .si{color:#D95350;}.css-1q1c6ox .highlight .sx{color:#D95350;}.css-1q1c6ox .highlight .sr{color:#A5542F;}.css-1q1c6ox .highlight .s1{color:#D95350;}.css-1q1c6ox .highlight .ss{color:#D95350;}.css-1q1c6ox .highlight .bp{color:#9196a1;}.css-1q1c6ox .highlight .vc{color:#1772F6;}.css-1q1c6ox .highlight .vg{color:#1772F6;}.css-1q1c6ox .highlight .vi{color:#1772F6;}.css-1q1c6ox .highlight .il{color:#1772F6;}.css-1q1c6ox .highlight::-webkit-scrollbar{width:6px;height:6px;}.css-1q1c6ox .highlight::-webkit-scrollbar-thumb:horizontal{background-color:rgba(25,27,31,0.5);border-radius:6px;}.css-1q1c6ox .highlight::-webkit-scrollbar-thumb:horizontal:hover{background-color:rgba(25,27,31,0.6);}.css-1q1c6ox .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1q1c6ox .LinkCard.old,.css-1q1c6ox .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1q1c6ox .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1q1c6ox .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1q1c6ox .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:90px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px;background-color:#f8f8fa;}.css-1q1c6ox .LinkCard.new,.css-1q1c6ox .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1q1c6ox .LinkCard.new.LinkCard--customStyle{display:block;padding:0;background:linear-gradient(180deg,#ffffff 0%,rgba(248,248,250,0.4) 100%);}.css-1q1c6ox .LinkCard.new.LinkCard--customStyle .LinkCard-wrapper{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:90px;padding:12px;}.css-1q1c6ox .LinkCard.new.LinkCard--customStyle .LinkCard-image{width:66px;height:66px;border-radius:6px;margin-right:12px;margin-left:0;}.css-1q1c6ox .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1q1c6ox .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-1q1c6ox .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1q1c6ox .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:4px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1q1c6ox .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1q1c6ox .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1q1c6ox .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1q1c6ox .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1q1c6ox .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1q1c6ox .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-1q1c6ox .LinkCard.new .LinkCard-desc{display:block;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;}.css-1q1c6ox .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1q1c6ox .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:6px;padding:0 4px;border-radius:3px;line-height:16px;background:#ebeced;}.css-1q1c6ox .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1q1c6ox .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1q1c6ox .LinkCard.new .LinkCard-desc-wrapper{overflow:visible;white-space:nowrap;}.css-1q1c6ox .LinkCard.new .LinkCard-desc-wrapper-line-clamp{display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1q1c6ox .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:66px;height:66px;margin-right:12px;margin-left:0;object-fit:cover;border-radius:6px;overflow:hidden;}.css-1q1c6ox .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-1q1c6ox .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-1q1c6ox .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1q1c6ox .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1q1c6ox .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-1q1c6ox .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-1q1c6ox .LinkCard.new .LinkCard-richText .bold{font-weight:500;}.css-1q1c6ox .LinkCard.new .LinkCard-richText .tag{margin-left:8px;}.css-1q1c6ox .LinkCard.new .LinkCard-richText object{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-desc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-duration{font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:15px;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-collect{height:18px;width:20px;-webkit-flex:0 0 20px;-ms-flex:0 0 20px;flex:0 0 20px;margin-left:auto;color:#81858f;position:relative;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-collect-icon{position:absolute;bottom:0;right:0;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-play{height:34px;width:34px;-webkit-flex:0 0 34px;-ms-flex:0 0 34px;flex:0 0 34px;margin-left:12px;color:#1772F6;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:100%;background:rgba(23,114,246,0.08);-webkit-align-self:flex-end;-ms-flex-item-align:end;align-self:flex-end;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard-play-icon{margin-left:2px;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}.css-1q1c6ox .LinkCard.new .PodcastEpisodeLinkCard .LinkCard-contents .LinkCard-contents-main .LinkCard-title{margin-bottom:6px;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:390px;border-radius:8px;overflow:hidden;border:1px solid #ebeced;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-content{display:block;padding:12px 12px 10px;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-header{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:8px;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-avatar-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-avatar{width:16px;height:16px;border-radius:100%;object-fit:cover;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-source{margin-left:6px;font-size:12px;line-height:16px;color:#9196a1;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-answer{display:-webkit-box;font-size:13px;line-height:19px;color:#535861;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-footer{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:10px 12px;background-color:#f8f8fa;cursor:pointer;line-height:19px;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-logo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;fill:#5a4df8;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-footer-text{display:block;font-size:13px;font-weight:500;line-height:19px;color:#191B1F;margin-left:6px;}.css-1q1c6ox .LinkCard.new .ZhidaLinkCard-arrow{color:#5a4df8;margin-left:auto;}.css-1q1c6ox .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1q1c6ox .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1q1c6ox .FileLinkCard-info{margin-left:12px;}.css-1q1c6ox .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1q1c6ox .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1q1c6ox .FileLinkCard-source{white-space:pre;}.css-1q1c6ox img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-1q1c6ox img.content_image[data-size="normal"],.css-1q1c6ox img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-1q1c6ox img.content_image[data-size="small"],.css-1q1c6ox img.origin_image[data-size="small"]{width:320px;max-width:100%;}@-webkit-keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}@keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}</style><div class="RichText ztext Post-RichText css-1q1c6ox" options="[object Object]"><p data-first-child="" data-pid="8SU2-G0A"><b>本文主要介绍强化学习（RL）的关键背景、RL和LLM的结合，以及各种RLHF算法。</b></p><h2 data-into-catalog-status="" id="h_693582342_0"><b>背景</b></h2><h3 data-into-catalog-status="" id="h_693582342_1"><b>什么是强化学习？</b></h3><p data-pid="7ZTDwKdX"><b>强化学习（Reinforcement Learning, RL）</b> 是机器学习的一个分支，目标是让<b>智能体（agent）</b>通过与<b>环境（environment）</b>的交互来学习最优的<b>行为策略（policy）</b>，从而最大化某个<b>累积回报（cumulative reward）</b>。</p><p data-pid="bJLuzXla">其<b>核心思想是通过试错和反馈的机制</b>，找到在每个情境下的最优决策。</p><p data-pid="ra2iblgq">强化学习的<b>优化目标</b>是<b>通过选择策略来最大化累积奖励</b>。 具体来说，智能体的目标是寻找一个最优策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi^*"> \pi^*</span> ，使得它在各个状态下的累积回报最大。形式上，可以用<b><span data-search-entity="10">价值函数</span></b>（Value Function）或<b><span data-search-entity="4">动作价值函数</span></b>（Q函数）来表示。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pica.zhimg.com/v2-25e2a31c9d52d9a6c59018346f99c304_r.jpg" data-original-token="v2-25e2a31c9d52d9a6c59018346f99c304" data-rawheight="434" data-rawwidth="720" data-size="normal" src="images/v2-25e2a31c9d52d9a6c59018346f99c304_1440w.jpg" width="720"/><figcaption>强化学习问题示意图【15】</figcaption></figure><h3 data-into-catalog-status="" id="h_693582342_2"><b>强化学习的要素</b></h3><p data-pid="zfEuI6tX">在强化学习中，我们通常将问题描述为一个<b><span data-search-entity="1">马尔可夫决策过程</span></b>（Markov Decision Process, MDP），它包含以下几个关键元素：</p><p data-pid="mfRCerH8"> 1. <b>状态（State, S）</b>：智能体所处的环境状态，可能是游戏画面的一帧，也可能是机器人观测到的传感器数据。</p><p data-pid="zCsxg5f-"> 2. <b>动作（Action, A）</b>：智能体在给定状态下可以执行的一系列操作。</p><p data-pid="LH30wG2L"> 3. <b>状态转移（Transition Dynamics, P）</b>：从当前状态  <span class="ztext-math" data-eeimg="1" data-tex="s ">s </span> 采取动作  <span class="ztext-math" data-eeimg="1" data-tex="a">a</span>  后，会以一定的概率转移到下一个状态  <span class="ztext-math" data-eeimg="1" data-tex="s{\prime}">s{\prime}</span> 。</p><p data-pid="MS-gxlqN"> 4. <b>奖励（Reward, R）</b>：智能体在每个时刻（或每次转移）获得的即时回报，反映了该动作在此状态下的好坏。</p><p data-pid="2Pan6IAM"> 5. <b>折扣因子（Discount Factor, </b><span class="ztext-math" data-eeimg="1" data-tex="\gamma">\gamma</span> <b>）</b>：用于平衡当前奖励和未来奖励的重要性，数值通常在 (0, 1] 之间。</p><p data-pid="4fonFCRO"> 6. <b>轨迹（Trajectory）：</b>在强化学习中，智能体从环境的初始状态开始，与环境交互直至到达终止状态所经历的一系列状态、动作以及相应的奖励就构成了一条完整的“轨迹”。形式上可以表示为： <span class="ztext-math" data-eeimg="1" data-tex="\tau =  \{s_0, a_0, r_1, s_1, a_1, r_2, \dots, s_{T-1}, a_{T-1}, r_T, s_T\}">\tau =  \{s_0, a_0, r_1, s_1, a_1, r_2, \dots, s_{T-1}, a_{T-1}, r_T, s_T\}</span> 。其中， <span class="ztext-math" data-eeimg="1" data-tex="T">T</span> 表示回合（Episode）结束的时间步。在某些不定长的任务中， <span class="ztext-math" data-eeimg="1" data-tex="T ">T </span> 可能是一个随机值。轨迹可以帮助我们理解智能体如何从起始状态一步步演化到最终状态，并获取相应的奖励序列。</p><p data-pid="qawTlVL8">7. <b>经验（Experience）：</b>智能体可与环境交互多次，进行多次实验，形成多个轨迹。多个轨迹的集合被称为经验。即 <span class="ztext-math" data-eeimg="1" data-tex="E = \{\tau_1, \tau_2, ..., \tau_K \}">E = \{\tau_1, \tau_2, ..., \tau_K \}</span> 。</p><p data-pid="6ILlc7iU">8. <b>回报（Return）：</b>强化学习的目标是学习一个好的策略，智能体按照这样的策略和环境交互，让累积奖励达到最大。<b>这个累积奖励就是回报</b>。通常指从某一个时间步 t 开始，对未来所有奖励的加总，常用符号 <span class="ztext-math" data-eeimg="1" data-tex="G_t">G_t</span> 表示（有时也用 <span class="ztext-math" data-eeimg="1" data-tex="R">R</span> 表示）。其定义为</p><p data-pid="IKrng3-I"><span class="ztext-math" data-eeimg="1" data-tex="G_t = \sum_{t=0}^{T} r_{t}">G_t = \sum_{t=0}^{T} r_{t}</span> </p><p data-pid="wLm-z13q">通常现在的奖励和将来的奖励权重是不同的，比如今天奖励1万元和50年后奖励1万元，两者的价值大概率不同。因此会加上折扣因子<span class="ztext-math" data-eeimg="1" data-tex="\gamma">\gamma</span>。若存在折扣因子，则回报可以写为：</p><p data-pid="hiTB3Zxh"><span class="ztext-math" data-eeimg="1" data-tex="G_t = r_{0} + \gamma r_{1} + \gamma^2 r_{2} + \dots + \gamma^{T} r_T = \sum_{t=0}^{T} \gamma^t r_t">G_t = r_{0} + \gamma r_{1} + \gamma^2 r_{2} + \dots + \gamma^{T} r_T = \sum_{t=0}^{T} \gamma^t r_t</span> </p><p data-pid="NQ5mCM1O">其中 <span class="ztext-math" data-eeimg="1" data-tex="\gamma \in (0, 1]">\gamma \in (0, 1]</span> 用于平衡当前奖励和未来奖励的重要性。如果没有折扣（ <span class="ztext-math" data-eeimg="1" data-tex="\gamma = 1">\gamma = 1</span> ），那么回报就是从时间步 <span class="ztext-math" data-eeimg="1" data-tex="t">t</span> 开始直到回合结束所获得的总奖励。在有了回报概念后，我们知道，<b>强化学习的目标是最大化期望回报</b>。</p><p data-pid="Epoe2QXW">9. <b>价值函数（Value Function）</b>：<b>价值函数衡量的是“期望回报”</b>。具体分为状态价值函数和动作价值函数。</p><ul><ul><li data-pid="6xoMc9uL"><b>状态价值函数</b> <span class="ztext-math" data-eeimg="1" data-tex="V^\pi(s)">V^\pi(s)</span> 表示在状态 s 下，后续按照策略 <span class="ztext-math" data-eeimg="1" data-tex="\pi">\pi</span> 行动所能获得的<b>期望回报</b>：<br/> <span class="ztext-math" data-eeimg="1" data-tex="V^\pi(s) = \mathbb{E}[\,G_t \mid s_t = s\,].">V^\pi(s) = \mathbb{E}[\,G_t \mid s_t = s\,].</span> <br/> </li><li data-pid="Pkv3QEG9"><b>动作价值函数</b> <span class="ztext-math" data-eeimg="1" data-tex="Q^\pi(s, a)">Q^\pi(s, a)</span> 表示在状态 s 下执行动作 a，并在之后按照策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi"> \pi</span> 行动所能获得的<b>期望回报</b>：<br/> <span class="ztext-math" data-eeimg="1" data-tex="Q^\pi(s, a) = \mathbb{E}[\,G_t \mid s_t = s, a_t = a\,].">Q^\pi(s, a) = \mathbb{E}[\,G_t \mid s_t = s, a_t = a\,].</span> </li><li data-pid="sfBOKBED">V用来<b>评价在某个状态下的策略表现好坏</b>。Q用来<b>评价在某个状态-动作对下的策略表现好坏</b>。价值的估计至关重要，下文还会讨论。</li></ul></ul><p data-pid="Iv_U_vl4"><b>10：<span data-search-entity="3">优势函数</span>（Advantage Function）</b>：优势函数度量的是，在给定状态 s 下，执行某个动作 a 比起在该状态的平均水平（即状态价值）<b>好多少或差多少</b>。它的常见形式是<br/> <span class="ztext-math" data-eeimg="1" data-tex=" A^\pi(s, a) \;=\; Q^\pi(s, a) \;-\; V^\pi(s)."> A^\pi(s, a) \;=\; Q^\pi(s, a) \;-\; V^\pi(s).</span> </p><ul><ul><li data-pid="xIu9OAo-"> 如果 <span class="ztext-math" data-eeimg="1" data-tex="A^\pi(s, a)">A^\pi(s, a)</span> 大于 0，说明在状态 s 下执行动作 a 要比该状态的平均策略价值要好。<br/> </li><li data-pid="xiJnAjLK"> 如果 <span class="ztext-math" data-eeimg="1" data-tex="A^\pi(s, a)">A^\pi(s, a)</span> 小于 0，则说明这个动作比“平均”水平要差。</li></ul></ul><p data-pid="ufAAvd9z">11. 探索（<b>Exploration</b>）和学习（<b>Learning</b>）：强化学习一般分为两个阶段。第一个是<b>探索阶段</b>，智能体先按照某些<b>策略</b>和环境进行交互，形成经验。第二个是<b>学习阶段</b>，智能体按照某些算法，从经验中学习，进而优化自己的<b>策略</b>。</p><p data-pid="lKWFFYMR">12. <b>行为策略（Behavior Policy）和目标策略（Target Policy）：行为策略</b>是智能体在环境交互时实际执行的策略。<b>目标策略</b>是智能体最终想要学到的策略，通常记为 <span class="ztext-math" data-eeimg="1" data-tex="\pi">\pi</span> 。两个策略相同就是<span data-search-entity="8">on-policy</span>，否则就是<span data-search-entity="6">off-policy</span>，这点我们在下文中会详细探讨。</p><hr/><h3 data-into-catalog-status="" id="h_693582342_3"><b>价值函数和<span data-search-entity="7">贝尔曼方程</span></b></h3><p data-pid="JN0IWVsM">在强化学习中，<b>价值函数（Value Function）用来衡量在某个状态（或状态-动作对）下，按照某一策略</b> <span class="ztext-math" data-eeimg="1" data-tex=" \pi "> \pi </span> <b>行动所能获得的期望回报</b>；而<b>贝尔曼方程（Bellman Equation）</b>则刻画了该价值函数所必须满足的“递归一致性”关系。</p><p data-pid="E2MCpXNt">简而言之，<b>价值函数</b>是目标，<b>贝尔曼方程</b>是描述这个目标如何在相邻时间步之间相互关联的关键公式。</p><p data-pid="_l5i_bPJ"><b>1. 二者的关系</b></p><p data-pid="ennIz6V4"><b>价值函数的定义</b></p><ul><ul><li data-pid="JB60iW2G"><b>状态价值函数</b>：<br/> <span class="ztext-math" data-eeimg="1" data-tex="V^\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s],">V^\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s],</span> <br/> 即“在状态 s 下持续按照策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi"> \pi</span> 行动所能获得的期望回报”。<br/> </li><li data-pid="i4FYalh9"><b>动作价值函数</b>：<br/> <span class="ztext-math" data-eeimg="1" data-tex=" Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s,\; a_t = a],"> Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s,\; a_t = a],</span> <br/> 即“在状态 s 下先执行动作 a，然后继续按照策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi "> \pi </span> 行动所能获得的期望回报”。<br/> </li></ul></ul><p data-pid="Eol4J72x"><b>贝尔曼方程的自洽性（递归性）</b><br/> 价值函数可以通过对下一步状态（或动作）的价值进行加权求期望而“自我定义”：</p><ul><ul><li data-pid="XHEBnKlO"> 对状态价值 <span class="ztext-math" data-eeimg="1" data-tex="V^\pi(s)">V^\pi(s)</span> 来说，状态 s 的价值是“执行任一动作后的即时奖励 + 折扣后下一状态价值”的期望：<br/> <span class="ztext-math" data-eeimg="1" data-tex=" V^\pi(s) = \mathbb{E}{a\sim\pi(\cdot|s)}\Bigl[\,r(s,a) + \gamma \,\mathbb{E}{s{\prime}\sim P(\cdot|s,a)}[V^\pi(s{\prime})] \Bigr]."> V^\pi(s) = \mathbb{E}{a\sim\pi(\cdot|s)}\Bigl[\,r(s,a) + \gamma \,\mathbb{E}{s{\prime}\sim P(\cdot|s,a)}[V^\pi(s{\prime})] \Bigr].</span> <br/> </li><li data-pid="0KRFJ4k2"> 对动作价值 <span class="ztext-math" data-eeimg="1" data-tex="Q^\pi(s,a)">Q^\pi(s,a)</span> 来说，则是“该状态动作得到的即时奖励 + 折扣后下一状态的动作价值”的期望：<br/> <span class="ztext-math" data-eeimg="1" data-tex="Q^\pi(s,a) = \mathbb{E}{s{\prime}\sim P(\cdot|s,a)}\Bigl[\,r(s,a) + \gamma \,\mathbb{E}{a{\prime}\sim\pi(\cdot|s{\prime})}[Q^\pi(s{\prime},a{\prime})] \Bigr].">Q^\pi(s,a) = \mathbb{E}{s{\prime}\sim P(\cdot|s,a)}\Bigl[\,r(s,a) + \gamma \,\mathbb{E}{a{\prime}\sim\pi(\cdot|s{\prime})}[Q^\pi(s{\prime},a{\prime})] \Bigr].</span> </li></ul></ul><p data-pid="POYY5S6_">这些方程说明了：<b>当前状态（或状态-动作）价值</b>可以通过“下一步的价值”来计算出来，从而使价值函数具备可以迭代求解的性质。</p><p data-pid="doC9HLZj"><b>2. 为什么需要了解贝尔曼方程</b></p><p data-pid="lo_r6zkJ"><b> （1）动态规划与强化学习的理论基础</b><br/> 贝尔曼方程是强化学习算法的核心理论根基，揭示了价值函数能够被分解并通过递归方式计算的原理。几乎所有的基于价值的强化学习方法（Q-Learning、SARSA 等）都源于对贝尔曼方程的近似求解或逼近。</p><p data-pid="xhBmmY4s"><b>（2）求解或逼近价值函数</b></p><ul><ul><li data-pid="J-vxEprJ"> 在有完备环境模型、可枚举状态下，可以用<b>价值迭代（Value Iteration）或策略迭代（Policy Iteration）</b>对贝尔曼方程进行数值求解，找到最优价值函数和最优策略。</li><li data-pid="kQtCjSZB"> 在无完备模型、无法枚举的复杂环境中，可以采用蒙特卡洛、时序差分（TD Learning）以及基于神经网络的近似方法（如 DQN）对贝尔曼方程进行抽样估计和逼近。</li></ul></ul><p data-pid="tj_K4aft"><b>（3）理解算法更新规则</b><br/> 很多强化学习算法中的“更新公式”，本质上都可以视作对贝尔曼方程进行<b>采样</b>或<b>梯度</b>逼近。例如：<br/> <span class="ztext-math" data-eeimg="1" data-tex="Q(s,a) \;\leftarrow\; Q(s,a) \;+\; \alpha \bigl[\,r + \gamma\,\max_{a{\prime}}Q(s{\prime},a{\prime}) \;-\; Q(s,a)\bigr],">Q(s,a) \;\leftarrow\; Q(s,a) \;+\; \alpha \bigl[\,r + \gamma\,\max_{a{\prime}}Q(s{\prime},a{\prime}) \;-\; Q(s,a)\bigr],</span> <br/> 这正是 Q-Learning 中对<b>最优贝尔曼方程</b>的单步采样更新。</p><p data-pid="RIgjvipC"><b>（4）扩展到更复杂场景</b></p><ul><ul><li data-pid="FJ1VqN2O"> 在策略梯度和 Actor-Critic 方法中，Critic 的目标就是学习满足“贝尔曼方程”的价值函数或 Q 函数。<br/> </li><li data-pid="bXiYc-TN"> 其它如分层强化学习、多智能体强化学习等场景中，依旧离不开对贝尔曼方程的变形或推广。</li></ul></ul><p class="ztext-empty-paragraph"><br/></p><p data-pid="50b-eOhV"><b>总之，</b></p><ul><li data-pid="K1Qp9wKu"><b>价值函数</b>为评估“当前状态（或动作）有多好”提供了一个“期望回报”的度量。</li><li data-pid="aLVM5xsi"><b>贝尔曼方程</b>阐明了价值函数可以被分解成与下一状态和相应奖励相联系的自洽递归关系。</li></ul><hr/><h3 data-into-catalog-status="" id="h_693582342_4">A、V、Q的关系</h3><p data-pid="y68D0-Rk">优势函数（A）与值函数（V）、动作值函数（Q）的区别的<b>联系</b>：</p><p data-pid="jtovxCDY">优势函数是Q和V的差值。</p><p data-pid="5Zo2iSKp"><span class="ztext-math" data-eeimg="1" data-tex="A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)">A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)</span> </p><p class="ztext-empty-paragraph"><br/></p><p data-pid="WW0fzLE7">优势函数与值函数、动作值函数的<b>区别：</b></p><ul><li data-pid="L2JIpkJ0"><b>值函数（V）</b>：衡量一个状态的价值，反映了在某个状态下，智能体根据当前策略所能期望得到的总回报。</li><li data-pid="iHF65CI-"><b>动作值函数（Q）</b>：直接衡量在某状态下采取某动作后，智能体所能期望得到的总回报。</li><li data-pid="7ir3BSAC"><b>优势函数（A）</b>：是一个动作对比的度量，表明选择某个动作相较于平均策略的回报增益。和V、Q不同的是，A提供了一个相对的评价。</li></ul><h3 data-into-catalog-status="" id="h_693582342_5">估计值函数的三种方法：MC、TD、GAE</h3><p data-pid="8glILTIG">RL中，价值的估计非常重要。估计值函数的常见方法有三种：蒙特卡洛（MC）、时间差分（TD）、<span data-search-entity="9">广义优势估计</span>（GAE）。</p><p data-pid="PI-g1rmD">它们在估计值函数时各有优缺点，具体在方差和偏差之间存在不同的权衡。</p><p data-pid="TR07lfGn"><b>1. <span data-search-entity="5">蒙特卡洛方法</span>（MC）</b></p><p data-pid="5fTolgVf"> • <b>原理</b>：蒙特卡洛方法通过在完整的轨迹上计算回报（即从当前状态开始到最终状态的累积奖励）来估计状态值函数或动作值函数。这个方法完全依赖于最终的回报，因此需要等待完整的路径（完整的Episode）来计算值。</p><p data-pid="5nl-i2cb"> • <b>优点</b>：</p><ul><li data-pid="Zreq8ceR"> 不需要任何模型假设，直接依赖实际回报。</li><li data-pid="5dy5ABVs"> 在长期内是无偏的（即期望值是正确的），因为它直接使用实际的累积奖励。</li></ul><p data-pid="0eny9x65"> • <b>缺点</b>：</p><ul><li data-pid="V-d_dHtB"> 高方差：由于只使用每个Episode的最终回报，导致每个回报的估计可能存在较大波动，尤其是在奖励信号稀疏或变动较大的情况下，估计的方差较大。</li></ul><p data-pid="MoHA73lB"><b>总结</b>：MC方法因为依赖完整的回报，所以它的估计有较大的方差，但没有偏差。</p><p data-pid="JiM8r8Pp"><b>2. <span data-search-entity="2">时序差分方法</span>（TD）</b></p><p data-pid="X8tHJBs0"> • <b>原理</b>：时序差分方法则是通过<b>递推更新</b>（也称为bootstrapping）来估计值函数，它并不需要等待完整的Episode，而是根据每一步的即时反馈进行更新。TD方法将当前的估计值与下一时刻的估计值进行比较，通过差分来更新当前状态的值。</p><p data-pid="5SRtBJAa"> • <b>优点</b>：</p><ul><li data-pid="UOaXqHr7"> 较低的方差：由于TD方法使用每一步的即时反馈，它不依赖于完整Episode的回报，估计过程可以在更短时间内进行，因而方差较小。</li><li data-pid="g785w2TE"> 可以在线学习，不需要等待完整的Episode。</li></ul><p data-pid="80Ga4p2C"> • <b>缺点</b>：</p><ul><li data-pid="Ju4O7MYw"> 高偏差：由于TD方法使用的是估计值而非真实回报，它会引入一定的偏差。特别是它依赖当前的估计来更新，因此如果初始估计有偏，后续的更新也会继承这个偏差。</li></ul><p data-pid="YVlxClbl"> • <b>总结</b>：TD方法具有较低的方差，但它引入了偏差，因为它依赖于现有的估计，而非实际的回报。</p><p data-pid="2MkKYrji"><b>3. 广义优势估计（GAE）</b></p><p data-pid="3I6WQntz"> • <b>原理</b>：GAE是一种折中方法，它结合了MC和TD的方法，旨在通过平衡方差和偏差来提高估计的稳定性和效率。GAE通过引入一个<b>超参数</b> <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> （类似于TD方法中的折扣因子），在计算优势函数时利用TD方法的部分信息，而不是完全依赖于真实的回报。</p><p data-pid="7O_jhiXo"> • 具体来说，GAE通过对TD误差进行加权平均来估计优势函数，从而减少单步TD误差带来的偏差，并控制方差的大小。</p><p data-pid="EXhjPQ3u"> • 它的更新公式为：</p><p data-pid="4CPXHJ6S"><span class="ztext-math" data-eeimg="1" data-tex="A_t^{\lambda} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}">A_t^{\lambda} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}</span> </p><p class="ztext-empty-paragraph"><br/></p><p data-pid="pqJbQ_f0">其中， <span class="ztext-math" data-eeimg="1" data-tex="\delta_{t+l}">\delta_{t+l}</span> 表示每一步的TD误差， <span class="ztext-math" data-eeimg="1" data-tex="\gamma">\gamma</span> 是折扣因子， <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> 是用于加权的超参数，控制了TD和MC的折中。</p><ul><li data-pid="GFblOCB_"><b>优点</b>：</li><ul><li data-pid="GyXEuUHJ"> 在方差和偏差之间取得了更好的折中。通过调节 <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> ，GAE可以灵活控制偏差和方差之间的权衡。</li></ul><li data-pid="KW_754hN"><b>缺点</b>：</li><ul><li data-pid="GvcGP_7m"> 相较于纯粹的TD或MC，GAE需要更多的计算，因为它需要在每一步计算加权的TD误差。</li></ul><li data-pid="qqMWUFMG"><b>总结</b>：GAE通过加权TD误差，能够在MC的低偏差和TD的低方差之间取得一个折中。</li></ul><p class="ztext-empty-paragraph"><br/></p><p data-pid="LNrD6WmX"><b>4. 高方差、低偏差 vs. 高偏差、低方差的折中</b></p><ul><li data-pid="bD3wJTg-"><b>MC方法</b>高方差是由于它依赖于整个轨迹的回报，导致估计可能存在较大波动，但它是无偏的，因此在长时间运行时可以获得精确的估计。</li><li data-pid="tgVDmhFd"><b>TD方法</b>则通过引入当前估计来更新状态值或动作值，它减少了方差，因为每次更新都基于当前估计的反馈。然而，它依赖于已有的估计，因此会引入偏差。</li><li data-pid="zd6Sz3-T"><b>GAE</b>则通过平衡这两者的优缺点，调整 <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> 来控制估计中的偏差和方差，从而获得更稳定、更准确的估计。当 <span class="ztext-math" data-eeimg="1" data-tex="\lambda = 0">\lambda = 0</span> 时，GAE等价于TD(0)，当 <span class="ztext-math" data-eeimg="1" data-tex="\lambda = 1">\lambda = 1</span> 时，GAE等价于MC方法。</li></ul><p data-pid="eNW3GfsU"><b>总结</b></p><ul><li data-pid="Mr_2Bkt6"><b>MC方法</b>：高方差，无偏。</li><li data-pid="KWA_9-Bi"><b>TD方法</b>：低方差，高偏差。</li><li data-pid="c6v-w-BU"><b>GAE方法</b>：折中，既能减少偏差，也能降低方差，通过调整 <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> 来灵活控制。</li></ul><p data-pid="WB0jot3d">这个折中使得GAE在实际应用中往往能取得更好的性能，特别是在复杂的强化学习任务中。</p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_6">NLP中的RL</h3><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-c74f58b19921bf0a52c1d1bd196ea263_r.jpg" data-original-token="v2-c74f58b19921bf0a52c1d1bd196ea263" data-rawheight="365" data-rawwidth="1080" data-size="normal" src="images/v2-c74f58b19921bf0a52c1d1bd196ea263_1440w.jpg" width="1080"/><figcaption>RL in NLP【5】</figcaption></figure><p data-pid="yp45uHWF"><b>状态S：</b>输入prompt</p><p data-pid="PPWFZOxW"><b>动作A：</b>输出response（即LLM输出下一个token）</p><p data-pid="OFQYRP5W"><b>奖励R：</b>根据prompt+response进行奖励模型打分</p><p data-pid="WHryxs3g"><b>整体目标</b>：给定prompt，调整policy，生成符合人类喜好（RM偏序信号）的response</p><h3 data-into-catalog-status="" id="h_693582342_7">经典强化学习算法的分类</h3><p class="ztext-empty-paragraph"><br/></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-eba10ba1c76228967df531c5ac287f3f_r.jpg" data-original-token="v2-eba10ba1c76228967df531c5ac287f3f" data-rawheight="1022" data-rawwidth="1896" data-size="normal" src="images/v2-eba10ba1c76228967df531c5ac287f3f_1440w.jpg" width="1896"/><figcaption>强化学习算法分类【14】</figcaption></figure><p data-pid="b4io_ZAm">在这里，对里面的部分算法做简要介绍。</p><hr/><p data-pid="svLje94K"><b>马尔可夫决策过程（MDP）</b></p><p data-pid="w1bXPlNB"><b>马尔可夫性质</b>表示未来状态只与当前状态（和动作）有关，与过去的历史无关。这使得我们可以用一个状态来完整地描述环境。</p><p class="ztext-empty-paragraph"><br/></p><p data-pid="bHq6m5nL"><b>马尔可夫决策过程（MDP）</b>是一个在“马尔可夫性质”假设下建模序列决策的问题框架，一般用五元组 <span class="ztext-math" data-eeimg="1" data-tex="\langle S, A, P, R, \gamma \rangle">\langle S, A, P, R, \gamma \rangle</span> 表示，其中：</p><p data-pid="XdJxk-it"> • <span class="ztext-math" data-eeimg="1" data-tex="S">S</span> ：状态空间</p><p data-pid="bJ8K9TYJ"> • <span class="ztext-math" data-eeimg="1" data-tex="A">A</span> ：动作空间</p><p data-pid="zLlPi1OB"> • <span class="ztext-math" data-eeimg="1" data-tex="P(s{\prime} \mid s,a)">P(s{\prime} \mid s,a)</span> ：状态转移概率，描述在状态 <span class="ztext-math" data-eeimg="1" data-tex="s">s</span> 执行动作 <span class="ztext-math" data-eeimg="1" data-tex="a">a</span> 后转移到下一个状态 <span class="ztext-math" data-eeimg="1" data-tex="s{\prime}">s{\prime}</span> 的概率</p><p data-pid="I12RloYQ"> • <span class="ztext-math" data-eeimg="1" data-tex="R(s,a)">R(s,a)</span> ：即时奖励函数，衡量在状态 <span class="ztext-math" data-eeimg="1" data-tex="s">s</span> 执行动作 <span class="ztext-math" data-eeimg="1" data-tex="a">a</span> 所得到的回报</p><p data-pid="8zBOSM4m"> • <span class="ztext-math" data-eeimg="1" data-tex="\gamma">\gamma</span> ：折扣因子，用于平衡当前奖励和未来奖励</p><p data-pid="lU1OuEWj">在MDP中，智能体通过与环境的反复交互（“状态—动作—转移—奖励”的循环），旨在找到一条最优策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi^*"> \pi^*</span> ，最大化长期累积折扣回报。</p><hr/><p data-pid="yuRWgW0N"><b>Bandit problem（强盗问题或赌博机问题）</b></p><p data-pid="nhOfMbvI"><b>Bandit problem</b> 是强化学习和决策理论中的经典问题，用于研究<b>在不确定环境下的探索与利用权衡（exploration vs. exploitation trade-off）</b>。</p><p data-pid="HUnxvkgE">赌博机（Bandit）问题可视作MDP的极简形式：<b>系统仅有一个“状态”，或者说对后续状态没有区分</b>。每台赌博机（或称拉杆）对应一个动作，每个动作产生的奖励分布未知且相互独立。智能体在每次操作前需在各臂之间进行抉择，在缺乏状态转移的情况下，通过不断探索各臂的回报分布并利用已有信息，从而最大化累积奖励。<b>相比MDP，Bandit只关注“当前选择—即时奖励”的单步决策，不涉及序列性的状态演化</b>。</p><p class="ztext-empty-paragraph"><br/></p><p data-pid="PH_YovlZ"><b>Bandit Problem 的类型</b></p><p data-pid="oaqItwj_"> 1. <b>多臂老虎机（Multi-Armed Bandit, MAB）</b></p><ul><li data-pid="pDkpNxnK"> 基础形式，假设每台老虎机的奖励分布是固定的（静态）。</li><li data-pid="CZbRk8kQ"> 常见算法包括：</li><ul><li data-pid="n7fB_-ZG"><b>ε-greedy</b>：以一定概率随机探索，其他时候利用当前最优选项。</li><li data-pid="JKZI1KHf"><b>UCB（上置信界）</b>：根据当前奖励估计和不确定性选择最优选项。</li><li data-pid="s0rQk8-X"><b>Thompson Sampling</b>：通过贝叶斯推断选择最优选项。</li></ul></ul><p data-pid="TVflJdvB"> 2. <b>Contextual Bandit（上下文老虎机）</b></p><ul><li data-pid="WfpXGmhN"> 每次决策前会观察到一个“上下文”（context），不同上下文可能对应不同的最优选项。</li><li data-pid="FN69AY2d"> 类似于推荐系统问题：根据用户特征选择推荐内容。</li></ul><p data-pid="7P1_IfWG"> 3. <b>Non-Stationary Bandit（非静态老虎机）</b></p><p data-pid="m4EIhQ9H"> • 奖励分布会随着时间动态变化，需要更快速适应新分布的算法。</p><hr/><p data-pid="8lwsb24d"><b>DQN（Deep Q-Network）</b></p><p data-pid="plQ9xHeU"><b>DQN</b>将深度神经网络与Q-Learning相结合，使用卷积神经网络近似Q函数，在Atari游戏上取得革命性成果。</p><p data-pid="59Bq9HSz">关键技巧：</p><p data-pid="MnKeDMe5"> 1. <b>Experience Replay</b>：将交互经验存储在回放池中，随机小批量采样来打破数据相关性；</p><p data-pid="QOOl0XjI"> 2. <b>Target Network</b>：固定目标Q网络一段时间再更新，避免网络剧烈震荡。</p><hr/><p data-pid="t9-FplYw"><b><span data-search-entity="0">Actor-Critic架构</span></b></p><p data-pid="ShiDMja4">为平衡高方差和低偏差，在<b>Actor-Critic</b>中将策略函数（Actor）和价值函数（Critic）同时学习：</p><p data-pid="-d4yRcEh"> • <b>Actor</b>：输出策略 <span class="ztext-math" data-eeimg="1" data-tex=" \pi_\theta(a|s)"> \pi_\theta(a|s)</span> ；</p><p data-pid="wVvchfLB"> • <b>Critic</b>：估计价值函数  <span class="ztext-math" data-eeimg="1" data-tex="V_w(s) ">V_w(s) </span> 或  <span class="ztext-math" data-eeimg="1" data-tex="Q_w(s,a)">Q_w(s,a)</span> ；</p><p data-pid="pA5EBRbB"> • 每一次采样时，利用 Critic 来估计动作优势（Advantage），更新 Actor 的梯度，使得训练更稳定。</p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_8">RL的两大基本特征</h3><p data-pid="X6Gw1vaD">强化学习（Reinforcement Learning, RL）拥有两大关键特征：一是通过持续的<b>试错搜索（trial-and-error）来发现最佳行为，二是面临延迟回报（delayed reward）</b>的挑战。这使得强化学习与监督学习在“如何评价模型”这一核心问题上存在显著差异：</p><p data-pid="xN2Qr9Gf"> • 在监督学习中，模型会根据事先给定的正确标签（label）得到即时且明确的反馈；</p><p data-pid="PwTuLDOp"> • 而在强化学习中，智能体的评价取决于整个互动过程中所作出的一系列动作，只有通过对<b>累计获得的奖励</b>进行评估，才能衡量策略的优劣。</p><p data-pid="TtIebSnA">换言之，监督学习用“已知正确答案”来指导模型学习，而强化学习则是让智能体在环境中不断尝试、观察反馈，再基于累积奖励对策略进行调整，从而逐步逼近最优方案。</p><h3 data-into-catalog-status="" id="h_693582342_9">探索与利用的平衡</h3><p data-pid="7p_N_1VB">在强化学习（RL）中，<b>探索（Exploration）与利用（Exploitation）</b>的平衡是核心挑战：探索指尝试新动作以获取环境信息，利用则基于当前知识选择最优动作以最大化奖励。<b>过度探索可能导致低效，过度利用则易陷入局部最优</b>。常见方法包括：</p><ol><li data-pid="SqtNiORt"><b>ε-greedy</b>：以概率ε随机探索，其余时间贪婪利用；</li><li data-pid="EDUeLfMU"><b>Upper Confidence Bound（UCB）</b>：通过置信区间量化动作不确定性，平衡二者；</li><li data-pid="PPmVty_4"><b>Thompson Sampling</b>：基于贝叶斯后验分布动态调整动作选择；</li><li data-pid="FBQauWVk"><b>基于内在激励</b>（如好奇心驱动）或<b>信息增益</b>，鼓励访问未充分探索的状态；</li><li data-pid="PqVCY7x9"><b>Softmax策略</b>：按动作价值概率分布采样，兼顾高价值与潜在高回报动作。部分算法（如MCTS）结合基于模型的规划进一步优化此平衡。</li></ol><h3 data-into-catalog-status="" id="h_693582342_10">RL中的on-policy 和 off-policy有什么区别？</h3><p data-pid="21wZP-T9">在强化学习中，<b>on-policy</b> 和 <b>off-policy</b> 的区别，核心在于“<b>数据采样的策略（Policy）与学习的策略是否相同</b>”。以下从原理、示例、优缺点几个方面说明：</p><p data-pid="qvU2tP6O"><b>1. 原理概念</b></p><p data-pid="iJq13-a2"> 1. <b>on-policy</b></p><p data-pid="MLn0mk-l"> • <b>数据采样策略 与 当前正在学习/更新的策略 相同</b>。</p><p data-pid="JH2ycA6X"> • 智能体执行的行为策略（Behavior Policy）就是要学习或评估的目标策略（Target Policy）。</p><p data-pid="SgXIhleo"> • 换言之：智能体在环境中如何行动，就和我们要学的那条策略完全一致。</p><p data-pid="4tY7bQWG"> 2. <b>off-policy</b></p><p data-pid="J0uGO9ed"> • <b>数据采样策略 与 学习/评估的目标策略 不必相同</b>。</p><p data-pid="f3KlBYUL"> • 可以用一种行为策略与环境交互，收集到的数据却用来学习另一种目标策略。</p><p data-pid="w5IV9CeW"> • 常见做法是保留一个<b>经验回放池（Replay Buffer）</b>，历史数据可以不断被重用（不一定是按照当前策略采样得到的）。</p><hr/><p data-pid="7YpQu7jy"><b>2. 示例对比</b></p><p data-pid="DEN_wjxP"> 1. <b>on-policy</b></p><ul><li data-pid="K9Ml88_h"><b>PPO</b>、<b>A2C/A3C</b> 等。</li><li data-pid="uVFSTN86"> 在 PPO 中，为了保证策略稳定性，需要用最新的（或近似最新的）策略去采样交互数据，然后紧接着对这批数据进行更新，更新后又要丢弃旧数据，重新采样。</li><li data-pid="FU_wmocA"> 好处是可以严格保证“策略分布”和“数据分布”一致，<b>收敛性</b>更易分析。</li><li data-pid="BjOfJcgr"> 坏处是<b>数据利用率低</b>，因为一旦策略更新，这批数据就算“过期”了，很难重复使用来训练新策略。</li></ul><p data-pid="iU4iQenZ"> 2. <b>off-policy</b></p><ul><li data-pid="QEdLcu3_"><b>Q-Learning</b>、<b>DQN</b>、<b>SAC</b>、<b>TD3</b> 等。</li><li data-pid="f9mKbD1G"> DQN 中的经验回放池就是典型的 off-policy：采集数据时使用的是 <span class="ztext-math" data-eeimg="1" data-tex="\epsilon-greedy">\epsilon-greedy</span> 策略（带随机探索），但在更新 Q 函数时，我们朝着 “greedy” 或某个更优的目标策略方向改进。</li><li data-pid="01JTO3vh"> 好处是可以<b>重复使用</b>历史数据，样本效率更高；还可以从人类示教数据或其他智能体的轨迹中学习。</li><li data-pid="IV8rrqB6"> 坏处是策略与数据分布不一致带来的复杂性，可能更难保证收敛，更新过程也更易出现分布偏移（Distribution Shift）问题。</li></ul><hr/><p data-pid="AZKaNJyG"><b>3. 优缺点总结</b></p><table data-draft-node="block" data-draft-type="table" data-row-style="normal" data-size="normal"><tbody><tr><th>关键点</th><th>on-policy</th><th>off-policy</th></tr><tr><td>数据采样策略</td><td>与目标策略相同</td><td>与目标策略不同，可自行选择或混用</td></tr><tr><td>数据利用率</td><td>只能使用最近采样数据，效率较低</td><td>可以反复使用历史数据，效率较高</td></tr><tr><td>学习稳定性</td><td>分布一致性更好，算法分析更直接</td><td>数据分布差异大时，学习可能会不稳定</td></tr><tr><td>示例算法</td><td>A2C/A3C、PPO 等</td><td>Q-Learning、DQN、SAC、TD3 等</td></tr><tr><td>适用场景</td><td>中小型或交互稳定场景</td><td>大规模、需要高数据效率或可以使用离线数据场景</td></tr></tbody></table><hr/><p data-pid="cTB3F2S9"><b>4. 为什么要区分 on-policy 和 off-policy</b></p><p data-pid="YiJn9fOu"> • <b>策略分布一致性</b>：</p><p data-pid="ocnK3J-D">强化学习的目标是学习一条最优策略（或评估某条策略的价值）。若数据来自与目标策略不同的分布，我们就需要更多技巧（如重要性采样、行为策略修正）来保证学习的正确性。</p><p data-pid="Pp30YYBh"> • <b>数据效率</b>：</p><p data-pid="73bgaGol">on-policy 经常“采样-训练-丢弃”，数据无法重复利用；off-policy 通过经验回放或从其他来源引入大量轨迹数据，能反复训练，提高数据效率。</p><p data-pid="g3L4VKpc"> • <b>安全性与可控性</b>：</p><p data-pid="YPgHVxUG">某些场景需要在策略稳定前就保证系统安全，off-policy 可以用保守的行为策略来收集数据，而学的却是更激进的目标策略。</p><hr/><p data-pid="U-YZdjLk"><b>总结</b></p><ul><li data-pid="I9MwSwc7"><b>on-policy</b>：数据采样与目标策略一致，保证分布统一，算法理论分析更简洁，但数据浪费较大。</li><li data-pid="97onRfqd"><b>off-policy</b>：数据来源灵活，可以重复使用过去的经验，样本效率更高，但需要处理分布偏移带来的额外复杂性。</li></ul><p data-pid="bhoH2qJB">这就是两者在强化学习中的本质区别。<b>off-policy RL可以理解为“纸上得来终觉浅”，on-policy RL可以理解为“绝知此事要躬行”。</b></p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_11">RL中的online和offline有什么区别？</h3><p data-pid="TTMS50l9">在线强化学习(Online Reinforcement Learning)和离线强化学习(Offline Reinforcement Learning)是强化学习领域的两种不同学习范式，<b>它们的主要区别在于如何使用经验数据（即智能体与环境交互产生的状态、动作、奖励序列）来训练模型。</b>下面是两者之间的几个关键差异：</p><p data-pid="_2BRiYri"><b>数据收集方式</b>：</p><ul><li data-pid="0oOmbKjg"><b>在线强化学习</b>：在学习过程中，智能体直接与环境实时交互，每一步选择动作、接收环境反馈（奖励和下一个状态），并立即用这些新鲜数据更新其策略或价值函数。这意味着学习策略会直接影响到实际行为，并且环境需要是可交互的。</li><li data-pid="Ihkw9Cek"><b>离线强化学习</b>：事先从一个固定的数据集（通常是之前交互产生的）中学习，不与环境实时互动。这个数据集可以是通过专家演示、历史记录或其他代理的行为收集的。离线学习的目标是从这些静态数据中最大化学习效率，而无需进一步探索环境。</li></ul><p data-pid="6tUZ0oLR"><b>探索与利用的平衡</b>：</p><ul><li data-pid="rWiansNL"><b>在线学习</b>强调在探索未知策略与利用当前最优策略之间找到平衡，因为每次决策都直接影响到学习过程和未来奖励。</li><li data-pid="R4KWuOc_"><b>离线学习</b>则主要关注于利用已有的数据，由于没有新的数据采集，探索新策略的能力受限，因此重点在于如何有效利用现有数据优化策略。</li></ul><p data-pid="rPetWPgn"><b>安全性和稳定性</b>：</p><ul><li data-pid="KfzkLPM1"><b>离线强化学习</b>因为不直接与环境互动，所以特别适合于那些直接尝试可能有高风险或成本的场景，比如医疗决策、金融交易或物理机器人控制。</li><li data-pid="M0bLAHZr"><b>在线学习</b>可能涉及试错，对于某些敏感或昂贵的环境来说，错误的决策可能会导致不可逆的后果。</li></ul><p data-pid="_NwR-cMe"><b>数据效率与收敛速度</b>：</p><ul><li data-pid="PtggLk8H"><b>离线学习</b>可能在数据利用上更为高效，因为它试图从有限的数据集中榨取尽可能多的信息。但找到最佳策略的速度可能受限于数据集的覆盖范围和多样性。</li><li data-pid="RAaGBqr3"><b>在线学习</b>理论上可以无限探索，直至找到最优策略，但这个过程可能需要大量与环境的交互，从而在数据需求和时间成本上更高。</li></ul><p data-pid="9vt1KVtP"><b>策略优化自由度</b>：</p><ul><li data-pid="cR4jxvC6"><b>在线学习</b>允许智能体根据即时反馈调整策略，因此在策略空间中的探索更加灵活。</li><li data-pid="HCP_tdrK"><b>离线学习</b>受限于已收集数据的策略空间，难以评估未被数据覆盖的动作的好坏，因此优化策略时可能较为保守。</li></ul><p data-pid="73pBsJQe">综上所述，选择在线还是离线强化学习取决于具体的应用场景、可用资源、对安全性的要求以及对学习速度和数据效率的需求。</p><h3 data-into-catalog-status="" id="h_693582342_12"><b>on/off-policy和online/offline的区别</b></h3><p data-pid="9L9Cswnn">1. <b>不同维度</b></p><ul><li data-pid="Go0LqOqc"><b>on/off-policy</b> 解决的问题：<b>“采样策略”与“目标策略”的一致程度</b>。</li><li data-pid="rKFLxE4L"><b>online/offline</b> 解决的问题：<b>“是否可以持续交互收集新数据”</b>。</li></ul><p data-pid="IHMh9-OO"> 2. <b>常见组合</b></p><p data-pid="5yh8oKsM"><b>离线强化学习基本一定是 off-policy，但在线强化学习可以既有 on-policy，也可以有 off-policy。</b></p><ul><li data-pid="Ik1WFRLI"><b>在线 + on-policy</b>：比如 PPO、A2C 这些算法，需要跟环境交互，采集数据时就使用当前策略，采完就更新，旧数据不再使用。</li><li data-pid="rQ3WfmT0"><b>在线 + off-policy</b>：比如 DQN，虽然也是在线与环境交互，但 DQN 会把交互数据放到 replay buffer，后面训练时用到的旧数据不一定来自当前的策略，所以是 off-policy。</li><li data-pid="NDwYRXQa"><b>离线 + off-policy</b>：这最常见。离线 RL 必然不能和“当前目标策略”一致地采样，因为数据集已经固定了，通常是其他策略或历史操作生成的数据，所以几乎都是 off-policy。</li><li data-pid="_YGjMxKX"><b>离线 + on-policy</b>：理论上很难，因为离线数据本身就是固定收集的，跟当前想学的策略无法保持一致——所以离线强化学习通常都被视为 off-policy 的特例。</li></ul><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_13">重要性采样基本原理</h3><p data-pid="8kQQVAnG">重要性采样是一种统计方法，<b>用来在目标分布 </b><span class="ztext-math" data-eeimg="1" data-tex="p(x)">p(x)</span><b> 与采样分布 </b><span class="ztext-math" data-eeimg="1" data-tex="q(x)">q(x)</span><b> 不一致的情况下，调整采样结果的权重，使得从 </b><span class="ztext-math" data-eeimg="1" data-tex="q(x)">q(x)</span><b> 中采样的数据可以用于估计 </b><span class="ztext-math" data-eeimg="1" data-tex="p(x)">p(x)</span><b> 的期望。【10】</b></p><p data-pid="TCvguT4u">假设我们希望计算目标分布<span class="ztext-math" data-eeimg="1" data-tex="p(x)">p(x)</span> 下某个函数 <span class="ztext-math" data-eeimg="1" data-tex="f(x)">f(x)</span> 的期望：</p><p data-pid="h_W-bpbJ"><span class="ztext-math" data-eeimg="1" data-tex="\mathbb{E}_p[f(x)] = \int f(x)p(x) \, dx">\mathbb{E}_p[f(x)] = \int f(x)p(x) \, dx</span> </p><p data-pid="fv3NS8w4">如果<span class="ztext-math" data-eeimg="1" data-tex="p(x)">p(x)</span>很难直接采样，但可以从一个容易采样的分布 <span class="ztext-math" data-eeimg="1" data-tex="q(x)">q(x)</span> 中采样，则可以通过以下公式重写：</p><p data-pid="mzUOt3dT"><span class="ztext-math" data-eeimg="1" data-tex="\mathbb{E}_p[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx">\mathbb{E}_p[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx</span> </p><p data-pid="pGIW62ib">其中， <span class="ztext-math" data-eeimg="1" data-tex="\frac{p(x)}{q(x)}">\frac{p(x)}{q(x)}</span> 称为<b>重要性权重（Importance Weight）</b>。</p><p class="ztext-empty-paragraph"><br/></p><p data-pid="XLQK9UL-"><b>重要性采样的作用是什么？</b>为什么需要它？</p><p data-pid="mp8HaLYw">重要性采样允许我们使用与目标分布不同的采样分布，从而：</p><ul><li data-pid="lB84Q0ec">提高采样效率，尤其在<span class="ztext-math" data-eeimg="1" data-tex="p(x)">p(x)</span>较难采样的情况下；</li><li data-pid="N3FAwmkY"><b>在强化学习中，它允许我们基于旧策略生成的经验数据来优化新策略，而无需重新采样环境数据。</b></li></ul><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_14"><b>Bradley-Terry Model</b></h3><p data-pid="wAz0r8fg"><b>The Bradley-Terry Model</b> 是一种统计模型，用于处理成对比较（pairwise comparisons）的问题，目的是估计多个对象之间相对偏好的概率。例如，给定两项选择  A  和  B ，该模型估计 A 相对于 B 被偏好的概率。</p><p data-pid="b-SIa9F9"><b>Bradley-Terry Model 的基本形式</b></p><p data-pid="DD50RtGS">对于两个对象 <span class="ztext-math" data-eeimg="1" data-tex="i">i</span> 和 <span class="ztext-math" data-eeimg="1" data-tex="j">j</span> ，模型定义对象  <span class="ztext-math" data-eeimg="1" data-tex="i">i</span>  被偏好的概率为：</p><p data-pid="GbfGv6fi"><span class="ztext-math" data-eeimg="1" data-tex="P(i \succ j) = \frac{\exp(\beta_i)}{\exp(\beta_i) + \exp(\beta_j)}">P(i \succ j) = \frac{\exp(\beta_i)}{\exp(\beta_i) + \exp(\beta_j)}</span> </p><p data-pid="EvxoSIt8">其中：</p><p data-pid="C1rDQ-pO"> • <span class="ztext-math" data-eeimg="1" data-tex=" \beta_i"> \beta_i</span>  和  <span class="ztext-math" data-eeimg="1" data-tex="\beta_j">\beta_j</span>  分别是对象  <span class="ztext-math" data-eeimg="1" data-tex="i">i</span>  和  <span class="ztext-math" data-eeimg="1" data-tex="j">j</span>  的潜在得分（latent score）。</p><p data-pid="u9G7vn7P"> • 该模型的核心假设是：偏好的概率仅由对象的潜在得分决定，且满足<b>逻辑斯谛分布</b>。</p><p class="ztext-empty-paragraph"><br/></p><p data-pid="J6V7-UaZ"><b>Bradley-Terry Model 的用途</b></p><p data-pid="80GZ8Cmw"> • 该模型被广泛用于排序和评分系统，如体育比赛、推荐系统、搜索结果排序等。</p><p data-pid="MBGzjdhz"> • 在强化学习中，它可以用于<b>建模奖励函数</b>，特别是在基于偏好反馈的场景下。</p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_15">REINFORCE 和 PPO的区别和联系</h3><p data-pid="K2W_6Wt3">在强化学习中，REINFORCE 和 PPO 同属于<b>基于策略梯度</b>的方法，但在<b>更新方式</b>和<b>稳定性</b>上有本质差异。</p><p data-pid="gXjxqbCI"><b>REINFORCE</b></p><ul><li data-pid="ituwSB9S"><b>特点</b></li><ul><li data-pid="PtRx1dba"><b>最简单的策略梯度算法，也称“蒙特卡洛策略梯度”</b>，在每个回合结束后，直接基于整条轨迹的回报来更新策略。</li><li data-pid="ko7ayuP0"> 更新时使用 <span class="ztext-math" data-eeimg="1" data-tex="\log \pi_\theta(a_t \mid s_t)">\log \pi_\theta(a_t \mid s_t)</span> 乘以当时获得的总回报，获取策略梯度。</li></ul><li data-pid="35Ads88L"><b>优点</b></li><ul><li data-pid="LhMRiT4B"> 理论推导相对直接，概念简单，常用于基础教学和小规模实验。</li><li data-pid="68MhPEPk"> 对问题没有过多结构假设，容易与其他方法结合（如引入基线减小方差）。</li></ul><li data-pid="1JDwNQwa"><b>缺点</b></li><ul><li data-pid="I3337cst"><b>高方差</b>：需要大量样本才能得到稳定的梯度估计。</li><li data-pid="JGzQKZtE"><b>效率较低</b>：一次采样后便更新，不能多次利用同一批数据。</li></ul></ul><p class="ztext-empty-paragraph"><br/></p><p data-pid="3go08G_w"><b>PPO</b></p><ul><li data-pid="-hRq-9U5"><b>特点</b></li><ul><li data-pid="sD2d4rzB"> 属于 Trust Region Policy Optimization 的简化版，通过限制新旧策略的更新幅度来保证训练稳定性。</li><li data-pid="wo5etXnf"> 训练时，可在同一批数据上做多次迭代更新，但会对新旧策略的概率比率施加“剪切”或“KL 惩罚”，防止一步迈得过大。</li></ul><li data-pid="0VLrAkw2"><b>优点</b></li><ul><li data-pid="SGhQTcEM"><b>训练更稳定</b>：限制策略更新，使得学习过程不易崩溃。</li><li data-pid="NjemZq-Z"><b>样本利用率更高</b>：对同一批次数据多次梯度更新，提高效率。</li></ul><li data-pid="mLYoGW8g"><b>缺点</b></li><ul><li data-pid="qgIeZGhS"> 实现与调参相对更复杂，需要合适的超参数（如剪切阈值、KL 系数等）。</li><li data-pid="Ywi9JH_f"> 仍是 on-policy 方法，对离线数据的利用相对有限。</li></ul></ul><p class="ztext-empty-paragraph"><br/></p><p data-pid="ouVdfIFl"><b>REINFORCE 和 PPO的核心区别</b></p><ul><li data-pid="24aNXzdD"><b>更新方式</b></li><ul><li data-pid="vOchvYJO"> REINFORCE：整条轨迹结束后一次性更新，仅用该次采样的数据。</li><li data-pid="vgEYoVCu"> PPO：可以多次使用同一批数据，用截断或 KL 惩罚约束更新。</li></ul><li data-pid="i3NZzLYi"><b>稳定性与效率</b></li><ul><li data-pid="6stfEqrY"> REINFORCE：简单但高方差、低效率。</li><li data-pid="4wWYhHVT"> PPO：限制策略变化幅度，显著提升稳定性与收敛速度。</li></ul></ul><p data-pid="CICHH4rZ">因此，<b>REINFORCE</b> 常被视为最基础的策略梯度方法，适合讲解原理；而 <b>PPO</b> 则在实践中更常见，因其能在保持收敛性和稳定性的同时充分利用数据。</p><h3 data-into-catalog-status="" id="h_693582342_16">强化学习（RL）相比有监督微调（SFT）有哪些好处？</h3><p data-pid="S5Z58pmR">我们知道RL对于提升LLM的性能有很重要的作用，那么具体作用是什么呢？你真的知道为什么要用RL吗？RL的必要性在哪里？</p><p data-pid="eoXCtmYX"><b>第一点，RL相比SFT更有可能考虑整体影响。</b>SFT是针对单个token进行反馈，其目标是要求模型针对给定的输入给出确切的答案；而RL是针对整个输出文本进行反馈，并不针对单个token。这种反馈粒度的不同，使得RL既可以兼顾表达的多样性，又可以增强对微小变化的敏感性。由于自然语言的灵活性，相同的语义可以用不同的方式表达，SFT很难兼顾，而RL可以允许模型给出不同的多样性表达。此外SFT采用交叉熵损失，由于总和规则，总的Loss对个别词元的变化不敏感，也就是说改变个别词元对整体损失影响较小，但是在语言中，一个否定词就可以完全改变文本的整体含义。<b>RL可以通过奖励函数同时兼顾多样性和微小变化敏感性两个方面。</b></p><p data-pid="ME3-P0Qr"><b>第二点，RL更容易解决幻觉问题</b>。在模型不知道答案的情况下，SFT会促使模型给出答案。而使用RL则可以通过定制奖励函数，使得正确答案有很高的分数，放弃回答或回答不知道有中低分数，不正确的回答有非常高的负分，这样模型可以依赖内部知道选择放弃回答，从而缓解幻觉问题。</p><p data-pid="9mqiDZNM"><b>第三点，RL可以更好地解决多轮对话奖励累计的问题</b>。多轮对话的好坏，要考虑多次交互的整体情况，很难用SFT的方法构建，而使用RL，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断。</p><p data-pid="BvcOUrAx">此外，还可以参考这篇文章：</p><p data-pid="_EL2zGa4"><a class="internal" href="https://www.zhihu.com/question/651021172/answer/3513159005?utm_psn=1778953635276931072" target="_blank"><span class="invisible">https://www.</span><span class="visible">zhihu.com/question/6510</span><span class="invisible">21172/answer/3513159005?utm_psn=1778953635276931072</span><span class="ellipsis"></span></a></p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_17">大模型的3H原则是什么？</h3><p data-pid="rAWcNJvI">大模型的3H原则是：</p><ul><li data-pid="U0LEGn-i">帮助性（Helpfulness）</li><li data-pid="jl926ZPR">真实性（Honesty）</li><li data-pid="KSwsJHx-">无害性（Harmless）</li></ul><h3 data-into-catalog-status="" id="h_693582342_18">对齐税（Alignment Tax）</h3><p data-pid="-O63nlor">RLHF 微调通常伴随着多样性和语言流畅性的下降，这种现象被称为 <b>alignment tax【21，22】</b>。</p><p data-pid="prOyKb16"><b>分布外泛化能力（</b>out-of-distribution (OOD) generalisation<b>）</b>在模型面临广泛的现实场景时至关重要，而<b>输出多样性（</b>output diversity<b>）</b>则指模型生成多样化输出的能力。论文【22】研究发现，<b>与SFT相比，RLHF在处理新输入时的泛化能力更强，尤其是在训练数据与测试数据之间分布偏移较大的情况下</b>。然而，<b>与SFT相比，RLHF在多种衡量标准下显著降低了输出多样性</b>，这表明当前LLM微调方法在泛化能力和多样性之间存在权衡。</p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_19">Reward Hacking</h3><p data-pid="si8wNALG">在RL中，由于reward function设置不合理，导致agent只关心累计奖励，而并没有朝着预想的目标优化【24】。</p><p data-pid="TBPDL0px">“上有政策，下有对策”，下面这个故事可以很形象地展示Reward Hacking的问题：</p><blockquote data-pid="qNPhwSas">在印度殖民时期，政府为了减少眼镜蛇数量，悬赏每捕捉一条眼镜蛇给予奖励。一开始，人们积极捕蛇换钱，但后来一些人发现可以养蛇并假装捕捉，以赚取更多奖励。<br/>政府发现后取消了奖励政策，结果养蛇的人将蛇全部放生，导致蛇的数量比最初更多。<br/><b>寓意</b>：这是奖励机制设计失误（Reward Hacking）的典型案例，说明如果奖励不精准，会导致行为偏离初衷，甚至适得其反。</blockquote><p data-pid="ZEDBvj74">在RLHF中，奖励模型每次偏好的数据应该是针对我们优化目标更好的回答，但如果这些更好的回答都是更长的回答，那么模型在学习时会取巧，偏向于更长的回答。而在真正使用时，更长的回答未必更好。</p><p class="ztext-empty-paragraph"><br/></p><h2 data-into-catalog-status="" id="h_693582342_20">RLHF（RM+PPO）</h2><h3 data-into-catalog-status="" id="h_693582342_21">RLHF的整体流程是什么？</h3><p data-pid="H96cm0Ep">RLHF主要分为<b>奖励模型训练</b>和<b>近端策略优化（Proximal Policy Optimization, PPO）</b>两个步骤。</p><ul><li data-pid="QbLavdNO">奖励模型通过由人类反馈标注的<b>偏好数据</b>来学习人类的偏好，判断模型回复的有用性以及保证内容的无害性。奖励模型模拟了人类的偏好信息，能够不断地为模型的训练提供奖励信号。</li><li data-pid="ZU7HkiDN">在获得奖励模型后，需要借助强化学习对语言模型继续进行微调。近端策略优化可以根据奖励模型获得的反馈 优化模型，通过不断的迭代，让模型探索和发现更符合人类偏好的回复策略。</li></ul><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pica.zhimg.com/v2-fdd58ea4ecac5e65050979067271b89e_r.jpg" data-original-token="v2-fdd58ea4ecac5e65050979067271b89e" data-rawheight="1085" data-rawwidth="2037" data-size="normal" src="images/v2-fdd58ea4ecac5e65050979067271b89e_1440w.jpg" width="2037"/><figcaption>RLHF整体流程【3】</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_22">RLHF中有哪些数据集？</h3><p data-pid="m1I99wIy">RLHF中有三种数据集，分别是：</p><ul><li data-pid="HTul2V0E">偏好数据集：用来训练奖励模型。</li><li data-pid="_sUppl-s">提示数据集：在PPO流程中用来训练LLM。</li><li data-pid="u_tTAbFd">评估数据集：用来评估RLHF的效果。</li></ul><h3 data-into-catalog-status="" id="h_693582342_23">PPO中的4个模型及其作用</h3><p data-pid="bHchv8iU">PPO涉及到actor/critic/reward/reference 4个模型的协同训练和推理，这四个模型的详细解释如下：</p><ol><li data-pid="XLzwwkOG"><b>策略模型（Policy Model，或Actor Model）</b>，生成模型回复。它就是RLHF训练希望产出的模型。在PPO训练中更新梯度。</li><li data-pid="5dHdi12W"><b>奖励模型（Reward Model）</b>，输出奖励分数来评估回复质量的好坏。在PPO训练中不更新梯度。</li><li data-pid="UCFgNyXN"><b>评论模型（Critic Model，或Value Network）</b>，来预测回复的好坏，可以在训练过程中实时调整模型，选择对未来累积收益最大的行为。在PPO训练中更新梯度。</li><li data-pid="t-fWfEDL"><b>参考模型（Reference Model）</b>提供了一个 SFT 模型的备份，帮助模型不会出现过于极端的变化。在PPO训练中不更新梯度。</li></ol><h3 data-into-catalog-status="" id="h_693582342_24">PPO的Policy模型的目标函数</h3><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-ce99342d89a1478e43d433f8a5ced58b_r.jpg" data-original-token="v2-ce99342d89a1478e43d433f8a5ced58b" data-rawheight="1000" data-rawwidth="1239" data-size="normal" src="images/v2-ce99342d89a1478e43d433f8a5ced58b_1440w.jpg" width="1239"/><figcaption>PPO的目标函数</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_25">为什么需要训练奖励模型（RM）？</h3><p data-pid="yrOpv5wO">在RL的流程中，利用人类标注的反馈数据也可以对模型进行微调，但是，由于时间和成本的限制，针对每次优化迭代，人类很难提供足够的反馈。所以训练一个奖励模型是一个更为有效的方法，利用奖励模型模拟人类的评估过程。</p><p data-pid="P1ihUikr">RM决定了智能体如何从与环境的交互中学习并优化策略，以实现预定的目标，因此训练一个RM模型非常重要。</p><h3 data-into-catalog-status="" id="h_693582342_26">奖励模型（RM）是怎么训练的？</h3><p data-pid="DBAfnpW2"><b>奖励模型（RM）的训练流程如下：</b></p><p data-pid="dYh1B-Dz"> 1. <b>收集偏好数据</b>：用初始模型生成多个回答，让人类标注者进行两两比较或打分，从而获得「更优回答」的偏好数据。</p><p data-pid="SPasVBWt"> 2. <b>对偶对比训练</b>：令奖励模型输出一个标量评分 <span class="ztext-math" data-eeimg="1" data-tex="R(\text{回答})">R(\text{回答})</span> ；对同一问题的回答A和回答B，若人类偏好A，则训练目标是 <span class="ztext-math" data-eeimg="1" data-tex="R(A) &gt; R(B)">R(A) &gt; R(B)</span> ，常用对数似然形式的 <span class="ztext-math" data-eeimg="1" data-tex="\sigma(R(A) - R(B))">\sigma(R(A) - R(B))</span> 来表示。</p><p data-pid="PwitNCgc"> 3. <b>模型结构</b>：通常在预训练语言模型的基础上添加一个“Reward Head”，对文本输出打分。</p><p data-pid="SHDWaV79"> 4. <b>使用场景</b>：在PPO等强化学习阶段，用奖励模型为生成的回答打分，并将其作为奖励信号来优化策略模型，使生成结果更符合人类偏好。</p><h3 data-into-catalog-status="" id="h_693582342_27">RM的损失函数</h3><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-e3dd361d9d10aac42a1e7e5d313ce754_r.jpg" data-original-token="v2-e3dd361d9d10aac42a1e7e5d313ce754" data-rawheight="1000" data-rawwidth="2195" data-size="normal" src="images/v2-e3dd361d9d10aac42a1e7e5d313ce754_1440w.jpg" width="2195"/><figcaption>奖励模型RM的损失函数</figcaption></figure><h3 data-into-catalog-status="" id="h_693582342_28">PPO的流程是什么</h3><p data-pid="d2e0Cbie">PPO的实施流程如下：</p><ol><li data-pid="pD4VBPRv">环境采样：<b>策略模型</b>基于给定输入生成一系列的回复，<b>奖励模型</b>则对这些回复进行打分获得奖励。</li><li data-pid="5COOhlTY">优势估计：利用<b>评论模型</b>预测生成回复的未来累积奖励，并借助广义优势估计（Generalized Advantage Estimation，GAE）算法来估计优势函数，能够有助于更准确地评估每次行动的好处。</li><li data-pid="TOG0Eh_A">优化调整：使用优势函数来优化和调整<b>策略模型</b>，同时利用<b>参考模型</b>确保更新的策略不会有 太大的变化，从而维持模型的稳定性。</li></ol><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-36d53576eb29495894221e42cc0654b9_r.jpg" data-original-token="v2-36d53576eb29495894221e42cc0654b9" data-rawheight="608" data-rawwidth="1198" data-size="normal" src="images/v2-36d53576eb29495894221e42cc0654b9_1440w.jpg" width="1198"/><figcaption>RLHF中PPO的流程【4】</figcaption></figure><hr/><p data-pid="MIDBUP8N">作为PPO流程的补充，参考下面这张图，可以划分为<b>经验采样</b>和<b>训练</b>两个阶段【5】。</p><p data-pid="e7aK3ciV"><b>第一阶段经验采样</b>，也叫做rollout，就是滑跑的意思，在RL中可以理解为一次实验，也就是actor模型在当前环境下进行策略动作的过程，具体实现上就是actor模型根据prompt数据集进行generate生成response，然后根据prompt+response进行forward计算，得到logp/values/reward等元素，这里<b>涉及到actor、reference、critic、reward 4个模型的推理过程</b>；</p><p data-pid="ebU4wxVZ"><b>第二阶段就是训练流程</b>，涉及到<b>actor和critic两个模型</b>，从计算loss来看，算法上是相互独立的，本质上是两个模型独立训练。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-ece21d083fbd4ac15d6ca16d8a787de7_r.jpg" data-original-token="v2-ece21d083fbd4ac15d6ca16d8a787de7" data-rawheight="704" data-rawwidth="778" data-size="normal" src="images/v2-ece21d083fbd4ac15d6ca16d8a787de7_1440w.jpg" width="778"/><figcaption>PPO训练流程【5】</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_29">critic model和reward model有什么区别</h3><p data-pid="B0PMK7wD">在 RLHF 中，<b>Reward Model</b> 通过人类标注数据进行独立训练，旨在输出一个反映人类偏好的外部奖励分数，用于评估完整回复的质量；而 <b>Critic Model</b> 则与策略模型同步更新，侧重在训练过程中动态估算生成过程的价值或优势，帮助策略模型迭代优化。前者提供整体的外部评估信号，后者主要在内部学习如何预测长期回报。</p><h3 data-into-catalog-status="" id="h_693582342_30">PPO中优势函数指什么</h3><p data-pid="84d1NHKA">在PPO算法中，<b>优势函数（Advantage Function）用于评估状态-动作对的相对优劣程度。它衡量了执行某个动作相对于平均水平的优劣，即在给定状态下采取某个动作相对于采取平均动作的效果。</b></p><p data-pid="HsQQUVUL">优势函数可以用以下方式定义： <span class="ztext-math" data-eeimg="1" data-tex="Advantage(s, a) = Q(s, a) - V(s)">Advantage(s, a) = Q(s, a) - V(s)</span> </p><p data-pid="RLVwjiC9">其中， <span class="ztext-math" data-eeimg="1" data-tex="Advantage(s, a)">Advantage(s, a)</span> 表示在状态 <span class="ztext-math" data-eeimg="1" data-tex="s">s</span> 下采取动作 <span class="ztext-math" data-eeimg="1" data-tex="a">a</span> 的优势函数值， <span class="ztext-math" data-eeimg="1" data-tex="Q(s, a)">Q(s, a)</span> 表示状态动作对 <span class="ztext-math" data-eeimg="1" data-tex="(s, a)">(s, a)</span> 的动作值函数（也称为动作优势函数）， <span class="ztext-math" data-eeimg="1" data-tex="V(s)">V(s)</span> 表示状态值函数。</p><p data-pid="QbKEe_2s">优势函数的作用在于帮助评估当前动作的相对价值，以便在策略更新过程中确定应采取的动作。<b>通过比较不同动作的优势函数值，可以决定哪些动作是更好的选择</b>。正的优势函数值表示执行的动作比平均水平更好，而负的优势函数值表示执行的动作比平均水平更差。</p><p data-pid="ucDp5C6s">在PPO算法中，<b>优势函数用于计算策略更新的目标，以便调整策略概率分布来提高优势函数为正的动作的概率，并降低优势函数为负的动作的概率，从而改进策略的性能</b>。</p><p data-pid="PLHYR_ul">总而言之，优势函数在PPO算法中用于评估状态-动作对的相对优劣，帮助确定应该采取的动作，并在策略更新过程中引导策略向更优的方向调整。</p><h3 data-into-catalog-status="" id="h_693582342_31">GAE是什么？</h3><p data-pid="TEJ1uuTg"><b>GAE（Generalized Advantage Estimation）</b>在RLHF中是一种用来估计优势函数（Advantage Function）的方法，<b>目的是在策略梯度更新时减少方差并保持较小的偏差</b>。它通过在时间差分（TD）误差之上叠加一个带衰减因子（ <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span> ）的加权和，得到平滑且稳定的优势估计，从而帮助强化学习算法（如PPO）更有效地利用来自人类反馈的奖励信号进行训练。 <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span><b> 越接近 1，</b>代表将更多的未来奖励纳入考虑，优势估计会更加依赖长时间序列的回报，<b>偏差更小</b>但<b>方差更高</b>。 <span class="ztext-math" data-eeimg="1" data-tex="\lambda">\lambda</span><b> 越接近 0，</b>代表更依赖单步TD误差，优势估计主要由局部信息决定，<b>偏差更大</b>但<b>方差更小</b>。</p><h3 data-into-catalog-status="" id="h_693582342_32">重要性采样和clip</h3><p data-pid="CuWzGGKf">在PPO算法中，重要性采样（Importance Sampling）<b>和</b>剪辑（Clipping）机制是确保策略更新稳定性和效率的关键。</p><p data-pid="8WMcUCPy"><b>1. 重要性采样（Importance Sampling）</b></p><p data-pid="ihypdqSn">在强化学习中，策略的更新需要利用从环境中采集的数据。然而，直接使用新策略进行采样可能效率低下。为此，PPO采用重要性采样技术，利用旧策略下采集的数据来估计新策略的期望。</p><p data-pid="e9XrRu0_">具体而言，重要性采样比率定义为：<span class="ztext-math" data-eeimg="1" data-tex="\frac{\pi_\theta(y_t \mid s_t)}{\pi_{\text{old}}(y_t \mid s_t)}">\frac{\pi_\theta(y_t \mid s_t)}{\pi_{\text{old}}(y_t \mid s_t)}</span> 。其中， <span class="ztext-math" data-eeimg="1" data-tex="\pi_\theta(y_t \mid s_t)">\pi_\theta(y_t \mid s_t)</span> 和 <span class="ztext-math" data-eeimg="1" data-tex="\pi_{\text{old}}(y_t \mid s_t)">\pi_{\text{old}}(y_t \mid s_t)</span> 分别表示新旧策略， <span class="ztext-math" data-eeimg="1" data-tex="y_t">y_t</span> 和  <span class="ztext-math" data-eeimg="1" data-tex="s_t">s_t</span> 分别表示动作和状态。通过该比率，PPO能够调整旧策略下的采样数据，使其适用于新策略的更新，从而提高数据利用率。</p><p data-pid="M18jZsI7"><b>2. 剪辑机制（Clipping）</b></p><p data-pid="h7oP6XVB">尽管重要性采样提高了数据利用效率，但如果新旧策略差异过大，可能导致训练过程不稳定。为此，PPO引入剪辑机制，限制重要性采样比率的变化范围，防止策略更新过度。具体而言，在PPO的目标函数中， <span class="ztext-math" data-eeimg="1" data-tex="\epsilon">\epsilon</span> 是一个小的正数，用于限制比率的偏离程度。通过这种剪辑操作，PPO有效地限制了策略更新的幅度，确保训练过程的稳定性。</p><p data-pid="70BKcub_"><b>3. 重要性采样与clip的协同作用</b></p><p data-pid="7FJH9HOj">重要性采样和clip在PPO中协同工作：</p><p data-pid="cKKKahr3"> • <b>重要性采样</b>允许使用旧策略的数据来估计新策略的性能，提高数据利用效率。</p><p data-pid="eRiSzfLB"> • <b>clip</b>限制新旧策略之间的差异，防止策略更新过度，确保训练过程的稳定性。</p><p data-pid="DN8MGPFu">通过这两者的结合，PPO实现了在策略优化中的高效性和稳定性。</p><h3 data-into-catalog-status="" id="h_693582342_33">rollout是什么意思?</h3><p data-pid="Hmcnevb4">“Rollout”这个词来源于强化学习和搜索算法中的一个常见术语，意思是“展开”或“铺开”。在RL中，rollout指的是从某个初始状态开始，按照当前策略一步步“展开”或“走出”一个完整的行为轨迹（也称为一次实验或episode）。这种过程就像将一张纸缓缓展开一样，把从当前策略得到的决策序列“铺”出来，以便后续评估整个轨迹的表现（如计算奖励、优势等），为策略优化提供样本数据。</p><p data-pid="e6TsGWgU"><b>在RLHF-PPO中，“rollout”指的是利用当前策略模型（actor）在给定的输入（prompt）下生成一系列回复，并对这些回复进行评估（如计算log概率、价值估计和奖励）的过程</b>。这一过程相当于在环境中进行一次完整的实验，为后续利用优势估计和PPO算法优化策略提供必要的样本数据，同时确保更新时参考RM和reference模型的信息，以维持整体模型的稳定性。</p><h3 data-into-catalog-status="" id="h_693582342_34">make_experience和rollout、Episode有什么区别与联系</h3><p data-pid="pYQ7oydB">在RLHF-PPO的语境下，这三个术语都涉及生成和收集与环境交互的数据，但侧重点略有不同：</p><ul><li data-pid="Fl2By-i1"><b>make_experience</b>：这是一个泛指的说法，描述了让模型与环境交互、生成经验数据的整个过程。它强调“制造经验”，也就是收集状态、动作、奖励等数据，为后续训练提供素材。</li><li data-pid="FHJi95Qa"><b>rollout</b>：特指利用当前策略模型在环境中生成一段轨迹的过程。这段轨迹可以是完整的，也可以只是部分数据。Rollout更侧重于“展开”策略执行的具体过程，用来评估策略或计算优势等。</li><li data-pid="OvJmN-Yf"><b>episode</b>：指的是从环境的初始状态开始，到达到终止条件为止的一整段交互过程，是一种完整的轨迹。它是rollout的一种特殊情况，即rollout生成的是一个完整的体验序列。</li></ul><p data-pid="5KyZVVSZ"><b>联系与区别</b>：</p><ul><li data-pid="082qBSXN"><b>联系</b>：三者都涉及通过策略与环境交互来收集数据。通常，make_experience的过程可能就是通过执行rollout来实现，而如果rollout生成的是从开始到结束的完整轨迹，那么它就构成了一个episode。</li><li data-pid="27LuF9vl"><b>区别</b>：make_experience是一个更宽泛的概念，强调整个数据收集过程；rollout强调的是生成轨迹的动作，不一定要求轨迹完整；而episode专指完整的体验序列。</li></ul><p data-pid="wXB0qlcP">总的来说，make_experience包含了通过rollout生成经验，而rollout有时可能只是一部分episode，三者在数据生成过程中各有侧重。</p><h3 data-into-catalog-status="" id="h_693582342_35">PPO的痛点是什么</h3><p data-pid="MFru9wxY">算法上的痛点，调参困难，不容易训出好结果。</p><p data-pid="gb5WAWnY">工程实现上的痛点：涉及到多阶段的dataloader（prompt和rollout、train），actor模型的generate自回归过程，以及4个不同的模型推理，2个不同的模型训练。</p><h3 data-into-catalog-status="" id="h_693582342_36">有哪些PPO的技巧</h3><p data-pid="2sAN6bb8">参考文档【18】。</p><h2 data-into-catalog-status="" id="h_693582342_37">DPO【11】</h2><h3 data-into-catalog-status="" id="h_693582342_38">DPO的基本原理</h3><p data-pid="rRJAXfNK">DPO的基本原理：<b>增加偏好样本的对数概率与减小非偏好样本响应的对数概率</b>。它结合了<b>动态加权机制</b>，以避免仅使用概率比目标时遇到的模型退化问题。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-ee3c677200c094464dcd5e23ebd2c57d_r.jpg" data-original-token="v2-ee3c677200c094464dcd5e23ebd2c57d" data-rawheight="444" data-rawwidth="2108" data-size="normal" src="images/v2-ee3c677200c094464dcd5e23ebd2c57d_1440w.jpg" width="2108"/><figcaption>DPO在优化人类偏好时避免了使用强化学习。现有RLHF方法通常会首先使用一个奖励模型（Reward Model）来拟合一个包含提示（Prompt）和人类对响应对（Response Pair）偏好的数据集，然后通过强化学习找到一个能够最大化该奖励模型的策略（Policy）。相比之下，DPO 直接以简单的分类目标优化最能满足偏好的</figcaption></figure><blockquote data-pid="DX5prXTc">DPO在优化人类偏好时避免了使用强化学习。现有RLHF方法通常会首先使用一个奖励模型（Reward Model）来拟合一个包含提示（Prompt）和人类对响应对（Response Pair）偏好的数据集，然后通过强化学习找到一个能够最大化该奖励模型的策略（Policy）。相比之下，DPO 直接以简单的分类目标优化最能满足偏好的策略，通过拟合一个隐式奖励模型，其对应的最优策略可以以闭式形式（Closed Form）提取出来。</blockquote><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_39">DPO的loss</h3><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-aa334b22cfcb50fa82608e8d6a4bf0b9_r.jpg" data-original-token="v2-aa334b22cfcb50fa82608e8d6a4bf0b9" data-rawheight="1000" data-rawwidth="2334" data-size="normal" src="images/v2-aa334b22cfcb50fa82608e8d6a4bf0b9_1440w.jpg" width="2334"/><figcaption>DPO的损失函数</figcaption></figure><p data-pid="knUQhZTK">DPO loss的代码：</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="k">def</span> <span class="nf">dpo_loss</span><span class="p">(</span><span class="n">pi_logps</span><span class="p">,</span> <span class="n">ref_logps</span><span class="p">,</span> <span class="n">yw_idxs</span><span class="p">,</span> <span class="n">yl_idxs</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="s2">"""
</span><span class="s2">    pi_logps: policy logprobs, shape (B,)
</span><span class="s2">    ref_logps: reference model logprobs, shape (B,)
</span><span class="s2">    yw_idxs: preferred completion indices in [0, B-1], shape (T,)
</span><span class="s2">    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
</span><span class="s2">    beta: temperature controlling strength of KL penalty
</span><span class="s2">    Each pair of (yw_idxs[i], yl_idxs[i]) represents the
</span><span class="s2">      indices of a single preference pair.
</span><span class="s2">"""</span>
    <span class="n">pi_yw_logps</span><span class="p">,</span>  <span class="n">pi_yl_logps</span> <span class="o">=</span>  <span class="n">pi_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span>  <span class="n">pi_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>
    <span class="n">ref_yw_logps</span><span class="p">,</span> <span class="n">ref_yl_logps</span> <span class="o">=</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>
    <span class="n">pi_logratios</span>  <span class="o">=</span> <span class="n">pi_yw_logps</span> <span class="o">-</span> <span class="n">pi_yl_logps</span>
    <span class="n">ref_logratios</span> <span class="o">=</span> <span class="n">ref_yw_logps</span> <span class="o">-</span> <span class="n">ref_yl_logps</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logratios</span> <span class="o">-</span> <span class="n">ref_logratios</span><span class="p">))</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logps</span> <span class="o">-</span> <span class="n">ref_logps</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">rewards</span></code></pre></div><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_40">DPO相比PPO有哪些改进？</h3><p data-pid="XwghHmDV">在RLHF（人类反馈的强化学习）的背景下，DPO（Direct Preference Optimization，直接偏好优化）和PPO（Proximal Policy Optimization，近端策略优化）是两种用于微调大型语言模型（LLM）的方法。以下是这两种方法的一些关键改进点：</p><p data-pid="gxfLX2A2"><b>简化的训练流程</b>：</p><ul><li data-pid="JnQBi5HB"><b>DPO</b>：直接从人类偏好数据集中学习，不需要训练一个单独的奖励模型。这简化了训练流程，减少了计算资源的需求。</li><li data-pid="gHnUUslH"><b>PPO</b>：通常需要训练一个奖励模型，然后使用这个模型来指导策略的优化，这需要更多的计算资源和时间。</li></ul><p data-pid="tI6H-pXB"><b>对超参数的鲁棒性</b>：</p><ul><li data-pid="2SZmKrAW"><b>DPO</b>：对超参数的变化更为鲁棒，不容易陷入局部最优，也不需要频繁调整超参数。</li><li data-pid="Lx8v6qLN"><b>PPO</b>：对超参数的选择较为敏感，可能需要仔细调整以避免训练不稳定。</li></ul><p data-pid="uPoSUA79"><b>计算效率和效果</b>：</p><ul><li data-pid="74PMejA_"><b>DPO</b>：有些情况下，DPO能够用更少的资源达到接近PPO的效果，在计算效率方面有优势。不过在适合RL scaling的场景下，DPO在效果上和PPO的差距还是比较大的。</li><li data-pid="r49GNP49"><b>PPO</b>：虽然可以达到很高的性能，但通常需要更多的计算资源和数据。</li></ul><p data-pid="PwrxEVyf"><b>模型解释性和用户偏好的适应性</b>：</p><ul><li data-pid="jvUqy1OI"><b>DPO</b>：由于直接从用户偏好数据学习，DPO通常具有更好的解释性，更容易理解模型的决策过程，并且能够更灵活地适应不同用户的个性化偏好。</li><li data-pid="uLD9B-ZI"><b>PPO</b>：由于其基于奖励的优化机制，PPO可能在解释模型的行为时遇到困难，特别是在复杂的任务中，其决策过程可能不那么直观。</li></ul><h3 data-into-catalog-status="" id="h_693582342_41">DPO和SFT有什么相同点和不同点？</h3><p data-pid="B4uQtJC0">DPO（Direct Preference Optimization）和SFT（Supervised Fine-Tuning）都是用于优化大型语言模型（LLMs）的方法，旨在改善模型生成文本的质量，使其更符合人类的偏好。下面是它们的一些相同点和不同点：</p><p data-pid="80Ddb9Us"><b>相同点：</b></p><ol><li data-pid="bO4o8WSt"><b>目标导向</b>：两者都旨在通过调整语言模型的参数来优化其输出，使之更加符合特定的目标或偏好。</li><li data-pid="s6SBB3m5"><b>数据依赖</b>：DPO和SFT都<b>依赖于外部数据来指导模型的微调过程</b>（与之不同的是，PPO/GRPO是依赖外部的reward来指导模型的训练)。DPO使用偏好数据，而SFT通常使用带有标签的示例数据。</li></ol><p data-pid="-LLdw9XE"><b>不同点：</b></p><p data-pid="roZi17cV"><b>数据形式与处理</b>：</p><ul><li data-pid="vqr_FSK1"><b>DPO</b>：直接利用偏好数据进行优化，这种偏好数据可以体现为正负样本对比，无需显式构建奖励模型。它通过偏好反馈（如用户偏好评级）直接优化策略，使得模型学习到什么样的输出是更受欢迎或更符合预期的。</li><li data-pid="VO_FHcqk"><b>SFT</b>：依赖于有监督的标签数据，即输入-输出对的形式，其中输出是人工标注的理想响应或文本。这种方法更像是传统的机器学习任务中的监督学习，通过已知正确答案来调整模型以减少预测误差。</li></ul><p data-pid="tJbp4mee"><b>优化目标与过程</b>：</p><ul><li data-pid="uxRaPK1F"><b>DPO</b>：优化过程关注于<b>最大化偏好数据下策略的表现</b>，通过<b>对比学习</b>等技术直接在策略上进行优化，避免了额外的奖励函数建模步骤。</li><li data-pid="YZzNEN0N"><b>SFT</b>：目标是使模型输出尽可能匹配给定的标签数据，优化目标通常是最大化似然或<b>最小化交叉熵损失</b>，这是一个直接的监督学习问题。</li></ul><p data-pid="VWznV7-2"><b>复杂度与灵活性</b>：</p><ul><li data-pid="WDHUanVO"><b>DPO</b>：可能在处理复杂的偏好表达和非结构化的偏好数据上更有优势，因为它不依赖于精确的评分或奖励信号，而是通过比较学习。</li><li data-pid="ZdhLNCL9"><b>SFT</b>：在任务明确且有高质量标注数据的情况下更为直接有效，但可能在处理模糊或高度主观的偏好表达上不如DPO灵活。</li></ul><p data-pid="_CEyHFGn"><b>应用场景</b>：</p><ul><li data-pid="mzptlG18"><b>DPO</b>：适合于那些偏好标准不易量化或者偏好数据可以通过比较获得的场景，比如艺术创作、个性化推荐等。</li><li data-pid="4tUU7hQd"><b>SFT</b>：更适合于有明确正确答案或标准输出的任务，如问答系统、翻译任务等。</li></ul><p data-pid="rMMuojls">综上所述，DPO和SFT各有侧重，选择哪种方法取决于具体任务的需求、数据的性质以及对模型输出质量的具体要求。</p><h2 data-into-catalog-status="" id="h_693582342_42">DPO改进版本</h2><h3 data-into-catalog-status="" id="h_693582342_43">IPO【19】</h3><p data-pid="nP8DSBhP"><b>DPO 的一个缺点是它在人类偏好数据集上很快就会过拟合</b>。为了避免这种情况，谷歌 DeepMind 的研究人员引入了身份偏好优化（IPO），这种方法<b>为 DPO 损失添加了一个正则，能够在不使用「提前停止」等技巧的情况下让模型收敛</b>。</p><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_44">KTO【20】</h3><p data-pid="vZJnGp4s"><b>Kahneman-Tversky优化（KTO）</b>：KTO是一种基于人类心理认知过程的偏好优化算法。它通过分析人类在决策过程中的心理认知过程（如注意力分配、记忆提取等），来优化模型的输出。<b>KTO的一个显著特点是它不需要成对的偏好数据，只需将样本标注为“好”或“坏”，降低了数据收集的成本和难度</b>。然而，如何准确地模拟人类的心理认知过程，以及将这种模拟结果应用到实际场景中，仍是KTO需要解决的问题。</p><h2 data-into-catalog-status="" id="h_693582342_45">GRPO【12】</h2><h3 data-into-catalog-status="" id="h_693582342_46">相对PPO的改进</h3><p data-pid="VcYAnKtP">GRPO是对PPO的一种改进版本，属于online RL。它通过暴力采样求均值的方式替代了PPO中的Critic Model，同时保留了PPO中的重要性采样和裁剪机制。GRPO中冻结了Ref和RM 2个模型，仅需要训练Policy Model。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-1a0520d031941077b31cfc6bfec6b19c_r.jpg" data-original-token="v2-1a0520d031941077b31cfc6bfec6b19c" data-rawheight="836" data-rawwidth="1950" data-size="normal" src="images/v2-1a0520d031941077b31cfc6bfec6b19c_1440w.jpg" width="1950"/><figcaption>图4 | PPO与我们提出的GRPO的演示。GRPO放弃了价值模型，而是通过小组得分估计基线，从而显著减少了训练资源的消耗。【12】</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><h3 data-into-catalog-status="" id="h_693582342_47">目标函数</h3><p data-pid="VzqaCHFi">GRPO的目标函数</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic3.zhimg.com/v2-edb79bf548d7d181db49b0a4014a41ec_r.jpg" data-original-token="v2-edb79bf548d7d181db49b0a4014a41ec" data-rawheight="320" data-rawwidth="2552" data-size="normal" src="images/v2-edb79bf548d7d181db49b0a4014a41ec_1440w.jpg" width="2552"/></figure><h2 data-into-catalog-status="" id="h_693582342_48">DAPO 【27】</h2><p data-pid="nY1_Lm7S">这是字节跳动seed团队对GRPO的重要改进。<b>GRPO 面临四个主要问题：其单一剪切范围易导致低概率 Token 无法得到有效提升（熵坍塌），样本级别损失会弱化长序列 Token 的梯度贡献，一旦所有输出都是全对或全错则无梯度信号可用，且统一惩罚过长生成会引入噪声</b>。</p><p data-pid="VdvbhZCY">DAPO 则通过“四大改进”逐一解决：</p><ul><li data-pid="y5N6TXGt"><b>Clip-Higher 将剪切上下限分开以保留多样性；</b></li><li data-pid="fp9LWN8F"><b>Dynamic Sampling 过滤全对/全错样本保持有效梯度；</b></li><li data-pid="p19_aGLL"><b>Token-Level Policy Gradient Loss 平衡长序列 Token 贡献；</b></li><li data-pid="mF_KhvWq"><b>Overlong Reward Shaping 为过长文本进行柔性惩罚或掩码，减少噪声干扰。</b></li></ul><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic3.zhimg.com/v2-638ceea6aa03b5d88366ca081708628e_r.jpg" data-original-token="v2-638ceea6aa03b5d88366ca081708628e" data-rawheight="448" data-rawwidth="836" data-size="normal" src="images/v2-638ceea6aa03b5d88366ca081708628e_1440w.jpg" width="836"/></figure><p data-pid="7-EOZtEy">详细可参考下面文章【28】。</p><a class="LinkCard new" data-draft-node="block" data-draft-type="link-card" data-image="https://pica.zhimg.com/equation_ipico.jpg" data-text="大家好我是爱因：DAPO：GRPO的问题分析及四个改进策略" href="https://zhuanlan.zhihu.com/p/1888687830710084683" target="_blank"><span class="LinkCard-image LinkCard-image--default"></span><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span></a><p class="ztext-empty-paragraph"><br/></p><h2 data-into-catalog-status="" id="h_693582342_49">Dr. GRPO【29】</h2><p data-pid="9trQooiu">作者认为<b>GRPO算法在优化目标上存在的“长度偏置”会导致模型产生冗长但不一定正确的回答。为解决这一问题，作者提出了 Dr. GRPO，即在不牺牲推理性能的前提下消除不必要的冗长，显著提高了“token效率”（token efficiency）。基于此，他们给出了一份简化的 R1-Zero 训练配方</b>：在 Qwen2.5-Math-7B 模型上仅用 27 小时就获得了在多项数学竞赛数据集上的最新最优结果（AIME2024 准确率达到 43.3%）。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-dd930f961bd3709bd5c6950bddb62e7f_r.jpg" data-original-token="v2-dd930f961bd3709bd5c6950bddb62e7f" data-rawheight="476" data-rawwidth="1441" data-size="normal" src="images/v2-dd930f961bd3709bd5c6950bddb62e7f_1440w.jpg" width="1441"/><figcaption>图 1：左图：Dr. GRPO 通过去除长度和标准差归一化项，对 GRPO（Shao 等，2024）进行了简单但重要的修改，以解决其偏差问题。右图：我们提出的无偏优化器有效地防止模型生成越来越长但错误的回答，从而提升了 token 效率。</figcaption></figure><p data-pid="ZXIFztiH">详细可参考文章【30】。</p><a class="LinkCard new" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/equation_ipico.jpg" data-text="大家好我是爱因：理解 R1-Zero 式训练：一个批判性视角" href="https://zhuanlan.zhihu.com/p/1891404563971564359" target="_blank"><span class="LinkCard-image LinkCard-image--default"></span><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span></a><p class="ztext-empty-paragraph"><br/></p><h2 data-into-catalog-status="" id="h_693582342_50">RLOO【13，23】</h2><p data-pid="20LIQq13">背景：在基于策略梯度的RL方法中，REINFORCE 和 PPO相比更加基础和简单（详情参考本文“REINFORCE 和 PPO的区别和联系”小节）。RLOO（REINFORCE Leave-One-Out）是REINFORCE的改进版。</p><p data-pid="iomRbuLK">RLOO的提出者认为，由于RLHF的起点是训练好的SFT模型，而不是参数随机初始化的模型，在PPO中诸如GAE、CLIP这些用于稳定训练的策略是没必要的。</p><p data-pid="Ngo2tykI">通过将整个回复生成当作一个action，并用样本间差异取代critic model，RLOO可以使训练更简单，性能也很好。</p><p class="ztext-empty-paragraph"><br/></p><p data-pid="opFiWjDV"><b>RLOO 的提出动机：</b></p><ol><li data-pid="2FmnZCOl"><b>RLHF和传统RL不同</b>。相比传统的RL，RLHF中的初始policy更好，因此PPO中的很多假设和技巧是没必要的。</li><li data-pid="Q7o_dUhO"><b>降低计算资源需求</b>：PPO 通常需要同时加载四个模型（actor/critic/reward/reference），这对 GPU 内存提出了很高的要求。RLOO 通过减少所需模型的数量，降低了内存占用，使得训练过程更高效。</li><li data-pid="u0LWBaJr"><b>简化实现复杂性</b>：PPO 的实现涉及许多微妙的细节，可能难以正确把握。RLOO 采用更直接的 REINFORCE 风格优化，减少了实现过程中的复杂性。</li><li data-pid="H5vuLr9H"><b>提高训练效率</b>：RLOO 的设计使其能够使用更大的批量大小，缩短训练时间，从而加快模型的收敛速度。</li></ol><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-df12253f4eb3afa67788e97902f009dd_r.jpg" data-original-token="v2-df12253f4eb3afa67788e97902f009dd" data-rawheight="1184" data-rawwidth="2219" data-size="normal" src="images/v2-df12253f4eb3afa67788e97902f009dd_1440w.jpg" width="2219"/><figcaption>【23】</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><p data-pid="CigqiqQl"><b>RLOO 与 PPO 的区别和优点：</b></p><ol><li data-pid="-2z6ctzg"><b>模型副本数量</b>：<b>PPO 需要加载四个模型</b>副本，而 <b>RLOO 只需三个</b>（actor/reward/reference），减少了对 GPU 内存的需求。</li><li data-pid="Wg0AWHM2"><b>动作建模方式</b>：PPO 将每个生成的 token 视为单独的动作，而 RLOO 将整个生成序列视为一个动作。这种方式减少了稀疏奖励问题，提高了训练效率。</li><li data-pid="tVypmlym"><b>基线计算方法</b>：RLOO 使用批次中其他样本的奖励作为基线，避免了训练价值模型的需要，进一步简化了实现过程。</li><li data-pid="PFH35poG"><b>训练速度和内存使用</b>：根据实验，RLOO 在相同模型规模下，比 PPO 使用的 GPU 内存减少约 50-70%，训练速度提高 2-3 倍。【17】</li><li data-pid="SP9Sucv9"><b>性能表现</b>：在响应质量方面，RLOO 与 PPO 相当，并且始终优于一些流行的离线方法。</li></ol><p class="ztext-empty-paragraph"><br/></p><p data-pid="HJj1GER9">综上所述，RLOO 通过简化模型架构、降低计算资源需求和提高训练效率，提供了一种比 PPO 更加高效且易于实现的在线强化学习算法。</p><p class="ztext-empty-paragraph"><br/></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-14032877acc55b3d9d01ab3499f852f5_r.jpg" data-original-token="v2-14032877acc55b3d9d01ab3499f852f5" data-rawheight="1439" data-rawwidth="1080" data-size="normal" src="images/v2-14032877acc55b3d9d01ab3499f852f5_1440w.jpg" width="1080"/><figcaption>RLOO介绍【16】</figcaption></figure><p class="ztext-empty-paragraph"><br/></p><h2 data-into-catalog-status="" id="h_693582342_51">REINFORCE++【26】</h2><h3 data-into-catalog-status="" id="h_693582342_52">简介</h3><p data-pid="AswnOg3f"><b>REINFORCE++</b>是一种增强版的经典 <b>REINFORCE</b> 算法，结合了 <b>PPO</b> 的关键优化技术，同时去除了对 <b>critic network</b> 的依赖。</p><p data-pid="vnrCVO05"><b>REINFORCE++</b> 主要实现了三个核心目标：</p><ul><li data-pid="DjKiRVvJ">简洁性</li><li data-pid="MIs0FjsU">增强的训练稳定性</li><li data-pid="zgjj7l2n">降低计算开销。</li></ul><h3 data-into-catalog-status="" id="h_693582342_53">优点</h3><p data-pid="0xmR-yi8">论文中通过广泛的实证评估，证明了 <b>REINFORCE++</b> 在稳定性上优于 <b>GRPO</b>，并且在保持与 <b>PPO</b> 相当的性能的同时，实现了更高的计算效率。</p><ul><li data-pid="6akfFT8n"><b>通用场景（采用 Bradley-Terry Reward Models）</b>：<b>REINFORCE++</b> 在稳定性方面优于 <b>GRPO</b>，特别是在防止奖励和输出长度 <b>hacking</b> 方面（图 1）。</li><li data-pid="Fvjw1VYl"><b>基于规则的奖励模型（Rule-Based Reward Model）</b>：在基于规则的奖励场景下，<b>REINFORCE++</b> 与采用 <b>group normalization</b> 的 <b>GRPO</b> 取得了可比较的性能（图 2）。</li><li data-pid="mBC-UyI_"><b>数学奖励模型（Mathematical Reward Model）</b>：在数学问题求解场景中，<b>REINFORCE++</b> 相较于 <b>GRPO</b>，在每单位 <b>KL divergence</b> 下实现了更大的奖励提升（图 3）。</li></ul><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-6b91567c45c1943e6e35ec7a0da091f3_r.jpg" data-original-token="v2-6b91567c45c1943e6e35ec7a0da091f3" data-rawheight="756" data-rawwidth="1840" data-size="normal" src="images/v2-6b91567c45c1943e6e35ec7a0da091f3_1440w.jpg" width="1840"/><figcaption>图 1：通用领域结果表明，在采用 Bradley-Terry Reward Models 的一般场景中，PPO 和 REINFORCE++ 相较于 GRPO 具有较小的 length hacking 问题。</figcaption></figure><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-aad2c6070600ba224252b4e055186c61_r.jpg" data-original-token="v2-aad2c6070600ba224252b4e055186c61" data-rawheight="844" data-rawwidth="1830" data-size="normal" src="images/v2-aad2c6070600ba224252b4e055186c61_1440w.jpg" width="1830"/><figcaption>图 2：数学场景 1 表明，在基于规则的奖励下，REINFORCE++ 与 GRPO (Group Norm) 取得了可比较的结果。</figcaption></figure><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-original="https://picx.zhimg.com/v2-29d4f8b0094b902902649c0fae9f0b63_r.jpg" data-original-token="v2-29d4f8b0094b902902649c0fae9f0b63" data-rawheight="920" data-rawwidth="2002" data-size="normal" src="images/v2-29d4f8b0094b902902649c0fae9f0b63_1440w.jpg" width="2002"/><figcaption>图 3：数学场景 2 的结果表明，在相同单位 KL 消耗下，REINFORCE++ 和 RLOO 相较于 GRPO (Group Norm) 实现了更大的奖励提升。</figcaption></figure><h3 data-into-catalog-status="" id="h_693582342_54">优化方法</h3><p data-pid="PVVQq6jq">Reinforce++为提升训练稳定性和效率，融合了多种优化方法，从不同层面改进经典REINFORCE算法，在避免引入过多复杂度的同时，有效应对了RLHF面临的挑战。具体采用的方法如下： </p><p data-pid="UPlTIIfG">1. <b>Token-Level KL Penalty（令牌级KL散度惩罚）</b>：</p><p data-pid="RZTsL8gG">在RL模型和监督微调（SFT）模型的分布之间实施令牌级KL散度惩罚，并将其纳入奖励函数。公式为 <span class="ztext-math" data-eeimg="1" data-tex="r(s_{t}, a_{t}) = I(s_{t}=[EOS]) r(x, y)-\beta KL(t)">r(s_{t}, a_{t}) = I(s_{t}=[EOS]) r(x, y)-\beta KL(t)</span> ，其中</p><p data-pid="PbDAQgh2"><span class="ztext-math" data-eeimg="1" data-tex="KL(t)=log (\frac{\pi_{\theta_{old }}^{RL}(a_{t} | s_{t})}{\pi^{SFT}(a_{t} | s_{t})})">KL(t)=log (\frac{\pi_{\theta_{old }}^{RL}(a_{t} | s_{t})}{\pi^{SFT}(a_{t} | s_{t})})</span> </p><p data-pid="hWu3I4Yy">这种方式有助于更合理地分配奖励，并且能与过程奖励模型（PRM）无缝集成，提升训练的稳定性。</p><p data-pid="SDH7_qun">2. <b>PPO-Clip Integration（集成PPO的裁剪机制）</b>：</p><p data-pid="u9Z-Pyqv">引入近端策略优化（PPO）的裁剪机制来限制策略更新。通过公式 <span class="ztext-math" data-eeimg="1" data-tex="L^{CLIP}(\theta)=\mathbb{E}_{t}[min (r_{t}(\theta) \hat{A}_{t}, clip(r_{t}(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_{t})]">L^{CLIP}(\theta)=\mathbb{E}_{t}[min (r_{t}(\theta) \hat{A}_{t}, clip(r_{t}(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_{t})]</span> ，</p><p data-pid="DLfq6tUD">将概率比率 <span class="ztext-math" data-eeimg="1" data-tex="r_{t}(\theta)">r_{t}(\theta)</span> 限制在 <span class="ztext-math" data-eeimg="1" data-tex="[1-\epsilon, 1+\epsilon]">[1-\epsilon, 1+\epsilon]</span> 范围内（通常 <span class="ztext-math" data-eeimg="1" data-tex="\epsilon = 0.2">\epsilon = 0.2</span> ）。这使得算法既能利用正向优势，又能防止因过大的更新导致训练不稳定，维持了策略更新的信任区域。 </p><p data-pid="0vPxI8qK">3. <b>Mini-Batch Updates（小批量更新）</b>：</p><p data-pid="tUjh1-KX">通过小批量处理数据提升训练效率。具体表现为数据以较小的、可管理的块进行处理，而非全量更新；每个小批量允许进行多次参数更新，加快收敛速度；引入随机优化，增加了有益的随机性，有助于模型更好地泛化。 </p><p data-pid="xTPLZ4lS">4. <b>Reward Normalization and Clipping（奖励归一化和裁剪）</b>：</p><p data-pid="k1WX6tIV">对奖励进行全面处理以稳定训练。通过Z -score归一化来标准化奖励，减少异常值的影响；将奖励值限制在预定义的范围内，避免训练不稳定；应用适当的缩放因子，确保在更新过程中的数值稳定性。 </p><p data-pid="7XxZR0jq">5. <b>Advantage Normalization（优势归一化）</b>：</p><p data-pid="MKCzKa9R">Reinforce++中优势函数定义为 <span class="ztext-math" data-eeimg="1" data-tex="A_{t}(s_{t}, a_{t}) = r(x, y)-\beta \cdot \sum_{i=t}^{T} KL(i)">A_{t}(s_{t}, a_{t}) = r(x, y)-\beta \cdot \sum_{i=t}^{T} KL(i)</span> ，并使用z score归一化，即 <span class="ztext-math" data-eeimg="1" data-tex="A_{normalized }=\frac{A-\mu_{A}}{\sigma_{A}}">A_{normalized }=\frac{A-\mu_{A}}{\sigma_{A}}</span> </p><p data-pid="HSGdavsz">（ <span class="ztext-math" data-eeimg="1" data-tex="\mu_{A}">\mu_{A}</span> 和 <span class="ztext-math" data-eeimg="1" data-tex="\sigma_{A}">\sigma_{A}</span> 分别表示批次均值和标准差）。这种归一化方式保证了梯度的稳定性，防止训练过程中的发散。 </p><p class="ztext-empty-paragraph"><br/></p><h2 data-into-catalog-status="" id="h_693582342_55">参考资料</h2><p data-pid="6ajJAB4u"><a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03741" rel="nofollow noreferrer" target="_blank">【1】Deep Reinforcement Learning from Human Preferences</a></p><p data-pid="bIS0O91Z"><a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.02155" rel="nofollow noreferrer" target="_blank">【2】Training language models to follow instructions with human feedback</a></p><p data-pid="el2XOpmT"><a class="wrap external" href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1R94y1P7QX/%3Fshare_source%3Dcopy_web%26vd_source%3D4029b709c8dab921079939a0bdd5ec6c" rel="nofollow noreferrer" target="_blank">【3】吴恩达《从人类反馈中进行强化学习RLHF, Reinforcement Learning from Human Feedback》（中英字幕）_哔哩哔哩_bilibili</a></p><p data-pid="PSswR0kl">【4】大规模语言模型从理论到实践</p><p data-pid="50sfqv55">【5】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/tG_ktQ0WbZHQavtoJtaXbw" rel="nofollow noreferrer" target="_blank">从0到1构建RLHF系统——小红书大模型团队的探索与实践</a></p><p data-pid="vX7a5t1l">【6】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/end/p/17481052.html" rel="nofollow noreferrer" target="_blank">为什么RLHF中，PPO需要Critic模型而不是直接使用RewardModel - 风生水起 - 博客园</a></p><p data-pid="eblBdO1I">【7】<a class="internal" href="https://zhuanlan.zhihu.com/p/14888098807" target="_blank">初七123334：RLHF 对齐之 REINFORCE++ 算法 - 比 GRPO 稳定比PPO快</a></p><p data-pid="RwwEsgBn">【8】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//paperexplained.cn/aplayground/iarticle/detail/0454c3b5-be1a-4aff-a146-9c5adaf76600/" rel="nofollow noreferrer" target="_blank">演练 - 强化学习经典算法实验之REINFORCE - 字舞流文</a></p><p data-pid="w9Nbf6Uw">【9】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit%23slide%3Did.g2650ce3df47_0_0" rel="nofollow noreferrer" target="_blank">openRLHF slides</a></p><p data-pid="Z9ZLOTFp">【10】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/shizheng_Li/article/details/144468429" rel="nofollow noreferrer" target="_blank">重要性采样详解及其在PPO算法中的应用</a></p><p data-pid="xXHec9xk">【11】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2305.18290" rel="nofollow noreferrer" target="_blank">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p><p data-pid="9nz55F2p">【12】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2402.03300" rel="nofollow noreferrer" target="_blank">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></p><p data-pid="39QS92qh">【13】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2402.14740" rel="nofollow noreferrer" target="_blank">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</a></p><p data-pid="Md2k2Td7">【14】<a class="external" href="https://link.zhihu.com/?target=https%3A//deepreinforcementlearningbook.org/assets/pdfs/ch3.pdf" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">deepreinforcementlearningbook.org</span><span class="invisible">/assets/pdfs/ch3.pdf</span><span class="ellipsis"></span></a></p><p data-pid="Mt261kK0">【15】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//imxwell.com/blog/rl_basic/" rel="nofollow noreferrer" target="_blank">强化学习基本问题回顾总结 | Xwell's Blog</a></p><p data-pid="5vJKQcF_">【16】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//www.xiaohongshu.com/explore/6732c5ca000000001a01d343%3Fxsec_token%3DABO2vbisf8d7CC4oYpTya1zwMkKyW-ipv7a-PShemePLo%3D%26xsec_source%3Dpc_search%26source%3Dunknown" rel="nofollow noreferrer" target="_blank">RLOO：比PPO更适合LLM的RLHF方法 - 小红书</a></p><p data-pid="91k-MusE">【17】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//huggingface.co/blog/zh/putting_rl_back_in_rlhf_with_rloo%3Futm_source%3Dchatgpt.com" rel="nofollow noreferrer" target="_blank">putting rl back in rlhf with rloo</a></p><p data-pid="Ue43aV1e">【18】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//hijkzzz.notion.site/rlhf-implementation-tricks%3Fv%3D158d9a33ecc98132bf9e000c39227361" rel="nofollow noreferrer" target="_blank">Advanced Tricks for Training Large Language Models with Proximal Policy Optimization</a></p><p data-pid="1ahjqAdD">【19】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.12036" rel="nofollow noreferrer" target="_blank">A General Theoretical Paradigm to Understand Learning from Human Preferences</a></p><p data-pid="-oEF62Vt">【20】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2402.01306" rel="nofollow noreferrer" target="_blank">KTO: Model Alignment as Prospect Theoretic Optimization</a></p><p data-pid="qCX_dRWM">【21】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2112.00861" rel="nofollow noreferrer" target="_blank">A general language assistant as a laboratory for alignment</a></p><p data-pid="skNFbXma">【22】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.06452" rel="nofollow noreferrer" target="_blank">Understanding the effects of rlhf on llm generalisation and diversity</a></p><p data-pid="WysHclQw">【23】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1PSDmYkEj1" rel="nofollow noreferrer" target="_blank">RLOO一作讲解RLOO</a></p><p data-pid="XQDZhMdL">【24】<a class="external" href="https://link.zhihu.com/?target=https%3A//openai.com/index/faulty-reward-functions/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">openai.com/index/faulty</span><span class="invisible">-reward-functions/</span><span class="ellipsis"></span></a></p><p data-pid="VLBmFsCi">【25】<a class="internal" href="https://zhuanlan.zhihu.com/p/675329917" target="_blank">何枝：【RLHF】RL 究竟是如何与 LLM 做结合的？</a></p><p data-pid="0Q5PuYak">【26】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2501.03262" rel="nofollow noreferrer" target="_blank">REINFORCE++: A SIMPLE AND EFFICIENT APPROACH FOR ALIGNING LARGE LANGUAGE MODELS</a></p><p data-pid="CadJs6c4">【27】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.14476" rel="nofollow noreferrer" target="_blank">DAPO: An Open-Source LLM Reinforcement Learning System at Scale</a></p><p data-pid="K9uaE6Ru">【28】<a class="internal" href="https://zhuanlan.zhihu.com/p/1888687830710084683" target="_blank">大家好我是爱因：DAPO：GRPO的问题分析及四个改进策略</a></p><p data-pid="eJFCSR8W">【29】<a class="wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.20783" rel="nofollow noreferrer" target="_blank">Understanding R1-Zero-Like Training: A Critical Perspective</a></p><p data-pid="uajb5aoW">【30】<a class="internal" href="https://zhuanlan.zhihu.com/p/1891404563971564359" target="_blank">大家好我是爱因：理解 R1-Zero 式训练：一个批判性视角</a></p></div><span id="VirtualCatalogAnchorPoint"></span></span></div></div></div><div class="Reward"><div><button class="Reward-rewardBtn"><style data-emotion-css="zkfaav">.css-zkfaav{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-zkfaav"><svg class="ZDI ZDI--Gift24" fill="currentColor" height="18" viewbox="0 0 24 24" width="18"><path clip-rule="evenodd" d="M17.24 5.768c.522-2.089-1.397-3.968-3.474-3.401-.892.243-1.6.905-1.909 1.76a2.675 2.675 0 0 0-1.768-1.714C8.102 1.817 6.222 3.61 6.725 5.624l.172.689H4.796A2.546 2.546 0 0 0 2.25 8.859v1.814c0 .92.488 1.726 1.219 2.173v6.765a2.546 2.546 0 0 0 2.546 2.545h11.97a2.546 2.546 0 0 0 2.546-2.545v-6.765a2.544 2.544 0 0 0 1.219-2.173V8.86a2.546 2.546 0 0 0-2.546-2.546h-2.1l.135-.545Zm1.666 7.451H12.812v7.312h5.173a.92.92 0 0 0 .921-.92v-6.392Zm-7.718 0H5.094v6.392c0 .508.412.92.92.92H11.189V13.22Zm4.24-6.906h-2.415l.33-1.487a1.192 1.192 0 1 1 2.32.547l-.235.94Zm.634 1.625H19.204a.92.92 0 0 1 .921.92v1.815a.92.92 0 0 1-.92.921H4.794a.92.92 0 0 1-.92-.92V8.858a.92.92 0 0 1 .92-.921h11.268Zm-7.49-1.625h2.3l-.556-1.67a1.05 1.05 0 0 0-2.015.587l.27 1.083Z" fill-rule="evenodd"></path></svg><style data-emotion-css="169bzww">.css-169bzww{font-weight:500;margin-left:2px;}</style><style data-emotion-css="71c0x4">.css-71c0x4{box-sizing:border-box;margin:0;min-width:0;font-weight:500;margin-left:2px;}</style><div class="css-71c0x4">送礼物</div></div></button></div><style data-emotion-css="1vbn52a">.css-1vbn52a{box-sizing:border-box;margin:0;min-width:0;margin-bottom:60px;line-height:19px;text-align:center;color:#adb0b7;font-size:13px;}</style><div class="Reward-countZero css-1vbn52a">还没有人送礼物，鼓励一下作者吧</div><style data-emotion-css="1qckg6r">.css-1qckg6r{width:442px;}.css-1qckg6r .Modal-content{padding:16px 20px 32px;margin:0;}</style></div><div class="ContentItem-time" role="button" tabindex="0">编辑于 2025-04-05 15:16<!-- -->・<!-- -->上海</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><style data-emotion-css="ch8ocw">.css-ch8ocw{position:relative;display:inline-block;height:30px;padding:0 12px;font-size:14px;line-height:30px;color:#1772F6;vertical-align:top;border-radius:100px;background:rgba(23,114,246,0.1);}.css-ch8ocw:hover{background-color:rgba(23,114,246,0.15);}</style><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><style data-emotion-css="1xlfegr">.css-1xlfegr{background:transparent;box-shadow:none;}</style><style data-emotion-css="1gomreu">.css-1gomreu{position:relative;display:inline-block;}</style><div class="css-1gomreu">强化学习 (Reinforcement Learning)</div></a></span></div><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/25402720" target="_blank"><div class="css-1gomreu">大模型</div></a></span></div><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/27239249" target="_blank"><div class="css-1gomreu">多模态大模型</div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><span><button aria-label="赞同 335 " aria-live="polite" class="Button VoteButton FEfUrdfMIKpQDJDqkjte" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" height="10" viewbox="0 0 24 24" width="10"><path clip-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" fill-rule="evenodd"></path></svg></span>赞同 335</button></span><button aria-label="反对" aria-live="polite" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" height="10" viewbox="0 0 24 24" width="10"><path clip-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" fill-rule="evenodd"></path></svg></span></button></span><button class="Button BottomActions-CommentBtn FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path clip-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" fill-rule="evenodd"></path></svg></span>13 条评论</button><div class="Popover ShareMenu"><div aria-expanded="false" aria-haspopup="true" class="ShareMenu-toggler" id="null-toggle"><button class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="ZDI ZDI--PaperplaneFill24 Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button aria-live="polite" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path clip-rule="evenodd" d="M17.142 3.041c1.785.325 3.223 1.518 4.167 3.071 1.953 3.215.782 7.21-1.427 9.858a23.968 23.968 0 0 1-4.085 3.855c-.681.5-1.349.923-1.962 1.234-.597.303-1.203.532-1.748.587a.878.878 0 0 1-.15.002c-.545-.04-1.162-.276-1.762-.582a14.845 14.845 0 0 1-2.008-1.27 24.254 24.254 0 0 1-4.21-4.002c-2.1-2.56-3.16-6.347-1.394-9.463.92-1.624 2.362-2.892 4.173-3.266 1.657-.341 3.469.097 5.264 1.44 1.75-1.309 3.516-1.76 5.142-1.464Z" fill-rule="evenodd"></path></svg></span>喜欢</button><button class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Deliver Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><g clip-rule="evenodd" fill-rule="evenodd"><path d="M7.821 12a.75.75 0 0 1 .75-.75h6.857a.75.75 0 0 1 0 1.5H8.571a.75.75 0 0 1-.75-.75ZM8.965 8a.75.75 0 0 1 .75-.75h4.571a.75.75 0 0 1 0 1.5H9.715a.75.75 0 0 1-.75-.75Z"></path><path d="M7.527 3.15a2.35 2.35 0 0 0-2.309 1.91L3.165 15.84a.85.85 0 0 0-.015.16v2.5a2.35 2.35 0 0 0 2.35 2.35h13a2.35 2.35 0 0 0 2.35-2.35V16a.848.848 0 0 0-.015-.16L18.78 5.06a2.35 2.35 0 0 0-2.308-1.91H7.527Zm0 1.7a.65.65 0 0 0-.639.528l-1.88 9.872h13.984l-1.88-9.872a.65.65 0 0 0-.64-.528H7.528Z"></path></g></svg></span>申请转载</button><div class="Post-ActionMenuButton"><div class="Popover"><div aria-expanded="false" aria-haspopup="true" id="null-toggle"><button class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path d="M6 10.5a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3ZM10.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0ZM16.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z"></path></svg></span></button></div></div></div></div></div></div></article></div></div><style data-emotion-css="1qyytj7">.css-1qyytj7{max-width:296px;-webkit-align-self:normal;-ms-flex-item-align:normal;align-self:normal;}</style><div class="Post-Row-Content-right css-1qyytj7"><style data-emotion-css="cejtq2">.css-cejtq2{display:block;position:relative;top:-50px;width:0;height:0;}</style><a aria-keyshortcuts="Shift+S" aria-label="边栏锚点" class="css-cejtq2"></a><div></div><div style="position:sticky;top:62px"><div aria-label="关于作者" class="Card AuthorCard" role="complementary"><div class="Card-header AuthorCard-title"><div class="Card-headerText">关于作者</div></div><div class="Card-section"><div class="AuthorCard-user"><div class="AuthorCard-user-avatar"><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/iamein" target="_blank"><style data-emotion-css="1o44jgu">.css-1o44jgu{dynamic-range-limit:standard;border-radius:4px;}</style><style data-emotion-css="1qhd7fe">.css-1qhd7fe{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:60px;height:60px;dynamic-range-limit:standard;border-radius:4px;}</style><img alt="大家好我是爱因" class="Avatar UserLink-avatar css-1qhd7fe" src="images/v2-c5d9627d9e98fd98830eeced564275cb_l.jpg" srcset="https://picx.zhimg.com/v2-c5d9627d9e98fd98830eeced564275cb_xl.jpg?source=172ae18b 2x"/></a></span></div><div class="AuthorCard-user-content"><div class="AuthorCard-user-name"><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/iamein" target="_blank">大家好我是爱因</a></span></div></div></div></div><div class="Card-section"></div><div class="Card-section"><div class="AuthorCard-counts"><div class="NumberBoard"><a class="NumberBoard-item" data-za-detail-view-element_name="Answer" href="//www.zhihu.com/people/iamein/answers" preset="plain"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">回答</div><strong class="NumberBoard-itemValue"></strong></div></a><a class="NumberBoard-item" data-za-detail-view-element_name="Post" href="//www.zhihu.com/people/iamein/posts" preset="plain"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">文章</div><strong class="NumberBoard-itemValue"></strong></div></a><a class="NumberBoard-item" data-za-detail-view-element_name="Follower" href="//www.zhihu.com/people/iamein/followers" preset="plain"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue"></strong></div></a></div></div><div class="MemberButtonGroup AuthorCard-buttons"><button class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Plus FollowButton-icon" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path clip-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" fill-rule="evenodd"></path></svg></span>关注他</button><button class="Button FEfUrdfMIKpQDJDqkjte Button--grey Button--withIcon Button--withLabel ZdfrHW7Ef5ZjwFiiBJuS B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp" type="button"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comments Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor" height="1.2em" viewbox="0 0 24 24" width="1.2em"><path clip-rule="evenodd" d="M2 11c0 1.79.553 3.45 1.498 4.82L2.6 18.667a.6.6 0 0 0 .751.753l3.07-.96A8.5 8.5 0 1 0 2 11Zm11.46 9.414c-.457.16-.506.794-.034.904A6.96 6.96 0 0 0 15 21.5c1.148 0 2.422-.31 3.444-.912.357-.217.658-.378 1.043-.252l1.414.42c.357.112.679-.168.574-.546l-.47-1.57a.736.736 0 0 1 .05-.632c.602-1.108.945-2.32.945-3.498 0-1.07-.248-2.11-.7-3.046-.21-.435-.815-.25-.872.23-.47 3.954-3.211 7.394-6.968 8.72Z" fill-rule="evenodd"></path></svg></span>发私信</button></div></div></div></div></div></div><div class="Post-Sub Post-NormalSub"></div></div></main></div></div><script id="js-clientConfig" type="text/json">{"fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhida":"https:\u002F\u002Fzhida.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fapi\u002F","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com","openapi":"https:\u002F\u002Fopenapi.zhihu.com","svip":"https:\u002F\u002Fsvip.zhihu.com","event":"https:\u002F\u002Fevent.zhihu.com","en":"https:\u002F\u002Fen.zhihu.com"},"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","enHost":"en.zhihu.com","videoHost":"video.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","allowSignUp":true,"refreshValidityPeriod":"30","release":"2970-4caf5ee5","currentEntry":"column","isMobileEntry":false,"apollo":{"env":"prod","globalSilence":"","logMobileUnAuth":"1","topstory_rec_adp":"1","topstory_hot_adp":"1","editor_adapt_native":"0","editor_auto_rotate":"0","enable_request_filter":"1","balanceModalSign":"ChYHAcB5ihJECkAFnhDAYmdhsTWVJoNc","reportBackendPublishError":"1","dynamic_font_schema":"1","enable_vzuu_login":"office","za_zse_ck_refer":"1","gamekeeper_office":"blue","gamekeeper_production":"blue","video_upload_config":"{\"parallel\":10,\"timeout\":0,\"partSize\":20971520,\"endpoint\":\"https:\u002F\u002Fzhihu-video-input.oss-cn-hangzhou.aliyuncs.com\"}","ring_switch":"0","apology_statement":"0","za_sdk_version":"5.3.3","concurrent_config":"[[\"heifetz-topstory\",40],[\"heifetz-column\",0],[\"heifetz-other\",0],[\"heifetz-mobile\",40],[\"heifetz\",35],[\"heifetz-prime\",40]]","new_ai_assistant":"1","enable_pin_emoticon":"1","enable_assets_tracker":"[[\"office\", 1],[\"production\",0.02]]","app_oia_config":"{\"baidu_app\": {\"oia_modal\": \"1\", \"auto_pull\": \"1\"}, \"wx_app\": {\"oia_modal\": \"0\", \"auto_pull\": \"0\"}}","enable_bilibili_account_association":"1","ssr_11236":"0","pinDraftRemote":"1","test_canary":"member|0-100,1-0","use_new_player":"member|0-100,1-0","player_vendor":"member|0-100,1-0,2-0","use_hevc":"member|0-0,1-100","upload_use_signature":"member|0-0,1-100","use_backdrop_blur":"member|0-0,1-100","article_title_imagex":"member|0-0,1-100","play_station":"member|0-0,1-100","use_cached_supported_countries":"device|1-100,0-0","pc_favorite_toast":"member|0-0,1-100","pc_article_page_favorite_toast":"member|0-0,1-100","comment_v4":"member|0-100,1-0","segment_interaction":"member|0-0,1-100","contentItem_cover_imagex":"member|0-0,1-100","use_qrcode_login_v2":"device|1-100,0-0","inline_pin_editor":"1"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{},"cities":{"cityData":[]}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false,"article\u002FloadPostSearchEntity\u002F":false}},"entities":{"users":{"1f1705dc0d3c88feeb3767a8db29edea":{"uid":46539477090304,"userType":"people","id":"1f1705dc0d3c88feeb3767a8db29edea"},"iamein":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb.jpg?source=172ae18b","uid":"1019333493377122304","userType":"people","isFollowing":false,"urlToken":"iamein","id":"e03c883e599808250b135d35b0dc9c24","description":"","name":"大家好我是爱因","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002Fe03c883e599808250b135d35b0dc9c24","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b","isOrg":false,"type":"people","badge":[],"badgeV2":{"title":"LLM\u002FMLLM","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"0","medalName":"","avatarUrl":"","miniAvatarUrl":"","description":"","medalAvatarFrame":null}}},"questions":{},"answers":{},"articles":{"693582342":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fcontent_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGZQAmV0B12vrIz3fjfFlw==&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"entityWords":[{"name":"Actor-Critic架构","mention":"Actor-Critic架构","matchorder":1,"begin":19710,"end":19724,"entityid":-5088351489332100000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=Actor-Critic%E6%9E%B6%E6%9E%84&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0019438621545759815,"attachedInfoBytes":"sgJoChJBY3Rvci1Dcml0aWPmnrbmnoQSB1Vua25vd24Y\u002FpkBIIyaASgBNTHJ\u002Fjo6B2FydGljbGVAv4mf0PnoorG5AUgKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAuIBCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoSQWN0b3ItQ3JpdGlj5p625p6EQL+Jn9D56KKxuQFIAFIWYWlfZW50aXR5X2xsbV9yZWNhbGxlcl0xyf46YmgKEkFjdG9yLUNyaXRpY+aetuaehBIHVW5rbm93bhj+mQEgjJoBKAE1Mcn+OjoHYXJ0aWNsZUC\u002FiZ\u002FQ+eiisbkBSApSJGY1NjZmYjBiLWM3MjEtNDMyMC1hYmZjLTIwNDUwZjU5ODA4Zg==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"马尔可夫决策过程","mention":"马尔可夫决策过程","matchorder":1,"begin":1093,"end":1101,"entityid":-1158775295771903500,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0016972067674032587,"attachedInfoBytes":"sgJsChjpqazlsJTlj6\u002FlpKvlhrPnrZbov4fnqIsSB1Vua25vd24YxQggzQgoATXPdN46OgdhcnRpY2xlQMXzkciggM317wFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm8gLsAQokZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhmGgdhcnRpY2xlIgkyNDIyNTk2ODc6GOmprOWwlOWPr+Wkq+WGs+etlui\u002Fh+eoi0DF85HIoIDN9e8BSAFSFmFpX2VudGl0eV9sbG1fcmVjYWxsZXJdz3TeOmJsChjpqazlsJTlj6\u002FlpKvlhrPnrZbov4fnqIsSB1Vua25vd24YxQggzQgoATXPdN46OgdhcnRpY2xlQMXzkciggM317wFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"时序差分方法","mention":"时序差分方法","matchorder":1,"begin":12139,"end":12145,"entityid":-3920007170446424000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0012703217214493634,"attachedInfoBytes":"sgJmChLml7bluo\u002Flt67liIbmlrnms5USB1Vua25vd24Y614g8V4oATXsgKY6OgdhcnRpY2xlQNyRuoT2x9XMyQFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm8gLgAQokZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhmGgdhcnRpY2xlIgkyNDIyNTk2ODc6EuaXtuW6j+W3ruWIhuaWueazlUDckbqE9sfVzMkBSANSFmFpX2VudGl0eV9sbG1fcmVjYWxsZXJd7ICmOmJmChLml7bluo\u002Flt67liIbmlrnms5USB1Vua25vd24Y614g8V4oATXsgKY6OgdhcnRpY2xlQNyRuoT2x9XMyQFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"优势函数","mention":"优势函数","matchorder":1,"begin":5337,"end":5341,"entityid":-6361908750467538000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0012101564145723387,"attachedInfoBytes":"sgJgCgzkvJjlir\u002Flh73mlbASB1Vua25vd24Y2Skg3SkoATUcnp46OgdhcnRpY2xlQK\u002FGu5estv3apwFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm8gLUAQokZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhmGgdhcnRpY2xlIgkyNDIyNTk2ODc6DOS8mOWKv+WHveaVsECvxruXrLb92qcBSARSFmFpX2VudGl0eV9sbG1fcmVjYWxsZXJdHJ6eOmJgCgzkvJjlir\u002Flh73mlbASB1Vua25vd24Y2Skg3SkoATUcnp46OgdhcnRpY2xlQK\u002FGu5estv3apwFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"动作价值函数","mention":"动作价值函数","matchorder":1,"begin":597,"end":603,"entityid":1593133836363380000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0010867358880659594,"attachedInfoBytes":"sgJlChLliqjkvZzku7flgLzlh73mlbASB1Vua25vd24Y1QQg2wQoATXOcI46OgdhcnRpY2xlQKnq4OCI2fyNFkgKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAt4BCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoS5Yqo5L2c5Lu35YC85Ye95pWwQKnq4OCI2fyNFkgFUhZhaV9lbnRpdHlfbGxtX3JlY2FsbGVyXc5wjjpiZQoS5Yqo5L2c5Lu35YC85Ye95pWwEgdVbmtub3duGNUEINsEKAE1znCOOjoHYXJ0aWNsZUCp6uDgiNn8jRZIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"蒙特卡洛方法","mention":"蒙特卡洛方法","matchorder":1,"begin":11574,"end":11580,"entityid":8012154036438428000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0017689750552563055,"attachedInfoBytes":"sgJlChLokpnnibnljaHmtJvmlrnms5USB1Vua25vd24YtlogvFooATX03Oc6OgdhcnRpY2xlQJnP\u002F9zV9LiYb0gKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAt4BCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoS6JKZ54m55Y2h5rSb5pa55rOVQJnP\u002F9zV9LiYb0gGUhZhaV9lbnRpdHlfbGxtX3JlY2FsbGVyXfTc5zpiZQoS6JKZ54m55Y2h5rSb5pa55rOVEgdVbmtub3duGLZaILxaKAE19NznOjoHYXJ0aWNsZUCZz\u002F\u002Fc1fS4mG9IClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"off-policy","mention":"off-policy","matchorder":1,"begin":6400,"end":6410,"entityid":-2803037510259953000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=off-policy&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0015074147749487565,"attachedInfoBytes":"sgJeCgpvZmYtcG9saWN5EgdVbmtub3duGIAyIIoyKAE1cpTFOjoHYXJ0aWNsZUDE5fDmxIfnjNkBSApSJGY1NjZmYjBiLWM3MjEtNDMyMC1hYmZjLTIwNDUwZjU5ODA4ZvIC0AEKJGY1NjZmYjBiLWM3MjEtNDMyMC1hYmZjLTIwNDUwZjU5ODA4ZhoHYXJ0aWNsZSIJMjQyMjU5Njg3OgpvZmYtcG9saWN5QMTl8ObEh+eM2QFIB1IWYWlfZW50aXR5X2xsbV9yZWNhbGxlcl1ylMU6Yl4KCm9mZi1wb2xpY3kSB1Vua25vd24YgDIgijIoATVylMU6OgdhcnRpY2xlQMTl8ObEh+eM2QFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"贝尔曼方程","mention":"贝尔曼方程","matchorder":1,"begin":6446,"end":6451,"entityid":3107897666190848500,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.001502930529227342,"attachedInfoBytes":"sgJiCg\u002FotJ3lsJTmm7zmlrnnqIsSB1Vua25vd24YrjIgszIoATX7\u002FcQ6OgdhcnRpY2xlQOPlyLzgjt6QK0gKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAtgBCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoP6LSd5bCU5pu85pa556iLQOPlyLzgjt6QK0gIUhZhaV9lbnRpdHlfbGxtX3JlY2FsbGVyXfv9xDpiYgoP6LSd5bCU5pu85pa556iLEgdVbmtub3duGK4yILMyKAE1+\u002F3EOjoHYXJ0aWNsZUDj5ci84I7ekCtIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"on-policy","mention":"on-policy","matchorder":1,"begin":6386,"end":6395,"entityid":524073933189159000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=on-policy&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0014881187247510752,"attachedInfoBytes":"sgJcCglvbi1wb2xpY3kSB1Vua25vd24Y8jEg+zEoATX7DMM6OgdhcnRpY2xlQLuwoP7yzviiB0gKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAswBCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoJb24tcG9saWN5QLuwoP7yzviiB0gJUhZhaV9lbnRpdHlfbGxtX3JlY2FsbGVyXfsMwzpiXAoJb24tcG9saWN5EgdVbmtub3duGPIxIPsxKAE1+wzDOjoHYXJ0aWNsZUC7sKD+8s74ogdIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"广义优势估计","mention":"广义优势估计","matchorder":1,"begin":11469,"end":11475,"entityid":-5382038216749186000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF%E4%BC%B0%E8%AE%A1&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0013156413414208146,"attachedInfoBytes":"sgJmChLlub\u002FkuYnkvJjlir\u002FkvLDorqESB1Vua25vd24YzVkg01koATWZcaw6OgdhcnRpY2xlQNWL2pidmMqntQFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm8gLgAQokZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhmGgdhcnRpY2xlIgkyNDIyNTk2ODc6EuW5v+S5ieS8mOWKv+S8sOiuoUDVi9qYnZjKp7UBSApSFmFpX2VudGl0eV9sbG1fcmVjYWxsZXJdmXGsOmJmChLlub\u002FkuYnkvJjlir\u002FkvLDorqESB1Vua25vd24YzVkg01koATWZcaw6OgdhcnRpY2xlQNWL2pidmMqntQFIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"价值函数","mention":"价值函数","matchorder":1,"begin":569,"end":573,"entityid":4165443941006797300,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=242259687&content_type=Article&match_order=1&q=%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.000926013934929415,"attachedInfoBytes":"sgJfCgzku7flgLzlh73mlbASB1Vua25vd24YuQQgvQQoATW+v3I6OgdhcnRpY2xlQLLTjZyepqjnOUgKUiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGbyAtIBCiRmNTY2ZmIwYi1jNzIxLTQzMjAtYWJmYy0yMDQ1MGY1OTgwOGYaB2FydGljbGUiCTI0MjI1OTY4NzoM5Lu35YC85Ye95pWwQLLTjZyepqjnOUgLUhZhaV9lbnRpdHlfbGxtX3JlY2FsbGVyXb6\u002FcjpiXwoM5Lu35YC85Ye95pWwEgdVbmtub3duGLkEIL0EKAE1vr9yOjoHYXJ0aWNsZUCy042cnqao5zlIClIkZjU2NmZiMGItYzcyMS00MzIwLWFiZmMtMjA0NTBmNTk4MDhm","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""}],"id":"693582342","title":"大模型中的强化学习","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F693582342","imageUrl":"","titleImage":"","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-25e2a31c9d52d9a6c59018346f99c304_200x112.png\" data-caption=\"强化学习问题示意图【15】\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"434\" data-watermark=\"original\" data-original-src=\"v2-25e2a31c9d52d9a6c59018346f99c304\" data-watermark-src=\"v2-089ec189ea04a252dd383fd46a27585b\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-25e2a31c9d52d9a6c59018346f99c304_r.png\"\u002F\u003E\u003Cb\u003E本文主要介绍强化学习（RL）的关键背景、RL和LLM的结合，以及各种RLHF算法。\u003C\u002Fb\u003E\u003Cb\u003E背景\u003C\u002Fb\u003E\u003Cb\u003E什么是强化学习？\u003C\u002Fb\u003E \u003Cb\u003E强化学习（Reinforcement Learning, RL）\u003C\u002Fb\u003E 是机器学习的一个分支，目标是让\u003Cb\u003E智能体（agent）\u003C\u002Fb\u003E通过与\u003Cb\u003E环境（environment）\u003C\u002Fb\u003E的交互来学习最优的\u003Cb\u003E行为策略（policy）\u003C\u002Fb\u003E…","created":1713601427,"updated":1743837419,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb.jpg?source=172ae18b","uid":"1019333493377122304","userType":"people","isFollowing":false,"urlToken":"iamein","id":"e03c883e599808250b135d35b0dc9c24","description":"","name":"大家好我是爱因","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002Fe03c883e599808250b135d35b0dc9c24","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b","isOrg":false,"type":"people","badge":[],"badgeV2":{"title":"LLM\u002FMLLM","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"0","medalName":"","avatarUrl":"","miniAvatarUrl":"","description":"","medalAvatarFrame":null}},"commentPermission":"all","copyrightPermission":"need_review","state":"published","ipInfo":"上海","imageWidth":0,"imageHeight":0,"content":"\u003Cp data-pid=\"8SU2-G0A\"\u003E\u003Cb\u003E本文主要介绍强化学习（RL）的关键背景、RL和LLM的结合，以及各种RLHF算法。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E背景\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch3\u003E\u003Cb\u003E什么是强化学习？\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"7ZTDwKdX\"\u003E\u003Cb\u003E强化学习（Reinforcement Learning, RL）\u003C\u002Fb\u003E 是机器学习的一个分支，目标是让\u003Cb\u003E智能体（agent）\u003C\u002Fb\u003E通过与\u003Cb\u003E环境（environment）\u003C\u002Fb\u003E的交互来学习最优的\u003Cb\u003E行为策略（policy）\u003C\u002Fb\u003E，从而最大化某个\u003Cb\u003E累积回报（cumulative reward）\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"bJLuzXla\"\u003E其\u003Cb\u003E核心思想是通过试错和反馈的机制\u003C\u002Fb\u003E，找到在每个情境下的最优决策。\u003C\u002Fp\u003E\u003Cp data-pid=\"ra2iblgq\"\u003E强化学习的\u003Cb\u003E优化目标\u003C\u002Fb\u003E是\u003Cb\u003E通过选择策略来最大化累积奖励\u003C\u002Fb\u003E。 具体来说，智能体的目标是寻找一个最优策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi%5E%2A\" alt=\" \\pi^*\" eeimg=\"1\"\u002F\u003E ，使得它在各个状态下的累积回报最大。形式上，可以用\u003Cb\u003E价值函数\u003C\u002Fb\u003E（Value Function）或\u003Cb\u003E动作价值函数\u003C\u002Fb\u003E（Q函数）来表示。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-25e2a31c9d52d9a6c59018346f99c304_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"434\" data-original-token=\"v2-25e2a31c9d52d9a6c59018346f99c304\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-25e2a31c9d52d9a6c59018346f99c304_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E强化学习问题示意图【15】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E\u003Cb\u003E强化学习的要素\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"zfEuI6tX\"\u003E在强化学习中，我们通常将问题描述为一个\u003Cb\u003E马尔可夫决策过程\u003C\u002Fb\u003E（Markov Decision Process, MDP），它包含以下几个关键元素：\u003C\u002Fp\u003E\u003Cp data-pid=\"mfRCerH8\"\u003E 1. \u003Cb\u003E状态（State, S）\u003C\u002Fb\u003E：智能体所处的环境状态，可能是游戏画面的一帧，也可能是机器人观测到的传感器数据。\u003C\u002Fp\u003E\u003Cp data-pid=\"zCsxg5f-\"\u003E 2. \u003Cb\u003E动作（Action, A）\u003C\u002Fb\u003E：智能体在给定状态下可以执行的一系列操作。\u003C\u002Fp\u003E\u003Cp data-pid=\"LH30wG2L\"\u003E 3. \u003Cb\u003E状态转移（Transition Dynamics, P）\u003C\u002Fb\u003E：从当前状态  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s+\" alt=\"s \" eeimg=\"1\"\u002F\u003E 采取动作  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E  后，会以一定的概率转移到下一个状态  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%7B%5Cprime%7D\" alt=\"s{\\prime}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp data-pid=\"MS-gxlqN\"\u003E 4. \u003Cb\u003E奖励（Reward, R）\u003C\u002Fb\u003E：智能体在每个时刻（或每次转移）获得的即时回报，反映了该动作在此状态下的好坏。\u003C\u002Fp\u003E\u003Cp data-pid=\"2Pan6IAM\"\u003E 5. \u003Cb\u003E折扣因子（Discount Factor, \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E）\u003C\u002Fb\u003E：用于平衡当前奖励和未来奖励的重要性，数值通常在 (0, 1] 之间。\u003C\u002Fp\u003E\u003Cp data-pid=\"4fonFCRO\"\u003E 6. \u003Cb\u003E轨迹（Trajectory）：\u003C\u002Fb\u003E在强化学习中，智能体从环境的初始状态开始，与环境交互直至到达终止状态所经历的一系列状态、动作以及相应的奖励就构成了一条完整的“轨迹”。形式上可以表示为： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctau+%3D++%5C%7Bs_0%2C+a_0%2C+r_1%2C+s_1%2C+a_1%2C+r_2%2C+%5Cdots%2C+s_%7BT-1%7D%2C+a_%7BT-1%7D%2C+r_T%2C+s_T%5C%7D\" alt=\"\\tau =  \\{s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T, s_T\\}\" eeimg=\"1\"\u002F\u003E 。其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 表示回合（Episode）结束的时间步。在某些不定长的任务中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T+\" alt=\"T \" eeimg=\"1\"\u002F\u003E 可能是一个随机值。轨迹可以帮助我们理解智能体如何从起始状态一步步演化到最终状态，并获取相应的奖励序列。\u003C\u002Fp\u003E\u003Cp data-pid=\"qawTlVL8\"\u003E7. \u003Cb\u003E经验（Experience）：\u003C\u002Fb\u003E智能体可与环境交互多次，进行多次实验，形成多个轨迹。多个轨迹的集合被称为经验。即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=E+%3D+%5C%7B%5Ctau_1%2C+%5Ctau_2%2C+...%2C+%5Ctau_K+%5C%7D\" alt=\"E = \\{\\tau_1, \\tau_2, ..., \\tau_K \\}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp data-pid=\"6ILlc7iU\"\u003E8. \u003Cb\u003E回报（Return）：\u003C\u002Fb\u003E强化学习的目标是学习一个好的策略，智能体按照这样的策略和环境交互，让累积奖励达到最大。\u003Cb\u003E这个累积奖励就是回报\u003C\u002Fb\u003E。通常指从某一个时间步 t 开始，对未来所有奖励的加总，常用符号 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 表示（有时也用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R\" alt=\"R\" eeimg=\"1\"\u002F\u003E 表示）。其定义为\u003C\u002Fp\u003E\u003Cp data-pid=\"IKrng3-I\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+%5Csum_%7Bt%3D0%7D%5E%7BT%7D+r_%7Bt%7D\" alt=\"G_t = \\sum_{t=0}^{T} r_{t}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"wLm-z13q\"\u003E通常现在的奖励和将来的奖励权重是不同的，比如今天奖励1万元和50年后奖励1万元，两者的价值大概率不同。因此会加上折扣因子\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E。若存在折扣因子，则回报可以写为：\u003C\u002Fp\u003E\u003Cp data-pid=\"hiTB3Zxh\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+r_%7B0%7D+%2B+%5Cgamma+r_%7B1%7D+%2B+%5Cgamma%5E2+r_%7B2%7D+%2B+%5Cdots+%2B+%5Cgamma%5E%7BT%7D+r_T+%3D+%5Csum_%7Bt%3D0%7D%5E%7BT%7D+%5Cgamma%5Et+r_t\" alt=\"G_t = r_{0} + \\gamma r_{1} + \\gamma^2 r_{2} + \\dots + \\gamma^{T} r_T = \\sum_{t=0}^{T} \\gamma^t r_t\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"NQ5mCM1O\"\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma+%5Cin+%280%2C+1%5D\" alt=\"\\gamma \\in (0, 1]\" eeimg=\"1\"\u002F\u003E 用于平衡当前奖励和未来奖励的重要性。如果没有折扣（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma+%3D+1\" alt=\"\\gamma = 1\" eeimg=\"1\"\u002F\u003E ），那么回报就是从时间步 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t\" alt=\"t\" eeimg=\"1\"\u002F\u003E 开始直到回合结束所获得的总奖励。在有了回报概念后，我们知道，\u003Cb\u003E强化学习的目标是最大化期望回报\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"Epoe2QXW\"\u003E9. \u003Cb\u003E价值函数（Value Function）\u003C\u002Fb\u003E：\u003Cb\u003E价值函数衡量的是“期望回报”\u003C\u002Fb\u003E。具体分为状态价值函数和动作价值函数。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"6xoMc9uL\"\u003E\u003Cb\u003E状态价值函数\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi%28s%29\" alt=\"V^\\pi(s)\" eeimg=\"1\"\u002F\u003E 表示在状态 s 下，后续按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 行动所能获得的\u003Cb\u003E期望回报\u003C\u002Fb\u003E：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D%5B%5C%2CG_t+%5Cmid+s_t+%3D+s%5C%2C%5D.\" alt=\"V^\\pi(s) = \\mathbb{E}[\\,G_t \\mid s_t = s\\,].\" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"Pkv3QEG9\"\u003E\u003Cb\u003E动作价值函数\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi%28s%2C+a%29\" alt=\"Q^\\pi(s, a)\" eeimg=\"1\"\u002F\u003E 表示在状态 s 下执行动作 a，并在之后按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi\" alt=\" \\pi\" eeimg=\"1\"\u002F\u003E 行动所能获得的\u003Cb\u003E期望回报\u003C\u002Fb\u003E：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D%5B%5C%2CG_t+%5Cmid+s_t+%3D+s%2C+a_t+%3D+a%5C%2C%5D.\" alt=\"Q^\\pi(s, a) = \\mathbb{E}[\\,G_t \\mid s_t = s, a_t = a\\,].\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"sfBOKBED\"\u003EV用来\u003Cb\u003E评价在某个状态下的策略表现好坏\u003C\u002Fb\u003E。Q用来\u003Cb\u003E评价在某个状态-动作对下的策略表现好坏\u003C\u002Fb\u003E。价值的估计至关重要，下文还会讨论。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"Iv_U_vl4\"\u003E\u003Cb\u003E10：优势函数（Advantage Function）\u003C\u002Fb\u003E：优势函数度量的是，在给定状态 s 下，执行某个动作 a 比起在该状态的平均水平（即状态价值）\u003Cb\u003E好多少或差多少\u003C\u002Fb\u003E。它的常见形式是\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A%5E%5Cpi%28s%2C+a%29+%5C%3B%3D%5C%3B+Q%5E%5Cpi%28s%2C+a%29+%5C%3B-%5C%3B+V%5E%5Cpi%28s%29.\" alt=\" A^\\pi(s, a) \\;=\\; Q^\\pi(s, a) \\;-\\; V^\\pi(s).\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"xIu9OAo-\"\u003E 如果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%5Cpi%28s%2C+a%29\" alt=\"A^\\pi(s, a)\" eeimg=\"1\"\u002F\u003E 大于 0，说明在状态 s 下执行动作 a 要比该状态的平均策略价值要好。\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"xiJnAjLK\"\u003E 如果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%5Cpi%28s%2C+a%29\" alt=\"A^\\pi(s, a)\" eeimg=\"1\"\u002F\u003E 小于 0，则说明这个动作比“平均”水平要差。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"ufAAvd9z\"\u003E11. 探索（\u003Cb\u003EExploration\u003C\u002Fb\u003E）和学习（\u003Cb\u003ELearning\u003C\u002Fb\u003E）：强化学习一般分为两个阶段。第一个是\u003Cb\u003E探索阶段\u003C\u002Fb\u003E，智能体先按照某些\u003Cb\u003E策略\u003C\u002Fb\u003E和环境进行交互，形成经验。第二个是\u003Cb\u003E学习阶段\u003C\u002Fb\u003E，智能体按照某些算法，从经验中学习，进而优化自己的\u003Cb\u003E策略\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"lKWFFYMR\"\u003E12. \u003Cb\u003E行为策略（Behavior Policy）和目标策略（Target Policy）：行为策略\u003C\u002Fb\u003E是智能体在环境交互时实际执行的策略。\u003Cb\u003E目标策略\u003C\u002Fb\u003E是智能体最终想要学到的策略，通常记为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 。两个策略相同就是on-policy，否则就是off-policy，这点我们在下文中会详细探讨。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch3\u003E\u003Cb\u003E价值函数和贝尔曼方程\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"JN0IWVsM\"\u003E在强化学习中，\u003Cb\u003E价值函数（Value Function）用来衡量在某个状态（或状态-动作对）下，按照某一策略\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi+\" alt=\" \\pi \" eeimg=\"1\"\u002F\u003E \u003Cb\u003E行动所能获得的期望回报\u003C\u002Fb\u003E；而\u003Cb\u003E贝尔曼方程（Bellman Equation）\u003C\u002Fb\u003E则刻画了该价值函数所必须满足的“递归一致性”关系。\u003C\u002Fp\u003E\u003Cp data-pid=\"E2MCpXNt\"\u003E简而言之，\u003Cb\u003E价值函数\u003C\u002Fb\u003E是目标，\u003Cb\u003E贝尔曼方程\u003C\u002Fb\u003E是描述这个目标如何在相邻时间步之间相互关联的关键公式。\u003C\u002Fp\u003E\u003Cp data-pid=\"_l5i_bPJ\"\u003E\u003Cb\u003E1. 二者的关系\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ennIz6V4\"\u003E\u003Cb\u003E价值函数的定义\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"JB60iW2G\"\u003E\u003Cb\u003E状态价值函数\u003C\u002Fb\u003E：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5BG_t+%5Cmid+s_t+%3D+s%5D%2C\" alt=\"V^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s],\" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E 即“在状态 s 下持续按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi\" alt=\" \\pi\" eeimg=\"1\"\u002F\u003E 行动所能获得的期望回报”。\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"i4FYalh9\"\u003E\u003Cb\u003E动作价值函数\u003C\u002Fb\u003E：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+Q%5E%5Cpi%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5BG_t+%5Cmid+s_t+%3D+s%2C%5C%3B+a_t+%3D+a%5D%2C\" alt=\" Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s,\\; a_t = a],\" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E 即“在状态 s 下先执行动作 a，然后继续按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi+\" alt=\" \\pi \" eeimg=\"1\"\u002F\u003E 行动所能获得的期望回报”。\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"Eol4J72x\"\u003E\u003Cb\u003E贝尔曼方程的自洽性（递归性）\u003C\u002Fb\u003E\u003Cbr\u002F\u003E 价值函数可以通过对下一步状态（或动作）的价值进行加权求期望而“自我定义”：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"XHEBnKlO\"\u003E 对状态价值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi%28s%29\" alt=\"V^\\pi(s)\" eeimg=\"1\"\u002F\u003E 来说，状态 s 的价值是“执行任一动作后的即时奖励 + 折扣后下一状态价值”的期望：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+V%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D%7Ba%5Csim%5Cpi%28%5Ccdot%7Cs%29%7D%5CBigl%5B%5C%2Cr%28s%2Ca%29+%2B+%5Cgamma+%5C%2C%5Cmathbb%7BE%7D%7Bs%7B%5Cprime%7D%5Csim+P%28%5Ccdot%7Cs%2Ca%29%7D%5BV%5E%5Cpi%28s%7B%5Cprime%7D%29%5D+%5CBigr%5D.\" alt=\" V^\\pi(s) = \\mathbb{E}{a\\sim\\pi(\\cdot|s)}\\Bigl[\\,r(s,a) + \\gamma \\,\\mathbb{E}{s{\\prime}\\sim P(\\cdot|s,a)}[V^\\pi(s{\\prime})] \\Bigr].\" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"0KRFJ4k2\"\u003E 对动作价值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi%28s%2Ca%29\" alt=\"Q^\\pi(s,a)\" eeimg=\"1\"\u002F\u003E 来说，则是“该状态动作得到的即时奖励 + 折扣后下一状态的动作价值”的期望：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D%7Bs%7B%5Cprime%7D%5Csim+P%28%5Ccdot%7Cs%2Ca%29%7D%5CBigl%5B%5C%2Cr%28s%2Ca%29+%2B+%5Cgamma+%5C%2C%5Cmathbb%7BE%7D%7Ba%7B%5Cprime%7D%5Csim%5Cpi%28%5Ccdot%7Cs%7B%5Cprime%7D%29%7D%5BQ%5E%5Cpi%28s%7B%5Cprime%7D%2Ca%7B%5Cprime%7D%29%5D+%5CBigr%5D.\" alt=\"Q^\\pi(s,a) = \\mathbb{E}{s{\\prime}\\sim P(\\cdot|s,a)}\\Bigl[\\,r(s,a) + \\gamma \\,\\mathbb{E}{a{\\prime}\\sim\\pi(\\cdot|s{\\prime})}[Q^\\pi(s{\\prime},a{\\prime})] \\Bigr].\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"POYY5S6_\"\u003E这些方程说明了：\u003Cb\u003E当前状态（或状态-动作）价值\u003C\u002Fb\u003E可以通过“下一步的价值”来计算出来，从而使价值函数具备可以迭代求解的性质。\u003C\u002Fp\u003E\u003Cp data-pid=\"doC9HLZj\"\u003E\u003Cb\u003E2. 为什么需要了解贝尔曼方程\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"lo_r6zkJ\"\u003E\u003Cb\u003E （1）动态规划与强化学习的理论基础\u003C\u002Fb\u003E\u003Cbr\u002F\u003E 贝尔曼方程是强化学习算法的核心理论根基，揭示了价值函数能够被分解并通过递归方式计算的原理。几乎所有的基于价值的强化学习方法（Q-Learning、SARSA 等）都源于对贝尔曼方程的近似求解或逼近。\u003C\u002Fp\u003E\u003Cp data-pid=\"xhBmmY4s\"\u003E\u003Cb\u003E（2）求解或逼近价值函数\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"J-vxEprJ\"\u003E 在有完备环境模型、可枚举状态下，可以用\u003Cb\u003E价值迭代（Value Iteration）或策略迭代（Policy Iteration）\u003C\u002Fb\u003E对贝尔曼方程进行数值求解，找到最优价值函数和最优策略。\u003C\u002Fli\u003E\u003Cli data-pid=\"kQtCjSZB\"\u003E 在无完备模型、无法枚举的复杂环境中，可以采用蒙特卡洛、时序差分（TD Learning）以及基于神经网络的近似方法（如 DQN）对贝尔曼方程进行抽样估计和逼近。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"tj_K4aft\"\u003E\u003Cb\u003E（3）理解算法更新规则\u003C\u002Fb\u003E\u003Cbr\u002F\u003E 很多强化学习算法中的“更新公式”，本质上都可以视作对贝尔曼方程进行\u003Cb\u003E采样\u003C\u002Fb\u003E或\u003Cb\u003E梯度\u003C\u002Fb\u003E逼近。例如：\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29+%5C%3B%5Cleftarrow%5C%3B+Q%28s%2Ca%29+%5C%3B%2B%5C%3B+%5Calpha+%5Cbigl%5B%5C%2Cr+%2B+%5Cgamma%5C%2C%5Cmax_%7Ba%7B%5Cprime%7D%7DQ%28s%7B%5Cprime%7D%2Ca%7B%5Cprime%7D%29+%5C%3B-%5C%3B+Q%28s%2Ca%29%5Cbigr%5D%2C\" alt=\"Q(s,a) \\;\\leftarrow\\; Q(s,a) \\;+\\; \\alpha \\bigl[\\,r + \\gamma\\,\\max_{a{\\prime}}Q(s{\\prime},a{\\prime}) \\;-\\; Q(s,a)\\bigr],\" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E 这正是 Q-Learning 中对\u003Cb\u003E最优贝尔曼方程\u003C\u002Fb\u003E的单步采样更新。\u003C\u002Fp\u003E\u003Cp data-pid=\"RIgjvipC\"\u003E\u003Cb\u003E（4）扩展到更复杂场景\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cul\u003E\u003Cli data-pid=\"FJ1VqN2O\"\u003E 在策略梯度和 Actor-Critic 方法中，Critic 的目标就是学习满足“贝尔曼方程”的价值函数或 Q 函数。\u003Cbr\u002F\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"bXiYc-TN\"\u003E 其它如分层强化学习、多智能体强化学习等场景中，依旧离不开对贝尔曼方程的变形或推广。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"50b-eOhV\"\u003E\u003Cb\u003E总之，\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"K1Qp9wKu\"\u003E\u003Cb\u003E价值函数\u003C\u002Fb\u003E为评估“当前状态（或动作）有多好”提供了一个“期望回报”的度量。\u003C\u002Fli\u003E\u003Cli data-pid=\"aLVM5xsi\"\u003E\u003Cb\u003E贝尔曼方程\u003C\u002Fb\u003E阐明了价值函数可以被分解成与下一状态和相应奖励相联系的自洽递归关系。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Chr\u002F\u003E\u003Ch3\u003EA、V、Q的关系\u003C\u002Fh3\u003E\u003Cp data-pid=\"y68D0-Rk\"\u003E优势函数（A）与值函数（V）、动作值函数（Q）的区别的\u003Cb\u003E联系\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"jtovxCDY\"\u003E优势函数是Q和V的差值。\u003C\u002Fp\u003E\u003Cp data-pid=\"5Zo2iSKp\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Cpi%7D%28s%2C+a%29+%3D+Q%5E%7B%5Cpi%7D%28s%2C+a%29+-+V%5E%7B%5Cpi%7D%28s%29\" alt=\"A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"WW0fzLE7\"\u003E优势函数与值函数、动作值函数的\u003Cb\u003E区别：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"L2JIpkJ0\"\u003E\u003Cb\u003E值函数（V）\u003C\u002Fb\u003E：衡量一个状态的价值，反映了在某个状态下，智能体根据当前策略所能期望得到的总回报。\u003C\u002Fli\u003E\u003Cli data-pid=\"iHF65CI-\"\u003E\u003Cb\u003E动作值函数（Q）\u003C\u002Fb\u003E：直接衡量在某状态下采取某动作后，智能体所能期望得到的总回报。\u003C\u002Fli\u003E\u003Cli data-pid=\"7ir3BSAC\"\u003E\u003Cb\u003E优势函数（A）\u003C\u002Fb\u003E：是一个动作对比的度量，表明选择某个动作相较于平均策略的回报增益。和V、Q不同的是，A提供了一个相对的评价。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E估计值函数的三种方法：MC、TD、GAE\u003C\u002Fh3\u003E\u003Cp data-pid=\"8glILTIG\"\u003ERL中，价值的估计非常重要。估计值函数的常见方法有三种：蒙特卡洛（MC）、时间差分（TD）、广义优势估计（GAE）。\u003C\u002Fp\u003E\u003Cp data-pid=\"PI-g1rmD\"\u003E它们在估计值函数时各有优缺点，具体在方差和偏差之间存在不同的权衡。\u003C\u002Fp\u003E\u003Cp data-pid=\"TR07lfGn\"\u003E\u003Cb\u003E1. 蒙特卡洛方法（MC）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"5fTolgVf\"\u003E • \u003Cb\u003E原理\u003C\u002Fb\u003E：蒙特卡洛方法通过在完整的轨迹上计算回报（即从当前状态开始到最终状态的累积奖励）来估计状态值函数或动作值函数。这个方法完全依赖于最终的回报，因此需要等待完整的路径（完整的Episode）来计算值。\u003C\u002Fp\u003E\u003Cp data-pid=\"5nl-i2cb\"\u003E • \u003Cb\u003E优点\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Zreq8ceR\"\u003E 不需要任何模型假设，直接依赖实际回报。\u003C\u002Fli\u003E\u003Cli data-pid=\"5dy5ABVs\"\u003E 在长期内是无偏的（即期望值是正确的），因为它直接使用实际的累积奖励。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"0eny9x65\"\u003E • \u003Cb\u003E缺点\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"V-d_dHtB\"\u003E 高方差：由于只使用每个Episode的最终回报，导致每个回报的估计可能存在较大波动，尤其是在奖励信号稀疏或变动较大的情况下，估计的方差较大。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"MoHA73lB\"\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E：MC方法因为依赖完整的回报，所以它的估计有较大的方差，但没有偏差。\u003C\u002Fp\u003E\u003Cp data-pid=\"JiM8r8Pp\"\u003E\u003Cb\u003E2. 时序差分方法（TD）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"X8tHJBs0\"\u003E • \u003Cb\u003E原理\u003C\u002Fb\u003E：时序差分方法则是通过\u003Cb\u003E递推更新\u003C\u002Fb\u003E（也称为bootstrapping）来估计值函数，它并不需要等待完整的Episode，而是根据每一步的即时反馈进行更新。TD方法将当前的估计值与下一时刻的估计值进行比较，通过差分来更新当前状态的值。\u003C\u002Fp\u003E\u003Cp data-pid=\"5SRtBJAa\"\u003E • \u003Cb\u003E优点\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"UOaXqHr7\"\u003E 较低的方差：由于TD方法使用每一步的即时反馈，它不依赖于完整Episode的回报，估计过程可以在更短时间内进行，因而方差较小。\u003C\u002Fli\u003E\u003Cli data-pid=\"g785w2TE\"\u003E 可以在线学习，不需要等待完整的Episode。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"80Ga4p2C\"\u003E • \u003Cb\u003E缺点\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Ju4O7MYw\"\u003E 高偏差：由于TD方法使用的是估计值而非真实回报，它会引入一定的偏差。特别是它依赖当前的估计来更新，因此如果初始估计有偏，后续的更新也会继承这个偏差。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"YVlxClbl\"\u003E • \u003Cb\u003E总结\u003C\u002Fb\u003E：TD方法具有较低的方差，但它引入了偏差，因为它依赖于现有的估计，而非实际的回报。\u003C\u002Fp\u003E\u003Cp data-pid=\"2MkKYrji\"\u003E\u003Cb\u003E3. 广义优势估计（GAE）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"3I6WQntz\"\u003E • \u003Cb\u003E原理\u003C\u002Fb\u003E：GAE是一种折中方法，它结合了MC和TD的方法，旨在通过平衡方差和偏差来提高估计的稳定性和效率。GAE通过引入一个\u003Cb\u003E超参数\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E （类似于TD方法中的折扣因子），在计算优势函数时利用TD方法的部分信息，而不是完全依赖于真实的回报。\u003C\u002Fp\u003E\u003Cp data-pid=\"7O_jhiXo\"\u003E • 具体来说，GAE通过对TD误差进行加权平均来估计优势函数，从而减少单步TD误差带来的偏差，并控制方差的大小。\u003C\u002Fp\u003E\u003Cp data-pid=\"EXhjPQ3u\"\u003E • 它的更新公式为：\u003C\u002Fp\u003E\u003Cp data-pid=\"4CPXHJ6S\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_t%5E%7B%5Clambda%7D+%3D+%5Csum_%7Bl%3D0%7D%5E%7B%5Cinfty%7D+%28%5Cgamma+%5Clambda%29%5El+%5Cdelta_%7Bt%2Bl%7D\" alt=\"A_t^{\\lambda} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"pqJbQ_f0\"\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta_%7Bt%2Bl%7D\" alt=\"\\delta_{t+l}\" eeimg=\"1\"\u002F\u003E 表示每一步的TD误差， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 是折扣因子， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 是用于加权的超参数，控制了TD和MC的折中。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"GFblOCB_\"\u003E\u003Cb\u003E优点\u003C\u002Fb\u003E：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"GyXEuUHJ\"\u003E 在方差和偏差之间取得了更好的折中。通过调节 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E ，GAE可以灵活控制偏差和方差之间的权衡。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"KW_754hN\"\u003E\u003Cb\u003E缺点\u003C\u002Fb\u003E：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"GvcGP_7m\"\u003E 相较于纯粹的TD或MC，GAE需要更多的计算，因为它需要在每一步计算加权的TD误差。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"qqMWUFMG\"\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E：GAE通过加权TD误差，能够在MC的低偏差和TD的低方差之间取得一个折中。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"LNrD6WmX\"\u003E\u003Cb\u003E4. 高方差、低偏差 vs. 高偏差、低方差的折中\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"bD3wJTg-\"\u003E\u003Cb\u003EMC方法\u003C\u002Fb\u003E高方差是由于它依赖于整个轨迹的回报，导致估计可能存在较大波动，但它是无偏的，因此在长时间运行时可以获得精确的估计。\u003C\u002Fli\u003E\u003Cli data-pid=\"tgVDmhFd\"\u003E\u003Cb\u003ETD方法\u003C\u002Fb\u003E则通过引入当前估计来更新状态值或动作值，它减少了方差，因为每次更新都基于当前估计的反馈。然而，它依赖于已有的估计，因此会引入偏差。\u003C\u002Fli\u003E\u003Cli data-pid=\"zd6Sz3-T\"\u003E\u003Cb\u003EGAE\u003C\u002Fb\u003E则通过平衡这两者的优缺点，调整 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 来控制估计中的偏差和方差，从而获得更稳定、更准确的估计。当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%3D+0\" alt=\"\\lambda = 0\" eeimg=\"1\"\u002F\u003E 时，GAE等价于TD(0)，当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%3D+1\" alt=\"\\lambda = 1\" eeimg=\"1\"\u002F\u003E 时，GAE等价于MC方法。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"eNW3GfsU\"\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Mr_2Bkt6\"\u003E\u003Cb\u003EMC方法\u003C\u002Fb\u003E：高方差，无偏。\u003C\u002Fli\u003E\u003Cli data-pid=\"KWA_9-Bi\"\u003E\u003Cb\u003ETD方法\u003C\u002Fb\u003E：低方差，高偏差。\u003C\u002Fli\u003E\u003Cli data-pid=\"c6v-w-BU\"\u003E\u003Cb\u003EGAE方法\u003C\u002Fb\u003E：折中，既能减少偏差，也能降低方差，通过调整 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 来灵活控制。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"WB0jot3d\"\u003E这个折中使得GAE在实际应用中往往能取得更好的性能，特别是在复杂的强化学习任务中。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003ENLP中的RL\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c74f58b19921bf0a52c1d1bd196ea263_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"365\" data-original-token=\"v2-c74f58b19921bf0a52c1d1bd196ea263\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c74f58b19921bf0a52c1d1bd196ea263_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ERL in NLP【5】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"yp45uHWF\"\u003E\u003Cb\u003E状态S：\u003C\u002Fb\u003E输入prompt\u003C\u002Fp\u003E\u003Cp data-pid=\"PPWFZOxW\"\u003E\u003Cb\u003E动作A：\u003C\u002Fb\u003E输出response（即LLM输出下一个token）\u003C\u002Fp\u003E\u003Cp data-pid=\"OFQYRP5W\"\u003E\u003Cb\u003E奖励R：\u003C\u002Fb\u003E根据prompt+response进行奖励模型打分\u003C\u002Fp\u003E\u003Cp data-pid=\"WHryxs3g\"\u003E\u003Cb\u003E整体目标\u003C\u002Fb\u003E：给定prompt，调整policy，生成符合人类喜好（RM偏序信号）的response\u003C\u002Fp\u003E\u003Ch3\u003E经典强化学习算法的分类\u003C\u002Fh3\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-eba10ba1c76228967df531c5ac287f3f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1896\" data-rawheight=\"1022\" data-original-token=\"v2-eba10ba1c76228967df531c5ac287f3f\" class=\"origin_image zh-lightbox-thumb\" width=\"1896\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-eba10ba1c76228967df531c5ac287f3f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E强化学习算法分类【14】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"b4io_ZAm\"\u003E在这里，对里面的部分算法做简要介绍。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"svLje94K\"\u003E\u003Cb\u003E马尔可夫决策过程（MDP）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"w1bXPlNB\"\u003E\u003Cb\u003E马尔可夫性质\u003C\u002Fb\u003E表示未来状态只与当前状态（和动作）有关，与过去的历史无关。这使得我们可以用一个状态来完整地描述环境。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"bHq6m5nL\"\u003E\u003Cb\u003E马尔可夫决策过程（MDP）\u003C\u002Fb\u003E是一个在“马尔可夫性质”假设下建模序列决策的问题框架，一般用五元组 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clangle+S%2C+A%2C+P%2C+R%2C+%5Cgamma+%5Crangle\" alt=\"\\langle S, A, P, R, \\gamma \\rangle\" eeimg=\"1\"\u002F\u003E 表示，其中：\u003C\u002Fp\u003E\u003Cp data-pid=\"XdJxk-it\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S\" alt=\"S\" eeimg=\"1\"\u002F\u003E ：状态空间\u003C\u002Fp\u003E\u003Cp data-pid=\"bJ8K9TYJ\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A\" alt=\"A\" eeimg=\"1\"\u002F\u003E ：动作空间\u003C\u002Fp\u003E\u003Cp data-pid=\"zLlPi1OB\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28s%7B%5Cprime%7D+%5Cmid+s%2Ca%29\" alt=\"P(s{\\prime} \\mid s,a)\" eeimg=\"1\"\u002F\u003E ：状态转移概率，描述在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 执行动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 后转移到下一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%7B%5Cprime%7D\" alt=\"s{\\prime}\" eeimg=\"1\"\u002F\u003E 的概率\u003C\u002Fp\u003E\u003Cp data-pid=\"I12RloYQ\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28s%2Ca%29\" alt=\"R(s,a)\" eeimg=\"1\"\u002F\u003E ：即时奖励函数，衡量在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 执行动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 所得到的回报\u003C\u002Fp\u003E\u003Cp data-pid=\"8zBOSM4m\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E ：折扣因子，用于平衡当前奖励和未来奖励\u003C\u002Fp\u003E\u003Cp data-pid=\"lU1OuEWj\"\u003E在MDP中，智能体通过与环境的反复交互（“状态—动作—转移—奖励”的循环），旨在找到一条最优策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi%5E%2A\" alt=\" \\pi^*\" eeimg=\"1\"\u002F\u003E ，最大化长期累积折扣回报。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"yuRWgW0N\"\u003E\u003Cb\u003EBandit problem（强盗问题或赌博机问题）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"nhOfMbvI\"\u003E\u003Cb\u003EBandit problem\u003C\u002Fb\u003E 是强化学习和决策理论中的经典问题，用于研究\u003Cb\u003E在不确定环境下的探索与利用权衡（exploration vs. exploitation trade-off）\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"HUnxvkgE\"\u003E赌博机（Bandit）问题可视作MDP的极简形式：\u003Cb\u003E系统仅有一个“状态”，或者说对后续状态没有区分\u003C\u002Fb\u003E。每台赌博机（或称拉杆）对应一个动作，每个动作产生的奖励分布未知且相互独立。智能体在每次操作前需在各臂之间进行抉择，在缺乏状态转移的情况下，通过不断探索各臂的回报分布并利用已有信息，从而最大化累积奖励。\u003Cb\u003E相比MDP，Bandit只关注“当前选择—即时奖励”的单步决策，不涉及序列性的状态演化\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"PH_YovlZ\"\u003E\u003Cb\u003EBandit Problem 的类型\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"oaqItwj_\"\u003E 1. \u003Cb\u003E多臂老虎机（Multi-Armed Bandit, MAB）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"pDkpNxnK\"\u003E 基础形式，假设每台老虎机的奖励分布是固定的（静态）。\u003C\u002Fli\u003E\u003Cli data-pid=\"CZbRk8kQ\"\u003E 常见算法包括：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"n7fB_-ZG\"\u003E\u003Cb\u003Eε-greedy\u003C\u002Fb\u003E：以一定概率随机探索，其他时候利用当前最优选项。\u003C\u002Fli\u003E\u003Cli data-pid=\"JKZI1KHf\"\u003E\u003Cb\u003EUCB（上置信界）\u003C\u002Fb\u003E：根据当前奖励估计和不确定性选择最优选项。\u003C\u002Fli\u003E\u003Cli data-pid=\"s0rQk8-X\"\u003E\u003Cb\u003EThompson Sampling\u003C\u002Fb\u003E：通过贝叶斯推断选择最优选项。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"TVflJdvB\"\u003E 2. \u003Cb\u003EContextual Bandit（上下文老虎机）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"WfpXGmhN\"\u003E 每次决策前会观察到一个“上下文”（context），不同上下文可能对应不同的最优选项。\u003C\u002Fli\u003E\u003Cli data-pid=\"FN69AY2d\"\u003E 类似于推荐系统问题：根据用户特征选择推荐内容。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"7P1_IfWG\"\u003E 3. \u003Cb\u003ENon-Stationary Bandit（非静态老虎机）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"m4EIhQ9H\"\u003E • 奖励分布会随着时间动态变化，需要更快速适应新分布的算法。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"8lwsb24d\"\u003E\u003Cb\u003EDQN（Deep Q-Network）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"plQ9xHeU\"\u003E\u003Cb\u003EDQN\u003C\u002Fb\u003E将深度神经网络与Q-Learning相结合，使用卷积神经网络近似Q函数，在Atari游戏上取得革命性成果。\u003C\u002Fp\u003E\u003Cp data-pid=\"59Bq9HSz\"\u003E关键技巧：\u003C\u002Fp\u003E\u003Cp data-pid=\"MnKeDMe5\"\u003E 1. \u003Cb\u003EExperience Replay\u003C\u002Fb\u003E：将交互经验存储在回放池中，随机小批量采样来打破数据相关性；\u003C\u002Fp\u003E\u003Cp data-pid=\"QOOl0XjI\"\u003E 2. \u003Cb\u003ETarget Network\u003C\u002Fb\u003E：固定目标Q网络一段时间再更新，避免网络剧烈震荡。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"t9-FplYw\"\u003E\u003Cb\u003EActor-Critic架构\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ShiDMja4\"\u003E为平衡高方差和低偏差，在\u003Cb\u003EActor-Critic\u003C\u002Fb\u003E中将策略函数（Actor）和价值函数（Critic）同时学习：\u003C\u002Fp\u003E\u003Cp data-pid=\"-d4yRcEh\"\u003E • \u003Cb\u003EActor\u003C\u002Fb\u003E：输出策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cpi_%5Ctheta%28a%7Cs%29\" alt=\" \\pi_\\theta(a|s)\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fp\u003E\u003Cp data-pid=\"wVvchfLB\"\u003E • \u003Cb\u003ECritic\u003C\u002Fb\u003E：估计价值函数  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_w%28s%29+\" alt=\"V_w(s) \" eeimg=\"1\"\u002F\u003E 或  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_w%28s%2Ca%29\" alt=\"Q_w(s,a)\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fp\u003E\u003Cp data-pid=\"pA5EBRbB\"\u003E • 每一次采样时，利用 Critic 来估计动作优势（Advantage），更新 Actor 的梯度，使得训练更稳定。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003ERL的两大基本特征\u003C\u002Fh3\u003E\u003Cp data-pid=\"X6Gw1vaD\"\u003E强化学习（Reinforcement Learning, RL）拥有两大关键特征：一是通过持续的\u003Cb\u003E试错搜索（trial-and-error）来发现最佳行为，二是面临延迟回报（delayed reward）\u003C\u002Fb\u003E的挑战。这使得强化学习与监督学习在“如何评价模型”这一核心问题上存在显著差异：\u003C\u002Fp\u003E\u003Cp data-pid=\"xN2Qr9Gf\"\u003E • 在监督学习中，模型会根据事先给定的正确标签（label）得到即时且明确的反馈；\u003C\u002Fp\u003E\u003Cp data-pid=\"PwTuLDOp\"\u003E • 而在强化学习中，智能体的评价取决于整个互动过程中所作出的一系列动作，只有通过对\u003Cb\u003E累计获得的奖励\u003C\u002Fb\u003E进行评估，才能衡量策略的优劣。\u003C\u002Fp\u003E\u003Cp data-pid=\"TtIebSnA\"\u003E换言之，监督学习用“已知正确答案”来指导模型学习，而强化学习则是让智能体在环境中不断尝试、观察反馈，再基于累积奖励对策略进行调整，从而逐步逼近最优方案。\u003C\u002Fp\u003E\u003Ch3\u003E探索与利用的平衡\u003C\u002Fh3\u003E\u003Cp data-pid=\"7p_N_1VB\"\u003E在强化学习（RL）中，\u003Cb\u003E探索（Exploration）与利用（Exploitation）\u003C\u002Fb\u003E的平衡是核心挑战：探索指尝试新动作以获取环境信息，利用则基于当前知识选择最优动作以最大化奖励。\u003Cb\u003E过度探索可能导致低效，过度利用则易陷入局部最优\u003C\u002Fb\u003E。常见方法包括：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"SqtNiORt\"\u003E\u003Cb\u003Eε-greedy\u003C\u002Fb\u003E：以概率ε随机探索，其余时间贪婪利用；\u003C\u002Fli\u003E\u003Cli data-pid=\"EDUeLfMU\"\u003E\u003Cb\u003EUpper Confidence Bound（UCB）\u003C\u002Fb\u003E：通过置信区间量化动作不确定性，平衡二者；\u003C\u002Fli\u003E\u003Cli data-pid=\"PPmVty_4\"\u003E\u003Cb\u003EThompson Sampling\u003C\u002Fb\u003E：基于贝叶斯后验分布动态调整动作选择；\u003C\u002Fli\u003E\u003Cli data-pid=\"FBQauWVk\"\u003E\u003Cb\u003E基于内在激励\u003C\u002Fb\u003E（如好奇心驱动）或\u003Cb\u003E信息增益\u003C\u002Fb\u003E，鼓励访问未充分探索的状态；\u003C\u002Fli\u003E\u003Cli data-pid=\"PqVCY7x9\"\u003E\u003Cb\u003ESoftmax策略\u003C\u002Fb\u003E：按动作价值概率分布采样，兼顾高价值与潜在高回报动作。部分算法（如MCTS）结合基于模型的规划进一步优化此平衡。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch3\u003ERL中的on-policy 和 off-policy有什么区别？\u003C\u002Fh3\u003E\u003Cp data-pid=\"21wZP-T9\"\u003E在强化学习中，\u003Cb\u003Eon-policy\u003C\u002Fb\u003E 和 \u003Cb\u003Eoff-policy\u003C\u002Fb\u003E 的区别，核心在于“\u003Cb\u003E数据采样的策略（Policy）与学习的策略是否相同\u003C\u002Fb\u003E”。以下从原理、示例、优缺点几个方面说明：\u003C\u002Fp\u003E\u003Cp data-pid=\"qvU2tP6O\"\u003E\u003Cb\u003E1. 原理概念\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"iJq13-a2\"\u003E 1. \u003Cb\u003Eon-policy\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"MLn0mk-l\"\u003E • \u003Cb\u003E数据采样策略 与 当前正在学习\u002F更新的策略 相同\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"JH2ycA6X\"\u003E • 智能体执行的行为策略（Behavior Policy）就是要学习或评估的目标策略（Target Policy）。\u003C\u002Fp\u003E\u003Cp data-pid=\"SgXIhleo\"\u003E • 换言之：智能体在环境中如何行动，就和我们要学的那条策略完全一致。\u003C\u002Fp\u003E\u003Cp data-pid=\"4tY7bQWG\"\u003E 2. \u003Cb\u003Eoff-policy\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"J0uGO9ed\"\u003E • \u003Cb\u003E数据采样策略 与 学习\u002F评估的目标策略 不必相同\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"f3KlBYUL\"\u003E • 可以用一种行为策略与环境交互，收集到的数据却用来学习另一种目标策略。\u003C\u002Fp\u003E\u003Cp data-pid=\"w5IV9CeW\"\u003E • 常见做法是保留一个\u003Cb\u003E经验回放池（Replay Buffer）\u003C\u002Fb\u003E，历史数据可以不断被重用（不一定是按照当前策略采样得到的）。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"7YpQu7jy\"\u003E\u003Cb\u003E2. 示例对比\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"DEN_wjxP\"\u003E 1. \u003Cb\u003Eon-policy\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"K9Ml88_h\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E、\u003Cb\u003EA2C\u002FA3C\u003C\u002Fb\u003E 等。\u003C\u002Fli\u003E\u003Cli data-pid=\"uVFSTN86\"\u003E 在 PPO 中，为了保证策略稳定性，需要用最新的（或近似最新的）策略去采样交互数据，然后紧接着对这批数据进行更新，更新后又要丢弃旧数据，重新采样。\u003C\u002Fli\u003E\u003Cli data-pid=\"FU_wmocA\"\u003E 好处是可以严格保证“策略分布”和“数据分布”一致，\u003Cb\u003E收敛性\u003C\u002Fb\u003E更易分析。\u003C\u002Fli\u003E\u003Cli data-pid=\"BjOfJcgr\"\u003E 坏处是\u003Cb\u003E数据利用率低\u003C\u002Fb\u003E，因为一旦策略更新，这批数据就算“过期”了，很难重复使用来训练新策略。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"iU4iQenZ\"\u003E 2. \u003Cb\u003Eoff-policy\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"QEdLcu3_\"\u003E\u003Cb\u003EQ-Learning\u003C\u002Fb\u003E、\u003Cb\u003EDQN\u003C\u002Fb\u003E、\u003Cb\u003ESAC\u003C\u002Fb\u003E、\u003Cb\u003ETD3\u003C\u002Fb\u003E 等。\u003C\u002Fli\u003E\u003Cli data-pid=\"f9mKbD1G\"\u003E DQN 中的经验回放池就是典型的 off-policy：采集数据时使用的是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon-greedy\" alt=\"\\epsilon-greedy\" eeimg=\"1\"\u002F\u003E 策略（带随机探索），但在更新 Q 函数时，我们朝着 “greedy” 或某个更优的目标策略方向改进。\u003C\u002Fli\u003E\u003Cli data-pid=\"01JTO3vh\"\u003E 好处是可以\u003Cb\u003E重复使用\u003C\u002Fb\u003E历史数据，样本效率更高；还可以从人类示教数据或其他智能体的轨迹中学习。\u003C\u002Fli\u003E\u003Cli data-pid=\"IV8rrqB6\"\u003E 坏处是策略与数据分布不一致带来的复杂性，可能更难保证收敛，更新过程也更易出现分布偏移（Distribution Shift）问题。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"AZKaNJyG\"\u003E\u003Cb\u003E3. 优缺点总结\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ctable data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Cth\u003E关键点\u003C\u002Fth\u003E\u003Cth\u003Eon-policy\u003C\u002Fth\u003E\u003Cth\u003Eoff-policy\u003C\u002Fth\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E数据采样策略\u003C\u002Ftd\u003E\u003Ctd\u003E与目标策略相同\u003C\u002Ftd\u003E\u003Ctd\u003E与目标策略不同，可自行选择或混用\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E数据利用率\u003C\u002Ftd\u003E\u003Ctd\u003E只能使用最近采样数据，效率较低\u003C\u002Ftd\u003E\u003Ctd\u003E可以反复使用历史数据，效率较高\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E学习稳定性\u003C\u002Ftd\u003E\u003Ctd\u003E分布一致性更好，算法分析更直接\u003C\u002Ftd\u003E\u003Ctd\u003E数据分布差异大时，学习可能会不稳定\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E示例算法\u003C\u002Ftd\u003E\u003Ctd\u003EA2C\u002FA3C、PPO 等\u003C\u002Ftd\u003E\u003Ctd\u003EQ-Learning、DQN、SAC、TD3 等\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E适用场景\u003C\u002Ftd\u003E\u003Ctd\u003E中小型或交互稳定场景\u003C\u002Ftd\u003E\u003Ctd\u003E大规模、需要高数据效率或可以使用离线数据场景\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"cTB3F2S9\"\u003E\u003Cb\u003E4. 为什么要区分 on-policy 和 off-policy\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"YiJn9fOu\"\u003E • \u003Cb\u003E策略分布一致性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"ocnK3J-D\"\u003E强化学习的目标是学习一条最优策略（或评估某条策略的价值）。若数据来自与目标策略不同的分布，我们就需要更多技巧（如重要性采样、行为策略修正）来保证学习的正确性。\u003C\u002Fp\u003E\u003Cp data-pid=\"Pp30YYBh\"\u003E • \u003Cb\u003E数据效率\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"73bgaGol\"\u003Eon-policy 经常“采样-训练-丢弃”，数据无法重复利用；off-policy 通过经验回放或从其他来源引入大量轨迹数据，能反复训练，提高数据效率。\u003C\u002Fp\u003E\u003Cp data-pid=\"g3L4VKpc\"\u003E • \u003Cb\u003E安全性与可控性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"YPgHVxUG\"\u003E某些场景需要在策略稳定前就保证系统安全，off-policy 可以用保守的行为策略来收集数据，而学的却是更激进的目标策略。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"U-YZdjLk\"\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"I9MwSwc7\"\u003E\u003Cb\u003Eon-policy\u003C\u002Fb\u003E：数据采样与目标策略一致，保证分布统一，算法理论分析更简洁，但数据浪费较大。\u003C\u002Fli\u003E\u003Cli data-pid=\"97onRfqd\"\u003E\u003Cb\u003Eoff-policy\u003C\u002Fb\u003E：数据来源灵活，可以重复使用过去的经验，样本效率更高，但需要处理分布偏移带来的额外复杂性。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"bhoH2qJB\"\u003E这就是两者在强化学习中的本质区别。\u003Cb\u003Eoff-policy RL可以理解为“纸上得来终觉浅”，on-policy RL可以理解为“绝知此事要躬行”。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003ERL中的online和offline有什么区别？\u003C\u002Fh3\u003E\u003Cp data-pid=\"TTMS50l9\"\u003E在线强化学习(Online Reinforcement Learning)和离线强化学习(Offline Reinforcement Learning)是强化学习领域的两种不同学习范式，\u003Cb\u003E它们的主要区别在于如何使用经验数据（即智能体与环境交互产生的状态、动作、奖励序列）来训练模型。\u003C\u002Fb\u003E下面是两者之间的几个关键差异：\u003C\u002Fp\u003E\u003Cp data-pid=\"_2BRiYri\"\u003E\u003Cb\u003E数据收集方式\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"0oOmbKjg\"\u003E\u003Cb\u003E在线强化学习\u003C\u002Fb\u003E：在学习过程中，智能体直接与环境实时交互，每一步选择动作、接收环境反馈（奖励和下一个状态），并立即用这些新鲜数据更新其策略或价值函数。这意味着学习策略会直接影响到实际行为，并且环境需要是可交互的。\u003C\u002Fli\u003E\u003Cli data-pid=\"Ihkw9Cek\"\u003E\u003Cb\u003E离线强化学习\u003C\u002Fb\u003E：事先从一个固定的数据集（通常是之前交互产生的）中学习，不与环境实时互动。这个数据集可以是通过专家演示、历史记录或其他代理的行为收集的。离线学习的目标是从这些静态数据中最大化学习效率，而无需进一步探索环境。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"6tUZ0oLR\"\u003E\u003Cb\u003E探索与利用的平衡\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"rWiansNL\"\u003E\u003Cb\u003E在线学习\u003C\u002Fb\u003E强调在探索未知策略与利用当前最优策略之间找到平衡，因为每次决策都直接影响到学习过程和未来奖励。\u003C\u002Fli\u003E\u003Cli data-pid=\"R4KWuOc_\"\u003E\u003Cb\u003E离线学习\u003C\u002Fb\u003E则主要关注于利用已有的数据，由于没有新的数据采集，探索新策略的能力受限，因此重点在于如何有效利用现有数据优化策略。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"rPetWPgn\"\u003E\u003Cb\u003E安全性和稳定性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"KfzkLPM1\"\u003E\u003Cb\u003E离线强化学习\u003C\u002Fb\u003E因为不直接与环境互动，所以特别适合于那些直接尝试可能有高风险或成本的场景，比如医疗决策、金融交易或物理机器人控制。\u003C\u002Fli\u003E\u003Cli data-pid=\"M0bLAHZr\"\u003E\u003Cb\u003E在线学习\u003C\u002Fb\u003E可能涉及试错，对于某些敏感或昂贵的环境来说，错误的决策可能会导致不可逆的后果。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"_NwR-cMe\"\u003E\u003Cb\u003E数据效率与收敛速度\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"PtggLk8H\"\u003E\u003Cb\u003E离线学习\u003C\u002Fb\u003E可能在数据利用上更为高效，因为它试图从有限的数据集中榨取尽可能多的信息。但找到最佳策略的速度可能受限于数据集的覆盖范围和多样性。\u003C\u002Fli\u003E\u003Cli data-pid=\"RAaGBqr3\"\u003E\u003Cb\u003E在线学习\u003C\u002Fb\u003E理论上可以无限探索，直至找到最优策略，但这个过程可能需要大量与环境的交互，从而在数据需求和时间成本上更高。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"9vt1KVtP\"\u003E\u003Cb\u003E策略优化自由度\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"cR4jxvC6\"\u003E\u003Cb\u003E在线学习\u003C\u002Fb\u003E允许智能体根据即时反馈调整策略，因此在策略空间中的探索更加灵活。\u003C\u002Fli\u003E\u003Cli data-pid=\"HCP_tdrK\"\u003E\u003Cb\u003E离线学习\u003C\u002Fb\u003E受限于已收集数据的策略空间，难以评估未被数据覆盖的动作的好坏，因此优化策略时可能较为保守。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"73pBsJQe\"\u003E综上所述，选择在线还是离线强化学习取决于具体的应用场景、可用资源、对安全性的要求以及对学习速度和数据效率的需求。\u003C\u002Fp\u003E\u003Ch3\u003E\u003Cb\u003Eon\u002Foff-policy和online\u002Foffline的区别\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"9L9Cswnn\"\u003E1. \u003Cb\u003E不同维度\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Go0LqOqc\"\u003E\u003Cb\u003Eon\u002Foff-policy\u003C\u002Fb\u003E 解决的问题：\u003Cb\u003E“采样策略”与“目标策略”的一致程度\u003C\u002Fb\u003E。\u003C\u002Fli\u003E\u003Cli data-pid=\"rKFLxE4L\"\u003E\u003Cb\u003Eonline\u002Foffline\u003C\u002Fb\u003E 解决的问题：\u003Cb\u003E“是否可以持续交互收集新数据”\u003C\u002Fb\u003E。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"IHMh9-OO\"\u003E 2. \u003Cb\u003E常见组合\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"5yh8oKsM\"\u003E\u003Cb\u003E离线强化学习基本一定是 off-policy，但在线强化学习可以既有 on-policy，也可以有 off-policy。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Ik1WFRLI\"\u003E\u003Cb\u003E在线 + on-policy\u003C\u002Fb\u003E：比如 PPO、A2C 这些算法，需要跟环境交互，采集数据时就使用当前策略，采完就更新，旧数据不再使用。\u003C\u002Fli\u003E\u003Cli data-pid=\"rQ3WfmT0\"\u003E\u003Cb\u003E在线 + off-policy\u003C\u002Fb\u003E：比如 DQN，虽然也是在线与环境交互，但 DQN 会把交互数据放到 replay buffer，后面训练时用到的旧数据不一定来自当前的策略，所以是 off-policy。\u003C\u002Fli\u003E\u003Cli data-pid=\"NDwYRXQa\"\u003E\u003Cb\u003E离线 + off-policy\u003C\u002Fb\u003E：这最常见。离线 RL 必然不能和“当前目标策略”一致地采样，因为数据集已经固定了，通常是其他策略或历史操作生成的数据，所以几乎都是 off-policy。\u003C\u002Fli\u003E\u003Cli data-pid=\"_YGjMxKX\"\u003E\u003Cb\u003E离线 + on-policy\u003C\u002Fb\u003E：理论上很难，因为离线数据本身就是固定收集的，跟当前想学的策略无法保持一致——所以离线强化学习通常都被视为 off-policy 的特例。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003E重要性采样基本原理\u003C\u002Fh3\u003E\u003Cp data-pid=\"8kQQVAnG\"\u003E重要性采样是一种统计方法，\u003Cb\u003E用来在目标分布 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 与采样分布 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28x%29\" alt=\"q(x)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 不一致的情况下，调整采样结果的权重，使得从 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28x%29\" alt=\"q(x)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 中采样的数据可以用于估计 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 的期望。【10】\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"TCvguT4u\"\u003E假设我们希望计算目标分布\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"\u002F\u003E 下某个函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"\u002F\u003E 的期望：\u003C\u002Fp\u003E\u003Cp data-pid=\"h_W-bpbJ\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D_p%5Bf%28x%29%5D+%3D+%5Cint+f%28x%29p%28x%29+%5C%2C+dx\" alt=\"\\mathbb{E}_p[f(x)] = \\int f(x)p(x) \\, dx\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"fv3NS8w4\"\u003E如果\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"\u002F\u003E很难直接采样，但可以从一个容易采样的分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28x%29\" alt=\"q(x)\" eeimg=\"1\"\u002F\u003E 中采样，则可以通过以下公式重写：\u003C\u002Fp\u003E\u003Cp data-pid=\"mzUOt3dT\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D_p%5Bf%28x%29%5D+%3D+%5Cint+f%28x%29+%5Cfrac%7Bp%28x%29%7D%7Bq%28x%29%7D+q%28x%29+%5C%2C+dx\" alt=\"\\mathbb{E}_p[f(x)] = \\int f(x) \\frac{p(x)}{q(x)} q(x) \\, dx\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"pGIW62ib\"\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7Bp%28x%29%7D%7Bq%28x%29%7D\" alt=\"\\frac{p(x)}{q(x)}\" eeimg=\"1\"\u002F\u003E 称为\u003Cb\u003E重要性权重（Importance Weight）\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"XLQK9UL-\"\u003E\u003Cb\u003E重要性采样的作用是什么？\u003C\u002Fb\u003E为什么需要它？\u003C\u002Fp\u003E\u003Cp data-pid=\"mp8HaLYw\"\u003E重要性采样允许我们使用与目标分布不同的采样分布，从而：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"lB84Q0ec\"\u003E提高采样效率，尤其在\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"\u002F\u003E较难采样的情况下；\u003C\u002Fli\u003E\u003Cli data-pid=\"N3FAwmkY\"\u003E\u003Cb\u003E在强化学习中，它允许我们基于旧策略生成的经验数据来优化新策略，而无需重新采样环境数据。\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003E\u003Cb\u003EBradley-Terry Model\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp data-pid=\"wAz0r8fg\"\u003E\u003Cb\u003EThe Bradley-Terry Model\u003C\u002Fb\u003E 是一种统计模型，用于处理成对比较（pairwise comparisons）的问题，目的是估计多个对象之间相对偏好的概率。例如，给定两项选择  A  和  B ，该模型估计 A 相对于 B 被偏好的概率。\u003C\u002Fp\u003E\u003Cp data-pid=\"b-SIa9F9\"\u003E\u003Cb\u003EBradley-Terry Model 的基本形式\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"DD50RtGS\"\u003E对于两个对象 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E ，模型定义对象  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E  被偏好的概率为：\u003C\u002Fp\u003E\u003Cp data-pid=\"GbfGv6fi\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28i+%5Csucc+j%29+%3D+%5Cfrac%7B%5Cexp%28%5Cbeta_i%29%7D%7B%5Cexp%28%5Cbeta_i%29+%2B+%5Cexp%28%5Cbeta_j%29%7D\" alt=\"P(i \\succ j) = \\frac{\\exp(\\beta_i)}{\\exp(\\beta_i) + \\exp(\\beta_j)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"EvxoSIt8\"\u003E其中：\u003C\u002Fp\u003E\u003Cp data-pid=\"C1rDQ-pO\"\u003E • \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cbeta_i\" alt=\" \\beta_i\" eeimg=\"1\"\u002F\u003E  和  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_j\" alt=\"\\beta_j\" eeimg=\"1\"\u002F\u003E  分别是对象  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E  和  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E  的潜在得分（latent score）。\u003C\u002Fp\u003E\u003Cp data-pid=\"u9G7vn7P\"\u003E • 该模型的核心假设是：偏好的概率仅由对象的潜在得分决定，且满足\u003Cb\u003E逻辑斯谛分布\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"J6V7-UaZ\"\u003E\u003Cb\u003EBradley-Terry Model 的用途\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"80GZ8Cmw\"\u003E • 该模型被广泛用于排序和评分系统，如体育比赛、推荐系统、搜索结果排序等。\u003C\u002Fp\u003E\u003Cp data-pid=\"MBGzjdhz\"\u003E • 在强化学习中，它可以用于\u003Cb\u003E建模奖励函数\u003C\u002Fb\u003E，特别是在基于偏好反馈的场景下。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003EREINFORCE 和 PPO的区别和联系\u003C\u002Fh3\u003E\u003Cp data-pid=\"K2W_6Wt3\"\u003E在强化学习中，REINFORCE 和 PPO 同属于\u003Cb\u003E基于策略梯度\u003C\u002Fb\u003E的方法，但在\u003Cb\u003E更新方式\u003C\u002Fb\u003E和\u003Cb\u003E稳定性\u003C\u002Fb\u003E上有本质差异。\u003C\u002Fp\u003E\u003Cp data-pid=\"gXjxqbCI\"\u003E\u003Cb\u003EREINFORCE\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"ituwSB9S\"\u003E\u003Cb\u003E特点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"PtRx1dba\"\u003E\u003Cb\u003E最简单的策略梯度算法，也称“蒙特卡洛策略梯度”\u003C\u002Fb\u003E，在每个回合结束后，直接基于整条轨迹的回报来更新策略。\u003C\u002Fli\u003E\u003Cli data-pid=\"ko7ayuP0\"\u003E 更新时使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clog+%5Cpi_%5Ctheta%28a_t+%5Cmid+s_t%29\" alt=\"\\log \\pi_\\theta(a_t \\mid s_t)\" eeimg=\"1\"\u002F\u003E 乘以当时获得的总回报，获取策略梯度。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"35Ads88L\"\u003E\u003Cb\u003E优点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"LhMRiT4B\"\u003E 理论推导相对直接，概念简单，常用于基础教学和小规模实验。\u003C\u002Fli\u003E\u003Cli data-pid=\"68MhPEPk\"\u003E 对问题没有过多结构假设，容易与其他方法结合（如引入基线减小方差）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"1JDwNQwa\"\u003E\u003Cb\u003E缺点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"I3337cst\"\u003E\u003Cb\u003E高方差\u003C\u002Fb\u003E：需要大量样本才能得到稳定的梯度估计。\u003C\u002Fli\u003E\u003Cli data-pid=\"JGzQKZtE\"\u003E\u003Cb\u003E效率较低\u003C\u002Fb\u003E：一次采样后便更新，不能多次利用同一批数据。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"3go08G_w\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"-hRq-9U5\"\u003E\u003Cb\u003E特点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"sD2d4rzB\"\u003E 属于 Trust Region Policy Optimization 的简化版，通过限制新旧策略的更新幅度来保证训练稳定性。\u003C\u002Fli\u003E\u003Cli data-pid=\"wo5etXnf\"\u003E 训练时，可在同一批数据上做多次迭代更新，但会对新旧策略的概率比率施加“剪切”或“KL 惩罚”，防止一步迈得过大。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"0VLrAkw2\"\u003E\u003Cb\u003E优点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"SGhQTcEM\"\u003E\u003Cb\u003E训练更稳定\u003C\u002Fb\u003E：限制策略更新，使得学习过程不易崩溃。\u003C\u002Fli\u003E\u003Cli data-pid=\"NjemZq-Z\"\u003E\u003Cb\u003E样本利用率更高\u003C\u002Fb\u003E：对同一批次数据多次梯度更新，提高效率。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"mLYoGW8g\"\u003E\u003Cb\u003E缺点\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"qgIeZGhS\"\u003E 实现与调参相对更复杂，需要合适的超参数（如剪切阈值、KL 系数等）。\u003C\u002Fli\u003E\u003Cli data-pid=\"Ywi9JH_f\"\u003E 仍是 on-policy 方法，对离线数据的利用相对有限。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ouVdfIFl\"\u003E\u003Cb\u003EREINFORCE 和 PPO的核心区别\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"24aNXzdD\"\u003E\u003Cb\u003E更新方式\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"vOchvYJO\"\u003E REINFORCE：整条轨迹结束后一次性更新，仅用该次采样的数据。\u003C\u002Fli\u003E\u003Cli data-pid=\"vgEYoVCu\"\u003E PPO：可以多次使用同一批数据，用截断或 KL 惩罚约束更新。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"i3NZzLYi\"\u003E\u003Cb\u003E稳定性与效率\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"6stfEqrY\"\u003E REINFORCE：简单但高方差、低效率。\u003C\u002Fli\u003E\u003Cli data-pid=\"4wWYhHVT\"\u003E PPO：限制策略变化幅度，显著提升稳定性与收敛速度。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"CICHH4rZ\"\u003E因此，\u003Cb\u003EREINFORCE\u003C\u002Fb\u003E 常被视为最基础的策略梯度方法，适合讲解原理；而 \u003Cb\u003EPPO\u003C\u002Fb\u003E 则在实践中更常见，因其能在保持收敛性和稳定性的同时充分利用数据。\u003C\u002Fp\u003E\u003Ch3\u003E强化学习（RL）相比有监督微调（SFT）有哪些好处？\u003C\u002Fh3\u003E\u003Cp data-pid=\"S5Z58pmR\"\u003E我们知道RL对于提升LLM的性能有很重要的作用，那么具体作用是什么呢？你真的知道为什么要用RL吗？RL的必要性在哪里？\u003C\u002Fp\u003E\u003Cp data-pid=\"eoXCtmYX\"\u003E\u003Cb\u003E第一点，RL相比SFT更有可能考虑整体影响。\u003C\u002Fb\u003ESFT是针对单个token进行反馈，其目标是要求模型针对给定的输入给出确切的答案；而RL是针对整个输出文本进行反馈，并不针对单个token。这种反馈粒度的不同，使得RL既可以兼顾表达的多样性，又可以增强对微小变化的敏感性。由于自然语言的灵活性，相同的语义可以用不同的方式表达，SFT很难兼顾，而RL可以允许模型给出不同的多样性表达。此外SFT采用交叉熵损失，由于总和规则，总的Loss对个别词元的变化不敏感，也就是说改变个别词元对整体损失影响较小，但是在语言中，一个否定词就可以完全改变文本的整体含义。\u003Cb\u003ERL可以通过奖励函数同时兼顾多样性和微小变化敏感性两个方面。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ME3-P0Qr\"\u003E\u003Cb\u003E第二点，RL更容易解决幻觉问题\u003C\u002Fb\u003E。在模型不知道答案的情况下，SFT会促使模型给出答案。而使用RL则可以通过定制奖励函数，使得正确答案有很高的分数，放弃回答或回答不知道有中低分数，不正确的回答有非常高的负分，这样模型可以依赖内部知道选择放弃回答，从而缓解幻觉问题。\u003C\u002Fp\u003E\u003Cp data-pid=\"9mqiDZNM\"\u003E\u003Cb\u003E第三点，RL可以更好地解决多轮对话奖励累计的问题\u003C\u002Fb\u003E。多轮对话的好坏，要考虑多次交互的整体情况，很难用SFT的方法构建，而使用RL，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断。\u003C\u002Fp\u003E\u003Cp data-pid=\"BvcOUrAx\"\u003E此外，还可以参考这篇文章：\u003C\u002Fp\u003E\u003Cp data-pid=\"_EL2zGa4\"\u003E\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F651021172\u002Fanswer\u002F3513159005?utm_psn=1778953635276931072\" class=\"internal\" target=\"_blank\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ezhihu.com\u002Fquestion\u002F6510\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E21172\u002Fanswer\u002F3513159005?utm_psn=1778953635276931072\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003E大模型的3H原则是什么？\u003C\u002Fh3\u003E\u003Cp data-pid=\"rAWcNJvI\"\u003E大模型的3H原则是：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"U0LEGn-i\"\u003E帮助性（Helpfulness）\u003C\u002Fli\u003E\u003Cli data-pid=\"jl926ZPR\"\u003E真实性（Honesty）\u003C\u002Fli\u003E\u003Cli data-pid=\"KSwsJHx-\"\u003E无害性（Harmless）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E对齐税（Alignment Tax）\u003C\u002Fh3\u003E\u003Cp data-pid=\"-O63nlor\"\u003ERLHF 微调通常伴随着多样性和语言流畅性的下降，这种现象被称为 \u003Cb\u003Ealignment tax【21，22】\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"prOyKb16\"\u003E\u003Cb\u003E分布外泛化能力（\u003C\u002Fb\u003Eout-of-distribution (OOD) generalisation\u003Cb\u003E）\u003C\u002Fb\u003E在模型面临广泛的现实场景时至关重要，而\u003Cb\u003E输出多样性（\u003C\u002Fb\u003Eoutput diversity\u003Cb\u003E）\u003C\u002Fb\u003E则指模型生成多样化输出的能力。论文【22】研究发现，\u003Cb\u003E与SFT相比，RLHF在处理新输入时的泛化能力更强，尤其是在训练数据与测试数据之间分布偏移较大的情况下\u003C\u002Fb\u003E。然而，\u003Cb\u003E与SFT相比，RLHF在多种衡量标准下显著降低了输出多样性\u003C\u002Fb\u003E，这表明当前LLM微调方法在泛化能力和多样性之间存在权衡。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003EReward Hacking\u003C\u002Fh3\u003E\u003Cp data-pid=\"si8wNALG\"\u003E在RL中，由于reward function设置不合理，导致agent只关心累计奖励，而并没有朝着预想的目标优化【24】。\u003C\u002Fp\u003E\u003Cp data-pid=\"TBPDL0px\"\u003E“上有政策，下有对策”，下面这个故事可以很形象地展示Reward Hacking的问题：\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"qNPhwSas\"\u003E在印度殖民时期，政府为了减少眼镜蛇数量，悬赏每捕捉一条眼镜蛇给予奖励。一开始，人们积极捕蛇换钱，但后来一些人发现可以养蛇并假装捕捉，以赚取更多奖励。\u003Cbr\u002F\u003E政府发现后取消了奖励政策，结果养蛇的人将蛇全部放生，导致蛇的数量比最初更多。\u003Cbr\u002F\u003E\u003Cb\u003E寓意\u003C\u002Fb\u003E：这是奖励机制设计失误（Reward Hacking）的典型案例，说明如果奖励不精准，会导致行为偏离初衷，甚至适得其反。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"ZEDBvj74\"\u003E在RLHF中，奖励模型每次偏好的数据应该是针对我们优化目标更好的回答，但如果这些更好的回答都是更长的回答，那么模型在学习时会取巧，偏向于更长的回答。而在真正使用时，更长的回答未必更好。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003ERLHF（RM+PPO）\u003C\u002Fh2\u003E\u003Ch3\u003ERLHF的整体流程是什么？\u003C\u002Fh3\u003E\u003Cp data-pid=\"H96cm0Ep\"\u003ERLHF主要分为\u003Cb\u003E奖励模型训练\u003C\u002Fb\u003E和\u003Cb\u003E近端策略优化（Proximal Policy Optimization, PPO）\u003C\u002Fb\u003E两个步骤。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"QbLavdNO\"\u003E奖励模型通过由人类反馈标注的\u003Cb\u003E偏好数据\u003C\u002Fb\u003E来学习人类的偏好，判断模型回复的有用性以及保证内容的无害性。奖励模型模拟了人类的偏好信息，能够不断地为模型的训练提供奖励信号。\u003C\u002Fli\u003E\u003Cli data-pid=\"ZU7HkiDN\"\u003E在获得奖励模型后，需要借助强化学习对语言模型继续进行微调。近端策略优化可以根据奖励模型获得的反馈 优化模型，通过不断的迭代，让模型探索和发现更符合人类偏好的回复策略。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-fdd58ea4ecac5e65050979067271b89e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2037\" data-rawheight=\"1085\" data-original-token=\"v2-fdd58ea4ecac5e65050979067271b89e\" class=\"origin_image zh-lightbox-thumb\" width=\"2037\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-fdd58ea4ecac5e65050979067271b89e_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ERLHF整体流程【3】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003ERLHF中有哪些数据集？\u003C\u002Fh3\u003E\u003Cp data-pid=\"m1I99wIy\"\u003ERLHF中有三种数据集，分别是：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"HTul2V0E\"\u003E偏好数据集：用来训练奖励模型。\u003C\u002Fli\u003E\u003Cli data-pid=\"_sUppl-s\"\u003E提示数据集：在PPO流程中用来训练LLM。\u003C\u002Fli\u003E\u003Cli data-pid=\"u_tTAbFd\"\u003E评估数据集：用来评估RLHF的效果。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003EPPO中的4个模型及其作用\u003C\u002Fh3\u003E\u003Cp data-pid=\"bHchv8iU\"\u003EPPO涉及到actor\u002Fcritic\u002Freward\u002Freference 4个模型的协同训练和推理，这四个模型的详细解释如下：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"XLzwwkOG\"\u003E\u003Cb\u003E策略模型（Policy Model，或Actor Model）\u003C\u002Fb\u003E，生成模型回复。它就是RLHF训练希望产出的模型。在PPO训练中更新梯度。\u003C\u002Fli\u003E\u003Cli data-pid=\"5dHdi12W\"\u003E\u003Cb\u003E奖励模型（Reward Model）\u003C\u002Fb\u003E，输出奖励分数来评估回复质量的好坏。在PPO训练中不更新梯度。\u003C\u002Fli\u003E\u003Cli data-pid=\"UCFgNyXN\"\u003E\u003Cb\u003E评论模型（Critic Model，或Value Network）\u003C\u002Fb\u003E，来预测回复的好坏，可以在训练过程中实时调整模型，选择对未来累积收益最大的行为。在PPO训练中更新梯度。\u003C\u002Fli\u003E\u003Cli data-pid=\"t-fWfEDL\"\u003E\u003Cb\u003E参考模型（Reference Model）\u003C\u002Fb\u003E提供了一个 SFT 模型的备份，帮助模型不会出现过于极端的变化。在PPO训练中不更新梯度。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch3\u003EPPO的Policy模型的目标函数\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ce99342d89a1478e43d433f8a5ced58b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1239\" data-rawheight=\"1000\" data-original-token=\"v2-ce99342d89a1478e43d433f8a5ced58b\" class=\"origin_image zh-lightbox-thumb\" width=\"1239\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ce99342d89a1478e43d433f8a5ced58b_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EPPO的目标函数\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003E为什么需要训练奖励模型（RM）？\u003C\u002Fh3\u003E\u003Cp data-pid=\"yrOpv5wO\"\u003E在RL的流程中，利用人类标注的反馈数据也可以对模型进行微调，但是，由于时间和成本的限制，针对每次优化迭代，人类很难提供足够的反馈。所以训练一个奖励模型是一个更为有效的方法，利用奖励模型模拟人类的评估过程。\u003C\u002Fp\u003E\u003Cp data-pid=\"P1ihUikr\"\u003ERM决定了智能体如何从与环境的交互中学习并优化策略，以实现预定的目标，因此训练一个RM模型非常重要。\u003C\u002Fp\u003E\u003Ch3\u003E奖励模型（RM）是怎么训练的？\u003C\u002Fh3\u003E\u003Cp data-pid=\"DBAfnpW2\"\u003E\u003Cb\u003E奖励模型（RM）的训练流程如下：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"dYh1B-Dz\"\u003E 1. \u003Cb\u003E收集偏好数据\u003C\u002Fb\u003E：用初始模型生成多个回答，让人类标注者进行两两比较或打分，从而获得「更优回答」的偏好数据。\u003C\u002Fp\u003E\u003Cp data-pid=\"SPasVBWt\"\u003E 2. \u003Cb\u003E对偶对比训练\u003C\u002Fb\u003E：令奖励模型输出一个标量评分 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28%5Ctext%7B%E5%9B%9E%E7%AD%94%7D%29\" alt=\"R(\\text{回答})\" eeimg=\"1\"\u002F\u003E ；对同一问题的回答A和回答B，若人类偏好A，则训练目标是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28A%29+%3E+R%28B%29\" alt=\"R(A) &gt; R(B)\" eeimg=\"1\"\u002F\u003E ，常用对数似然形式的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%28R%28A%29+-+R%28B%29%29\" alt=\"\\sigma(R(A) - R(B))\" eeimg=\"1\"\u002F\u003E 来表示。\u003C\u002Fp\u003E\u003Cp data-pid=\"PwitNCgc\"\u003E 3. \u003Cb\u003E模型结构\u003C\u002Fb\u003E：通常在预训练语言模型的基础上添加一个“Reward Head”，对文本输出打分。\u003C\u002Fp\u003E\u003Cp data-pid=\"SHDWaV79\"\u003E 4. \u003Cb\u003E使用场景\u003C\u002Fb\u003E：在PPO等强化学习阶段，用奖励模型为生成的回答打分，并将其作为奖励信号来优化策略模型，使生成结果更符合人类偏好。\u003C\u002Fp\u003E\u003Ch3\u003ERM的损失函数\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e3dd361d9d10aac42a1e7e5d313ce754_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2195\" data-rawheight=\"1000\" data-original-token=\"v2-e3dd361d9d10aac42a1e7e5d313ce754\" class=\"origin_image zh-lightbox-thumb\" width=\"2195\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e3dd361d9d10aac42a1e7e5d313ce754_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E奖励模型RM的损失函数\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003EPPO的流程是什么\u003C\u002Fh3\u003E\u003Cp data-pid=\"d2e0Cbie\"\u003EPPO的实施流程如下：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"pD4VBPRv\"\u003E环境采样：\u003Cb\u003E策略模型\u003C\u002Fb\u003E基于给定输入生成一系列的回复，\u003Cb\u003E奖励模型\u003C\u002Fb\u003E则对这些回复进行打分获得奖励。\u003C\u002Fli\u003E\u003Cli data-pid=\"5COOhlTY\"\u003E优势估计：利用\u003Cb\u003E评论模型\u003C\u002Fb\u003E预测生成回复的未来累积奖励，并借助广义优势估计（Generalized Advantage Estimation，GAE）算法来估计优势函数，能够有助于更准确地评估每次行动的好处。\u003C\u002Fli\u003E\u003Cli data-pid=\"TOG0Eh_A\"\u003E优化调整：使用优势函数来优化和调整\u003Cb\u003E策略模型\u003C\u002Fb\u003E，同时利用\u003Cb\u003E参考模型\u003C\u002Fb\u003E确保更新的策略不会有 太大的变化，从而维持模型的稳定性。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-36d53576eb29495894221e42cc0654b9_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"608\" data-original-token=\"v2-36d53576eb29495894221e42cc0654b9\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-36d53576eb29495894221e42cc0654b9_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ERLHF中PPO的流程【4】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Chr\u002F\u003E\u003Cp data-pid=\"MIDBUP8N\"\u003E作为PPO流程的补充，参考下面这张图，可以划分为\u003Cb\u003E经验采样\u003C\u002Fb\u003E和\u003Cb\u003E训练\u003C\u002Fb\u003E两个阶段【5】。\u003C\u002Fp\u003E\u003Cp data-pid=\"e7aK3ciV\"\u003E\u003Cb\u003E第一阶段经验采样\u003C\u002Fb\u003E，也叫做rollout，就是滑跑的意思，在RL中可以理解为一次实验，也就是actor模型在当前环境下进行策略动作的过程，具体实现上就是actor模型根据prompt数据集进行generate生成response，然后根据prompt+response进行forward计算，得到logp\u002Fvalues\u002Freward等元素，这里\u003Cb\u003E涉及到actor、reference、critic、reward 4个模型的推理过程\u003C\u002Fb\u003E；\u003C\u002Fp\u003E\u003Cp data-pid=\"ebU4wxVZ\"\u003E\u003Cb\u003E第二阶段就是训练流程\u003C\u002Fb\u003E，涉及到\u003Cb\u003Eactor和critic两个模型\u003C\u002Fb\u003E，从计算loss来看，算法上是相互独立的，本质上是两个模型独立训练。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ece21d083fbd4ac15d6ca16d8a787de7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"778\" data-rawheight=\"704\" data-original-token=\"v2-ece21d083fbd4ac15d6ca16d8a787de7\" class=\"origin_image zh-lightbox-thumb\" width=\"778\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ece21d083fbd4ac15d6ca16d8a787de7_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EPPO训练流程【5】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003Ecritic model和reward model有什么区别\u003C\u002Fh3\u003E\u003Cp data-pid=\"B0PMK7wD\"\u003E在 RLHF 中，\u003Cb\u003EReward Model\u003C\u002Fb\u003E 通过人类标注数据进行独立训练，旨在输出一个反映人类偏好的外部奖励分数，用于评估完整回复的质量；而 \u003Cb\u003ECritic Model\u003C\u002Fb\u003E 则与策略模型同步更新，侧重在训练过程中动态估算生成过程的价值或优势，帮助策略模型迭代优化。前者提供整体的外部评估信号，后者主要在内部学习如何预测长期回报。\u003C\u002Fp\u003E\u003Ch3\u003EPPO中优势函数指什么\u003C\u002Fh3\u003E\u003Cp data-pid=\"84d1NHKA\"\u003E在PPO算法中，\u003Cb\u003E优势函数（Advantage Function）用于评估状态-动作对的相对优劣程度。它衡量了执行某个动作相对于平均水平的优劣，即在给定状态下采取某个动作相对于采取平均动作的效果。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"HsQQUVUL\"\u003E优势函数可以用以下方式定义： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Advantage%28s%2C+a%29+%3D+Q%28s%2C+a%29+-+V%28s%29\" alt=\"Advantage(s, a) = Q(s, a) - V(s)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"RLVwjiC9\"\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Advantage%28s%2C+a%29\" alt=\"Advantage(s, a)\" eeimg=\"1\"\u002F\u003E 表示在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下采取动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 的优势函数值， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2C+a%29\" alt=\"Q(s, a)\" eeimg=\"1\"\u002F\u003E 表示状态动作对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%2C+a%29\" alt=\"(s, a)\" eeimg=\"1\"\u002F\u003E 的动作值函数（也称为动作优势函数）， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 表示状态值函数。\u003C\u002Fp\u003E\u003Cp data-pid=\"QbKEe_2s\"\u003E优势函数的作用在于帮助评估当前动作的相对价值，以便在策略更新过程中确定应采取的动作。\u003Cb\u003E通过比较不同动作的优势函数值，可以决定哪些动作是更好的选择\u003C\u002Fb\u003E。正的优势函数值表示执行的动作比平均水平更好，而负的优势函数值表示执行的动作比平均水平更差。\u003C\u002Fp\u003E\u003Cp data-pid=\"ucDp5C6s\"\u003E在PPO算法中，\u003Cb\u003E优势函数用于计算策略更新的目标，以便调整策略概率分布来提高优势函数为正的动作的概率，并降低优势函数为负的动作的概率，从而改进策略的性能\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"PLHYR_ul\"\u003E总而言之，优势函数在PPO算法中用于评估状态-动作对的相对优劣，帮助确定应该采取的动作，并在策略更新过程中引导策略向更优的方向调整。\u003C\u002Fp\u003E\u003Ch3\u003EGAE是什么？\u003C\u002Fh3\u003E\u003Cp data-pid=\"TEJ1uuTg\"\u003E\u003Cb\u003EGAE（Generalized Advantage Estimation）\u003C\u002Fb\u003E在RLHF中是一种用来估计优势函数（Advantage Function）的方法，\u003Cb\u003E目的是在策略梯度更新时减少方差并保持较小的偏差\u003C\u002Fb\u003E。它通过在时间差分（TD）误差之上叠加一个带衰减因子（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E ）的加权和，得到平滑且稳定的优势估计，从而帮助强化学习算法（如PPO）更有效地利用来自人类反馈的奖励信号进行训练。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 越接近 1，\u003C\u002Fb\u003E代表将更多的未来奖励纳入考虑，优势估计会更加依赖长时间序列的回报，\u003Cb\u003E偏差更小\u003C\u002Fb\u003E但\u003Cb\u003E方差更高\u003C\u002Fb\u003E。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 越接近 0，\u003C\u002Fb\u003E代表更依赖单步TD误差，优势估计主要由局部信息决定，\u003Cb\u003E偏差更大\u003C\u002Fb\u003E但\u003Cb\u003E方差更小\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Ch3\u003E重要性采样和clip\u003C\u002Fh3\u003E\u003Cp data-pid=\"CuWzGGKf\"\u003E在PPO算法中，重要性采样（Importance Sampling）\u003Cb\u003E和\u003C\u002Fb\u003E剪辑（Clipping）机制是确保策略更新稳定性和效率的关键。\u003C\u002Fp\u003E\u003Cp data-pid=\"8WMcUCPy\"\u003E\u003Cb\u003E1. 重要性采样（Importance Sampling）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ihypdqSn\"\u003E在强化学习中，策略的更新需要利用从环境中采集的数据。然而，直接使用新策略进行采样可能效率低下。为此，PPO采用重要性采样技术，利用旧策略下采集的数据来估计新策略的期望。\u003C\u002Fp\u003E\u003Cp data-pid=\"e9XrRu0_\"\u003E具体而言，重要性采样比率定义为：\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpi_%5Ctheta%28y_t+%5Cmid+s_t%29%7D%7B%5Cpi_%7B%5Ctext%7Bold%7D%7D%28y_t+%5Cmid+s_t%29%7D\" alt=\"\\frac{\\pi_\\theta(y_t \\mid s_t)}{\\pi_{\\text{old}}(y_t \\mid s_t)}\" eeimg=\"1\"\u002F\u003E 。其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_%5Ctheta%28y_t+%5Cmid+s_t%29\" alt=\"\\pi_\\theta(y_t \\mid s_t)\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_%7B%5Ctext%7Bold%7D%7D%28y_t+%5Cmid+s_t%29\" alt=\"\\pi_{\\text{old}}(y_t \\mid s_t)\" eeimg=\"1\"\u002F\u003E 分别表示新旧策略， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y_t\" alt=\"y_t\" eeimg=\"1\"\u002F\u003E 和  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"\u002F\u003E 分别表示动作和状态。通过该比率，PPO能够调整旧策略下的采样数据，使其适用于新策略的更新，从而提高数据利用率。\u003C\u002Fp\u003E\u003Cp data-pid=\"M18jZsI7\"\u003E\u003Cb\u003E2. 剪辑机制（Clipping）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"h7oP6XVB\"\u003E尽管重要性采样提高了数据利用效率，但如果新旧策略差异过大，可能导致训练过程不稳定。为此，PPO引入剪辑机制，限制重要性采样比率的变化范围，防止策略更新过度。具体而言，在PPO的目标函数中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 是一个小的正数，用于限制比率的偏离程度。通过这种剪辑操作，PPO有效地限制了策略更新的幅度，确保训练过程的稳定性。\u003C\u002Fp\u003E\u003Cp data-pid=\"70BKcub_\"\u003E\u003Cb\u003E3. 重要性采样与clip的协同作用\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"7FJH9HOj\"\u003E重要性采样和clip在PPO中协同工作：\u003C\u002Fp\u003E\u003Cp data-pid=\"cKKKahr3\"\u003E • \u003Cb\u003E重要性采样\u003C\u002Fb\u003E允许使用旧策略的数据来估计新策略的性能，提高数据利用效率。\u003C\u002Fp\u003E\u003Cp data-pid=\"eRiSzfLB\"\u003E • \u003Cb\u003Eclip\u003C\u002Fb\u003E限制新旧策略之间的差异，防止策略更新过度，确保训练过程的稳定性。\u003C\u002Fp\u003E\u003Cp data-pid=\"DN8MGPFu\"\u003E通过这两者的结合，PPO实现了在策略优化中的高效性和稳定性。\u003C\u002Fp\u003E\u003Ch3\u003Erollout是什么意思?\u003C\u002Fh3\u003E\u003Cp data-pid=\"Hmcnevb4\"\u003E“Rollout”这个词来源于强化学习和搜索算法中的一个常见术语，意思是“展开”或“铺开”。在RL中，rollout指的是从某个初始状态开始，按照当前策略一步步“展开”或“走出”一个完整的行为轨迹（也称为一次实验或episode）。这种过程就像将一张纸缓缓展开一样，把从当前策略得到的决策序列“铺”出来，以便后续评估整个轨迹的表现（如计算奖励、优势等），为策略优化提供样本数据。\u003C\u002Fp\u003E\u003Cp data-pid=\"e6TsGWgU\"\u003E\u003Cb\u003E在RLHF-PPO中，“rollout”指的是利用当前策略模型（actor）在给定的输入（prompt）下生成一系列回复，并对这些回复进行评估（如计算log概率、价值估计和奖励）的过程\u003C\u002Fb\u003E。这一过程相当于在环境中进行一次完整的实验，为后续利用优势估计和PPO算法优化策略提供必要的样本数据，同时确保更新时参考RM和reference模型的信息，以维持整体模型的稳定性。\u003C\u002Fp\u003E\u003Ch3\u003Emake_experience和rollout、Episode有什么区别与联系\u003C\u002Fh3\u003E\u003Cp data-pid=\"pYQ7oydB\"\u003E在RLHF-PPO的语境下，这三个术语都涉及生成和收集与环境交互的数据，但侧重点略有不同：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Fl2By-i1\"\u003E\u003Cb\u003Emake_experience\u003C\u002Fb\u003E：这是一个泛指的说法，描述了让模型与环境交互、生成经验数据的整个过程。它强调“制造经验”，也就是收集状态、动作、奖励等数据，为后续训练提供素材。\u003C\u002Fli\u003E\u003Cli data-pid=\"FHJi95Qa\"\u003E\u003Cb\u003Erollout\u003C\u002Fb\u003E：特指利用当前策略模型在环境中生成一段轨迹的过程。这段轨迹可以是完整的，也可以只是部分数据。Rollout更侧重于“展开”策略执行的具体过程，用来评估策略或计算优势等。\u003C\u002Fli\u003E\u003Cli data-pid=\"OvJmN-Yf\"\u003E\u003Cb\u003Eepisode\u003C\u002Fb\u003E：指的是从环境的初始状态开始，到达到终止条件为止的一整段交互过程，是一种完整的轨迹。它是rollout的一种特殊情况，即rollout生成的是一个完整的体验序列。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"5KyZVVSZ\"\u003E\u003Cb\u003E联系与区别\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"082qBSXN\"\u003E\u003Cb\u003E联系\u003C\u002Fb\u003E：三者都涉及通过策略与环境交互来收集数据。通常，make_experience的过程可能就是通过执行rollout来实现，而如果rollout生成的是从开始到结束的完整轨迹，那么它就构成了一个episode。\u003C\u002Fli\u003E\u003Cli data-pid=\"27LuF9vl\"\u003E\u003Cb\u003E区别\u003C\u002Fb\u003E：make_experience是一个更宽泛的概念，强调整个数据收集过程；rollout强调的是生成轨迹的动作，不一定要求轨迹完整；而episode专指完整的体验序列。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"wXB0qlcP\"\u003E总的来说，make_experience包含了通过rollout生成经验，而rollout有时可能只是一部分episode，三者在数据生成过程中各有侧重。\u003C\u002Fp\u003E\u003Ch3\u003EPPO的痛点是什么\u003C\u002Fh3\u003E\u003Cp data-pid=\"MFru9wxY\"\u003E算法上的痛点，调参困难，不容易训出好结果。\u003C\u002Fp\u003E\u003Cp data-pid=\"gb5WAWnY\"\u003E工程实现上的痛点：涉及到多阶段的dataloader（prompt和rollout、train），actor模型的generate自回归过程，以及4个不同的模型推理，2个不同的模型训练。\u003C\u002Fp\u003E\u003Ch3\u003E有哪些PPO的技巧\u003C\u002Fh3\u003E\u003Cp data-pid=\"2sAN6bb8\"\u003E参考文档【18】。\u003C\u002Fp\u003E\u003Ch2\u003EDPO【11】\u003C\u002Fh2\u003E\u003Ch3\u003EDPO的基本原理\u003C\u002Fh3\u003E\u003Cp data-pid=\"rRJAXfNK\"\u003EDPO的基本原理：\u003Cb\u003E增加偏好样本的对数概率与减小非偏好样本响应的对数概率\u003C\u002Fb\u003E。它结合了\u003Cb\u003E动态加权机制\u003C\u002Fb\u003E，以避免仅使用概率比目标时遇到的模型退化问题。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ee3c677200c094464dcd5e23ebd2c57d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2108\" data-rawheight=\"444\" data-original-token=\"v2-ee3c677200c094464dcd5e23ebd2c57d\" class=\"origin_image zh-lightbox-thumb\" width=\"2108\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ee3c677200c094464dcd5e23ebd2c57d_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EDPO在优化人类偏好时避免了使用强化学习。现有RLHF方法通常会首先使用一个奖励模型（Reward Model）来拟合一个包含提示（Prompt）和人类对响应对（Response Pair）偏好的数据集，然后通过强化学习找到一个能够最大化该奖励模型的策略（Policy）。相比之下，DPO 直接以简单的分类目标优化最能满足偏好的\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cblockquote data-pid=\"DX5prXTc\"\u003EDPO在优化人类偏好时避免了使用强化学习。现有RLHF方法通常会首先使用一个奖励模型（Reward Model）来拟合一个包含提示（Prompt）和人类对响应对（Response Pair）偏好的数据集，然后通过强化学习找到一个能够最大化该奖励模型的策略（Policy）。相比之下，DPO 直接以简单的分类目标优化最能满足偏好的策略，通过拟合一个隐式奖励模型，其对应的最优策略可以以闭式形式（Closed Form）提取出来。\u003C\u002Fblockquote\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003EDPO的loss\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-aa334b22cfcb50fa82608e8d6a4bf0b9_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2334\" data-rawheight=\"1000\" data-original-token=\"v2-aa334b22cfcb50fa82608e8d6a4bf0b9\" class=\"origin_image zh-lightbox-thumb\" width=\"2334\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-aa334b22cfcb50fa82608e8d6a4bf0b9_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EDPO的损失函数\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"knUQhZTK\"\u003EDPO loss的代码：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"kn\"\u003Eimport\u003C\u002Fspan\u003E \u003Cspan class=\"nn\"\u003Etorch.nn.functional\u003C\u002Fspan\u003E \u003Cspan class=\"kn\"\u003Eas\u003C\u002Fspan\u003E \u003Cspan class=\"nn\"\u003EF\u003C\u002Fspan\u003E\n\u003Cspan class=\"k\"\u003Edef\u003C\u002Fspan\u003E \u003Cspan class=\"nf\"\u003Edpo_loss\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Epi_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eyw_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eyl_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ebeta\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E):\u003C\u002Fspan\u003E\n    \u003Cspan class=\"s2\"\u003E&#34;&#34;&#34;\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    pi_logps: policy logprobs, shape (B,)\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    ref_logps: reference model logprobs, shape (B,)\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    yw_idxs: preferred completion indices in [0, B-1], shape (T,)\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    beta: temperature controlling strength of KL penalty\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E    Each pair of (yw_idxs[i], yl_idxs[i]) represents the\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E      indices of a single preference pair.\n\u003C\u002Fspan\u003E\u003Cspan class=\"s2\"\u003E&#34;&#34;&#34;\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Epi_yw_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E  \u003Cspan class=\"n\"\u003Epi_yl_logps\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E  \u003Cspan class=\"n\"\u003Epi_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E[\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eyw_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E],\u003C\u002Fspan\u003E  \u003Cspan class=\"n\"\u003Epi_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E[\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eyl_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E]\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Eref_yw_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_yl_logps\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E[\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eyw_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E],\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E[\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Eyl_idxs\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E]\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Epi_logratios\u003C\u002Fspan\u003E  \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Epi_yw_logps\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E-\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Epi_yl_logps\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Eref_logratios\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_yw_logps\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E-\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_yl_logps\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Elosses\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E-\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003EF\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Elogsigmoid\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Ebeta\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E*\u003C\u002Fspan\u003E \u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Epi_logratios\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E-\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_logratios\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E))\u003C\u002Fspan\u003E\n    \u003Cspan class=\"n\"\u003Erewards\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E=\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Ebeta\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E*\u003C\u002Fspan\u003E \u003Cspan class=\"p\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Epi_logps\u003C\u002Fspan\u003E \u003Cspan class=\"o\"\u003E-\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Eref_logps\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"o\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"n\"\u003Edetach\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E()\u003C\u002Fspan\u003E\n    \u003Cspan class=\"k\"\u003Ereturn\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Elosses\u003C\u002Fspan\u003E\u003Cspan class=\"p\"\u003E,\u003C\u002Fspan\u003E \u003Cspan class=\"n\"\u003Erewards\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003EDPO相比PPO有哪些改进？\u003C\u002Fh3\u003E\u003Cp data-pid=\"XwghHmDV\"\u003E在RLHF（人类反馈的强化学习）的背景下，DPO（Direct Preference Optimization，直接偏好优化）和PPO（Proximal Policy Optimization，近端策略优化）是两种用于微调大型语言模型（LLM）的方法。以下是这两种方法的一些关键改进点：\u003C\u002Fp\u003E\u003Cp data-pid=\"gxfLX2A2\"\u003E\u003Cb\u003E简化的训练流程\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"JnQBi5HB\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：直接从人类偏好数据集中学习，不需要训练一个单独的奖励模型。这简化了训练流程，减少了计算资源的需求。\u003C\u002Fli\u003E\u003Cli data-pid=\"gHnUUslH\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E：通常需要训练一个奖励模型，然后使用这个模型来指导策略的优化，这需要更多的计算资源和时间。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"tI6H-pXB\"\u003E\u003Cb\u003E对超参数的鲁棒性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"2SZmKrAW\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：对超参数的变化更为鲁棒，不容易陷入局部最优，也不需要频繁调整超参数。\u003C\u002Fli\u003E\u003Cli data-pid=\"Lx8v6qLN\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E：对超参数的选择较为敏感，可能需要仔细调整以避免训练不稳定。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"uPoSUA79\"\u003E\u003Cb\u003E计算效率和效果\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"74PMejA_\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：有些情况下，DPO能够用更少的资源达到接近PPO的效果，在计算效率方面有优势。不过在适合RL scaling的场景下，DPO在效果上和PPO的差距还是比较大的。\u003C\u002Fli\u003E\u003Cli data-pid=\"r49GNP49\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E：虽然可以达到很高的性能，但通常需要更多的计算资源和数据。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"PwrxEVyf\"\u003E\u003Cb\u003E模型解释性和用户偏好的适应性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"jvUqy1OI\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：由于直接从用户偏好数据学习，DPO通常具有更好的解释性，更容易理解模型的决策过程，并且能够更灵活地适应不同用户的个性化偏好。\u003C\u002Fli\u003E\u003Cli data-pid=\"uLD9B-ZI\"\u003E\u003Cb\u003EPPO\u003C\u002Fb\u003E：由于其基于奖励的优化机制，PPO可能在解释模型的行为时遇到困难，特别是在复杂的任务中，其决策过程可能不那么直观。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003EDPO和SFT有什么相同点和不同点？\u003C\u002Fh3\u003E\u003Cp data-pid=\"B4uQtJC0\"\u003EDPO（Direct Preference Optimization）和SFT（Supervised Fine-Tuning）都是用于优化大型语言模型（LLMs）的方法，旨在改善模型生成文本的质量，使其更符合人类的偏好。下面是它们的一些相同点和不同点：\u003C\u002Fp\u003E\u003Cp data-pid=\"80Ddb9Us\"\u003E\u003Cb\u003E相同点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"bO4o8WSt\"\u003E\u003Cb\u003E目标导向\u003C\u002Fb\u003E：两者都旨在通过调整语言模型的参数来优化其输出，使之更加符合特定的目标或偏好。\u003C\u002Fli\u003E\u003Cli data-pid=\"s6SBB3m5\"\u003E\u003Cb\u003E数据依赖\u003C\u002Fb\u003E：DPO和SFT都\u003Cb\u003E依赖于外部数据来指导模型的微调过程\u003C\u002Fb\u003E（与之不同的是，PPO\u002FGRPO是依赖外部的reward来指导模型的训练)。DPO使用偏好数据，而SFT通常使用带有标签的示例数据。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"-LLdw9XE\"\u003E\u003Cb\u003E不同点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"roZi17cV\"\u003E\u003Cb\u003E数据形式与处理\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"vqr_FSK1\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：直接利用偏好数据进行优化，这种偏好数据可以体现为正负样本对比，无需显式构建奖励模型。它通过偏好反馈（如用户偏好评级）直接优化策略，使得模型学习到什么样的输出是更受欢迎或更符合预期的。\u003C\u002Fli\u003E\u003Cli data-pid=\"VO_FHcqk\"\u003E\u003Cb\u003ESFT\u003C\u002Fb\u003E：依赖于有监督的标签数据，即输入-输出对的形式，其中输出是人工标注的理想响应或文本。这种方法更像是传统的机器学习任务中的监督学习，通过已知正确答案来调整模型以减少预测误差。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"tJbp4mee\"\u003E\u003Cb\u003E优化目标与过程\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"uxRaPK1F\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：优化过程关注于\u003Cb\u003E最大化偏好数据下策略的表现\u003C\u002Fb\u003E，通过\u003Cb\u003E对比学习\u003C\u002Fb\u003E等技术直接在策略上进行优化，避免了额外的奖励函数建模步骤。\u003C\u002Fli\u003E\u003Cli data-pid=\"YZzNEN0N\"\u003E\u003Cb\u003ESFT\u003C\u002Fb\u003E：目标是使模型输出尽可能匹配给定的标签数据，优化目标通常是最大化似然或\u003Cb\u003E最小化交叉熵损失\u003C\u002Fb\u003E，这是一个直接的监督学习问题。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"VWznV7-2\"\u003E\u003Cb\u003E复杂度与灵活性\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"WDHUanVO\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：可能在处理复杂的偏好表达和非结构化的偏好数据上更有优势，因为它不依赖于精确的评分或奖励信号，而是通过比较学习。\u003C\u002Fli\u003E\u003Cli data-pid=\"ZdhLNCL9\"\u003E\u003Cb\u003ESFT\u003C\u002Fb\u003E：在任务明确且有高质量标注数据的情况下更为直接有效，但可能在处理模糊或高度主观的偏好表达上不如DPO灵活。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"_CEyHFGn\"\u003E\u003Cb\u003E应用场景\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"mzptlG18\"\u003E\u003Cb\u003EDPO\u003C\u002Fb\u003E：适合于那些偏好标准不易量化或者偏好数据可以通过比较获得的场景，比如艺术创作、个性化推荐等。\u003C\u002Fli\u003E\u003Cli data-pid=\"4tUU7hQd\"\u003E\u003Cb\u003ESFT\u003C\u002Fb\u003E：更适合于有明确正确答案或标准输出的任务，如问答系统、翻译任务等。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"rMMuojls\"\u003E综上所述，DPO和SFT各有侧重，选择哪种方法取决于具体任务的需求、数据的性质以及对模型输出质量的具体要求。\u003C\u002Fp\u003E\u003Ch2\u003EDPO改进版本\u003C\u002Fh2\u003E\u003Ch3\u003EIPO【19】\u003C\u002Fh3\u003E\u003Cp data-pid=\"nP8DSBhP\"\u003E\u003Cb\u003EDPO 的一个缺点是它在人类偏好数据集上很快就会过拟合\u003C\u002Fb\u003E。为了避免这种情况，谷歌 DeepMind 的研究人员引入了身份偏好优化（IPO），这种方法\u003Cb\u003E为 DPO 损失添加了一个正则，能够在不使用「提前停止」等技巧的情况下让模型收敛\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003EKTO【20】\u003C\u002Fh3\u003E\u003Cp data-pid=\"vZJnGp4s\"\u003E\u003Cb\u003EKahneman-Tversky优化（KTO）\u003C\u002Fb\u003E：KTO是一种基于人类心理认知过程的偏好优化算法。它通过分析人类在决策过程中的心理认知过程（如注意力分配、记忆提取等），来优化模型的输出。\u003Cb\u003EKTO的一个显著特点是它不需要成对的偏好数据，只需将样本标注为“好”或“坏”，降低了数据收集的成本和难度\u003C\u002Fb\u003E。然而，如何准确地模拟人类的心理认知过程，以及将这种模拟结果应用到实际场景中，仍是KTO需要解决的问题。\u003C\u002Fp\u003E\u003Ch2\u003EGRPO【12】\u003C\u002Fh2\u003E\u003Ch3\u003E相对PPO的改进\u003C\u002Fh3\u003E\u003Cp data-pid=\"VcYAnKtP\"\u003EGRPO是对PPO的一种改进版本，属于online RL。它通过暴力采样求均值的方式替代了PPO中的Critic Model，同时保留了PPO中的重要性采样和裁剪机制。GRPO中冻结了Ref和RM 2个模型，仅需要训练Policy Model。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1a0520d031941077b31cfc6bfec6b19c_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1950\" data-rawheight=\"836\" data-original-token=\"v2-1a0520d031941077b31cfc6bfec6b19c\" class=\"origin_image zh-lightbox-thumb\" width=\"1950\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1a0520d031941077b31cfc6bfec6b19c_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E图4 | PPO与我们提出的GRPO的演示。GRPO放弃了价值模型，而是通过小组得分估计基线，从而显著减少了训练资源的消耗。【12】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch3\u003E目标函数\u003C\u002Fh3\u003E\u003Cp data-pid=\"VzqaCHFi\"\u003EGRPO的目标函数\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-edb79bf548d7d181db49b0a4014a41ec_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2552\" data-rawheight=\"320\" data-original-token=\"v2-edb79bf548d7d181db49b0a4014a41ec\" class=\"origin_image zh-lightbox-thumb\" width=\"2552\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-edb79bf548d7d181db49b0a4014a41ec_r.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003EDAPO 【27】\u003C\u002Fh2\u003E\u003Cp data-pid=\"nY1_Lm7S\"\u003E这是字节跳动seed团队对GRPO的重要改进。\u003Cb\u003EGRPO 面临四个主要问题：其单一剪切范围易导致低概率 Token 无法得到有效提升（熵坍塌），样本级别损失会弱化长序列 Token 的梯度贡献，一旦所有输出都是全对或全错则无梯度信号可用，且统一惩罚过长生成会引入噪声\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"VdvbhZCY\"\u003EDAPO 则通过“四大改进”逐一解决：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"y5N6TXGt\"\u003E\u003Cb\u003EClip-Higher 将剪切上下限分开以保留多样性；\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"fp9LWN8F\"\u003E\u003Cb\u003EDynamic Sampling 过滤全对\u002F全错样本保持有效梯度；\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"p19_aGLL\"\u003E\u003Cb\u003EToken-Level Policy Gradient Loss 平衡长序列 Token 贡献；\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"mF_KhvWq\"\u003E\u003Cb\u003EOverlong Reward Shaping 为过长文本进行柔性惩罚或掩码，减少噪声干扰。\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-638ceea6aa03b5d88366ca081708628e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"448\" data-original-token=\"v2-638ceea6aa03b5d88366ca081708628e\" class=\"origin_image zh-lightbox-thumb\" width=\"836\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-638ceea6aa03b5d88366ca081708628e_r.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"7-EOZtEy\"\u003E详细可参考下面文章【28】。\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F1888687830710084683\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpica.zhimg.com\u002Fequation.jpg\" class=\"internal\"\u003E大家好我是爱因：DAPO：GRPO的问题分析及四个改进策略\u003C\u002Fa\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EDr. GRPO【29】\u003C\u002Fh2\u003E\u003Cp data-pid=\"9trQooiu\"\u003E作者认为\u003Cb\u003EGRPO算法在优化目标上存在的“长度偏置”会导致模型产生冗长但不一定正确的回答。为解决这一问题，作者提出了 Dr. GRPO，即在不牺牲推理性能的前提下消除不必要的冗长，显著提高了“token效率”（token efficiency）。基于此，他们给出了一份简化的 R1-Zero 训练配方\u003C\u002Fb\u003E：在 Qwen2.5-Math-7B 模型上仅用 27 小时就获得了在多项数学竞赛数据集上的最新最优结果（AIME2024 准确率达到 43.3%）。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-dd930f961bd3709bd5c6950bddb62e7f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1441\" data-rawheight=\"476\" data-original-token=\"v2-dd930f961bd3709bd5c6950bddb62e7f\" class=\"origin_image zh-lightbox-thumb\" width=\"1441\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-dd930f961bd3709bd5c6950bddb62e7f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 1：左图：Dr. GRPO 通过去除长度和标准差归一化项，对 GRPO（Shao 等，2024）进行了简单但重要的修改，以解决其偏差问题。右图：我们提出的无偏优化器有效地防止模型生成越来越长但错误的回答，从而提升了 token 效率。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ZXIFztiH\"\u003E详细可参考文章【30】。\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F1891404563971564359\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpicx.zhimg.com\u002Fequation.jpg\" class=\"internal\"\u003E大家好我是爱因：理解 R1-Zero 式训练：一个批判性视角\u003C\u002Fa\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003ERLOO【13，23】\u003C\u002Fh2\u003E\u003Cp data-pid=\"20LIQq13\"\u003E背景：在基于策略梯度的RL方法中，REINFORCE 和 PPO相比更加基础和简单（详情参考本文“REINFORCE 和 PPO的区别和联系”小节）。RLOO（REINFORCE Leave-One-Out）是REINFORCE的改进版。\u003C\u002Fp\u003E\u003Cp data-pid=\"iomRbuLK\"\u003ERLOO的提出者认为，由于RLHF的起点是训练好的SFT模型，而不是参数随机初始化的模型，在PPO中诸如GAE、CLIP这些用于稳定训练的策略是没必要的。\u003C\u002Fp\u003E\u003Cp data-pid=\"Ngo2tykI\"\u003E通过将整个回复生成当作一个action，并用样本间差异取代critic model，RLOO可以使训练更简单，性能也很好。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"opFiWjDV\"\u003E\u003Cb\u003ERLOO 的提出动机：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"2FmnZCOl\"\u003E\u003Cb\u003ERLHF和传统RL不同\u003C\u002Fb\u003E。相比传统的RL，RLHF中的初始policy更好，因此PPO中的很多假设和技巧是没必要的。\u003C\u002Fli\u003E\u003Cli data-pid=\"Q7o_dUhO\"\u003E\u003Cb\u003E降低计算资源需求\u003C\u002Fb\u003E：PPO 通常需要同时加载四个模型（actor\u002Fcritic\u002Freward\u002Freference），这对 GPU 内存提出了很高的要求。RLOO 通过减少所需模型的数量，降低了内存占用，使得训练过程更高效。\u003C\u002Fli\u003E\u003Cli data-pid=\"u0LWBaJr\"\u003E\u003Cb\u003E简化实现复杂性\u003C\u002Fb\u003E：PPO 的实现涉及许多微妙的细节，可能难以正确把握。RLOO 采用更直接的 REINFORCE 风格优化，减少了实现过程中的复杂性。\u003C\u002Fli\u003E\u003Cli data-pid=\"H5vuLr9H\"\u003E\u003Cb\u003E提高训练效率\u003C\u002Fb\u003E：RLOO 的设计使其能够使用更大的批量大小，缩短训练时间，从而加快模型的收敛速度。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-df12253f4eb3afa67788e97902f009dd_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2219\" data-rawheight=\"1184\" data-original-token=\"v2-df12253f4eb3afa67788e97902f009dd\" class=\"origin_image zh-lightbox-thumb\" width=\"2219\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-df12253f4eb3afa67788e97902f009dd_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E【23】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"CigqiqQl\"\u003E\u003Cb\u003ERLOO 与 PPO 的区别和优点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"-2z6ctzg\"\u003E\u003Cb\u003E模型副本数量\u003C\u002Fb\u003E：\u003Cb\u003EPPO 需要加载四个模型\u003C\u002Fb\u003E副本，而 \u003Cb\u003ERLOO 只需三个\u003C\u002Fb\u003E（actor\u002Freward\u002Freference），减少了对 GPU 内存的需求。\u003C\u002Fli\u003E\u003Cli data-pid=\"Wg0AWHM2\"\u003E\u003Cb\u003E动作建模方式\u003C\u002Fb\u003E：PPO 将每个生成的 token 视为单独的动作，而 RLOO 将整个生成序列视为一个动作。这种方式减少了稀疏奖励问题，提高了训练效率。\u003C\u002Fli\u003E\u003Cli data-pid=\"tVypmlym\"\u003E\u003Cb\u003E基线计算方法\u003C\u002Fb\u003E：RLOO 使用批次中其他样本的奖励作为基线，避免了训练价值模型的需要，进一步简化了实现过程。\u003C\u002Fli\u003E\u003Cli data-pid=\"PFH35poG\"\u003E\u003Cb\u003E训练速度和内存使用\u003C\u002Fb\u003E：根据实验，RLOO 在相同模型规模下，比 PPO 使用的 GPU 内存减少约 50-70%，训练速度提高 2-3 倍。【17】\u003C\u002Fli\u003E\u003Cli data-pid=\"SP9Sucv9\"\u003E\u003Cb\u003E性能表现\u003C\u002Fb\u003E：在响应质量方面，RLOO 与 PPO 相当，并且始终优于一些流行的离线方法。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"HJj1GER9\"\u003E综上所述，RLOO 通过简化模型架构、降低计算资源需求和提高训练效率，提供了一种比 PPO 更加高效且易于实现的在线强化学习算法。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-14032877acc55b3d9d01ab3499f852f5_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1439\" data-original-token=\"v2-14032877acc55b3d9d01ab3499f852f5\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-14032877acc55b3d9d01ab3499f852f5_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ERLOO介绍【16】\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EREINFORCE++【26】\u003C\u002Fh2\u003E\u003Ch3\u003E简介\u003C\u002Fh3\u003E\u003Cp data-pid=\"AswnOg3f\"\u003E\u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E是一种增强版的经典 \u003Cb\u003EREINFORCE\u003C\u002Fb\u003E 算法，结合了 \u003Cb\u003EPPO\u003C\u002Fb\u003E 的关键优化技术，同时去除了对 \u003Cb\u003Ecritic network\u003C\u002Fb\u003E 的依赖。\u003C\u002Fp\u003E\u003Cp data-pid=\"vnrCVO05\"\u003E\u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E 主要实现了三个核心目标：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"DjKiRVvJ\"\u003E简洁性\u003C\u002Fli\u003E\u003Cli data-pid=\"MIs0FjsU\"\u003E增强的训练稳定性\u003C\u002Fli\u003E\u003Cli data-pid=\"zgjj7l2n\"\u003E降低计算开销。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E优点\u003C\u002Fh3\u003E\u003Cp data-pid=\"0xmR-yi8\"\u003E论文中通过广泛的实证评估，证明了 \u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E 在稳定性上优于 \u003Cb\u003EGRPO\u003C\u002Fb\u003E，并且在保持与 \u003Cb\u003EPPO\u003C\u002Fb\u003E 相当的性能的同时，实现了更高的计算效率。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"6akfFT8n\"\u003E\u003Cb\u003E通用场景（采用 Bradley-Terry Reward Models）\u003C\u002Fb\u003E：\u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E 在稳定性方面优于 \u003Cb\u003EGRPO\u003C\u002Fb\u003E，特别是在防止奖励和输出长度 \u003Cb\u003Ehacking\u003C\u002Fb\u003E 方面（图 1）。\u003C\u002Fli\u003E\u003Cli data-pid=\"Fvjw1VYl\"\u003E\u003Cb\u003E基于规则的奖励模型（Rule-Based Reward Model）\u003C\u002Fb\u003E：在基于规则的奖励场景下，\u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E 与采用 \u003Cb\u003Egroup normalization\u003C\u002Fb\u003E 的 \u003Cb\u003EGRPO\u003C\u002Fb\u003E 取得了可比较的性能（图 2）。\u003C\u002Fli\u003E\u003Cli data-pid=\"mBC-UyI_\"\u003E\u003Cb\u003E数学奖励模型（Mathematical Reward Model）\u003C\u002Fb\u003E：在数学问题求解场景中，\u003Cb\u003EREINFORCE++\u003C\u002Fb\u003E 相较于 \u003Cb\u003EGRPO\u003C\u002Fb\u003E，在每单位 \u003Cb\u003EKL divergence\u003C\u002Fb\u003E 下实现了更大的奖励提升（图 3）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6b91567c45c1943e6e35ec7a0da091f3_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1840\" data-rawheight=\"756\" data-original-token=\"v2-6b91567c45c1943e6e35ec7a0da091f3\" class=\"origin_image zh-lightbox-thumb\" width=\"1840\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6b91567c45c1943e6e35ec7a0da091f3_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 1：通用领域结果表明，在采用 Bradley-Terry Reward Models 的一般场景中，PPO 和 REINFORCE++ 相较于 GRPO 具有较小的 length hacking 问题。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad2c6070600ba224252b4e055186c61_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1830\" data-rawheight=\"844\" data-original-token=\"v2-aad2c6070600ba224252b4e055186c61\" class=\"origin_image zh-lightbox-thumb\" width=\"1830\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad2c6070600ba224252b4e055186c61_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 2：数学场景 1 表明，在基于规则的奖励下，REINFORCE++ 与 GRPO (Group Norm) 取得了可比较的结果。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-29d4f8b0094b902902649c0fae9f0b63_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2002\" data-rawheight=\"920\" data-original-token=\"v2-29d4f8b0094b902902649c0fae9f0b63\" class=\"origin_image zh-lightbox-thumb\" width=\"2002\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-29d4f8b0094b902902649c0fae9f0b63_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 3：数学场景 2 的结果表明，在相同单位 KL 消耗下，REINFORCE++ 和 RLOO 相较于 GRPO (Group Norm) 实现了更大的奖励提升。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E优化方法\u003C\u002Fh3\u003E\u003Cp data-pid=\"PVVQq6jq\"\u003EReinforce++为提升训练稳定性和效率，融合了多种优化方法，从不同层面改进经典REINFORCE算法，在避免引入过多复杂度的同时，有效应对了RLHF面临的挑战。具体采用的方法如下： \u003C\u002Fp\u003E\u003Cp data-pid=\"UPlTIIfG\"\u003E1. \u003Cb\u003EToken-Level KL Penalty（令牌级KL散度惩罚）\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"RZTsL8gG\"\u003E在RL模型和监督微调（SFT）模型的分布之间实施令牌级KL散度惩罚，并将其纳入奖励函数。公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r%28s_%7Bt%7D%2C+a_%7Bt%7D%29+%3D+I%28s_%7Bt%7D%3D%5BEOS%5D%29+r%28x%2C+y%29-%5Cbeta+KL%28t%29\" alt=\"r(s_{t}, a_{t}) = I(s_{t}=[EOS]) r(x, y)-\\beta KL(t)\" eeimg=\"1\"\u002F\u003E ，其中\u003C\u002Fp\u003E\u003Cp data-pid=\"PbDAQgh2\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=KL%28t%29%3Dlog+%28%5Cfrac%7B%5Cpi_%7B%5Ctheta_%7Bold+%7D%7D%5E%7BRL%7D%28a_%7Bt%7D+%7C+s_%7Bt%7D%29%7D%7B%5Cpi%5E%7BSFT%7D%28a_%7Bt%7D+%7C+s_%7Bt%7D%29%7D%29\" alt=\"KL(t)=log (\\frac{\\pi_{\\theta_{old }}^{RL}(a_{t} | s_{t})}{\\pi^{SFT}(a_{t} | s_{t})})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"hWu3I4Yy\"\u003E这种方式有助于更合理地分配奖励，并且能与过程奖励模型（PRM）无缝集成，提升训练的稳定性。\u003C\u002Fp\u003E\u003Cp data-pid=\"SDH7_qun\"\u003E2. \u003Cb\u003EPPO-Clip Integration（集成PPO的裁剪机制）\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"u9Z-Pyqv\"\u003E引入近端策略优化（PPO）的裁剪机制来限制策略更新。通过公式 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L%5E%7BCLIP%7D%28%5Ctheta%29%3D%5Cmathbb%7BE%7D_%7Bt%7D%5Bmin+%28r_%7Bt%7D%28%5Ctheta%29+%5Chat%7BA%7D_%7Bt%7D%2C+clip%28r_%7Bt%7D%28%5Ctheta%29%2C+1-%5Cepsilon%2C+1%2B%5Cepsilon%29+%5Chat%7BA%7D_%7Bt%7D%29%5D\" alt=\"L^{CLIP}(\\theta)=\\mathbb{E}_{t}[min (r_{t}(\\theta) \\hat{A}_{t}, clip(r_{t}(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_{t})]\" eeimg=\"1\"\u002F\u003E ，\u003C\u002Fp\u003E\u003Cp data-pid=\"DLfq6tUD\"\u003E将概率比率 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_%7Bt%7D%28%5Ctheta%29\" alt=\"r_{t}(\\theta)\" eeimg=\"1\"\u002F\u003E 限制在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5B1-%5Cepsilon%2C+1%2B%5Cepsilon%5D\" alt=\"[1-\\epsilon, 1+\\epsilon]\" eeimg=\"1\"\u002F\u003E 范围内（通常 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%3D+0.2\" alt=\"\\epsilon = 0.2\" eeimg=\"1\"\u002F\u003E ）。这使得算法既能利用正向优势，又能防止因过大的更新导致训练不稳定，维持了策略更新的信任区域。 \u003C\u002Fp\u003E\u003Cp data-pid=\"0vPxI8qK\"\u003E3. \u003Cb\u003EMini-Batch Updates（小批量更新）\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"tUjh1-KX\"\u003E通过小批量处理数据提升训练效率。具体表现为数据以较小的、可管理的块进行处理，而非全量更新；每个小批量允许进行多次参数更新，加快收敛速度；引入随机优化，增加了有益的随机性，有助于模型更好地泛化。 \u003C\u002Fp\u003E\u003Cp data-pid=\"xTPLZ4lS\"\u003E4. \u003Cb\u003EReward Normalization and Clipping（奖励归一化和裁剪）\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"k1WX6tIV\"\u003E对奖励进行全面处理以稳定训练。通过Z -score归一化来标准化奖励，减少异常值的影响；将奖励值限制在预定义的范围内，避免训练不稳定；应用适当的缩放因子，确保在更新过程中的数值稳定性。 \u003C\u002Fp\u003E\u003Cp data-pid=\"7XxZR0jq\"\u003E5. \u003Cb\u003EAdvantage Normalization（优势归一化）\u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cp data-pid=\"MKCzKa9R\"\u003EReinforce++中优势函数定义为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_%7Bt%7D%28s_%7Bt%7D%2C+a_%7Bt%7D%29+%3D+r%28x%2C+y%29-%5Cbeta+%5Ccdot+%5Csum_%7Bi%3Dt%7D%5E%7BT%7D+KL%28i%29\" alt=\"A_{t}(s_{t}, a_{t}) = r(x, y)-\\beta \\cdot \\sum_{i=t}^{T} KL(i)\" eeimg=\"1\"\u002F\u003E ，并使用z score归一化，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_%7Bnormalized+%7D%3D%5Cfrac%7BA-%5Cmu_%7BA%7D%7D%7B%5Csigma_%7BA%7D%7D\" alt=\"A_{normalized }=\\frac{A-\\mu_{A}}{\\sigma_{A}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"HSGdavsz\"\u003E（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%7BA%7D\" alt=\"\\mu_{A}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma_%7BA%7D\" alt=\"\\sigma_{A}\" eeimg=\"1\"\u002F\u003E 分别表示批次均值和标准差）。这种归一化方式保证了梯度的稳定性，防止训练过程中的发散。 \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E参考资料\u003C\u002Fh2\u003E\u003Cp data-pid=\"6ajJAB4u\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1706.03741\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E【1】Deep Reinforcement Learning from Human Preferences\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"bIS0O91Z\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2203.02155\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E【2】Training language models to follow instructions with human feedback\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"el2XOpmT\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.bilibili.com\u002Fvideo\u002FBV1R94y1P7QX\u002F%3Fshare_source%3Dcopy_web%26vd_source%3D4029b709c8dab921079939a0bdd5ec6c\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E【3】吴恩达《从人类反馈中进行强化学习RLHF, Reinforcement Learning from Human Feedback》（中英字幕）_哔哩哔哩_bilibili\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"PSswR0kl\"\u003E【4】大规模语言模型从理论到实践\u003C\u002Fp\u003E\u003Cp data-pid=\"50sfqv55\"\u003E【5】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fmp.weixin.qq.com\u002Fs\u002FtG_ktQ0WbZHQavtoJtaXbw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E从0到1构建RLHF系统——小红书大模型团队的探索与实践\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"vX7a5t1l\"\u003E【6】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.cnblogs.com\u002Fend\u002Fp\u002F17481052.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E为什么RLHF中，PPO需要Critic模型而不是直接使用RewardModel - 风生水起 - 博客园\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"eblBdO1I\"\u003E【7】\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F14888098807\" class=\"internal\" target=\"_blank\"\u003E初七123334：RLHF 对齐之 REINFORCE++ 算法 - 比 GRPO 稳定比PPO快\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"RwwEsgBn\"\u003E【8】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fpaperexplained.cn\u002Faplayground\u002Fiarticle\u002Fdetail\u002F0454c3b5-be1a-4aff-a146-9c5adaf76600\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E演练 - 强化学习经典算法实验之REINFORCE - 字舞流文\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"w9Nbf6Uw\"\u003E【9】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdocs.google.com\u002Fpresentation\u002Fd\u002F1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk\u002Fedit%23slide%3Did.g2650ce3df47_0_0\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EopenRLHF slides\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Z9ZLOTFp\"\u003E【10】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fblog.csdn.net\u002Fshizheng_Li\u002Farticle\u002Fdetails\u002F144468429\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E重要性采样详解及其在PPO算法中的应用\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"xXHec9xk\"\u003E【11】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2305.18290\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDirect Preference Optimization: Your Language Model is Secretly a Reward Model\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"9nz55F2p\"\u003E【12】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2402.03300\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"39QS92qh\"\u003E【13】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2402.14740\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBack to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Md2k2Td7\"\u003E【14】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdeepreinforcementlearningbook.org\u002Fassets\u002Fpdfs\u002Fch3.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Edeepreinforcementlearningbook.org\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E\u002Fassets\u002Fpdfs\u002Fch3.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Mt261kK0\"\u003E【15】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fimxwell.com\u002Fblog\u002Frl_basic\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E强化学习基本问题回顾总结 | Xwell&#39;s Blog\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"5vJKQcF_\"\u003E【16】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.xiaohongshu.com\u002Fexplore\u002F6732c5ca000000001a01d343%3Fxsec_token%3DABO2vbisf8d7CC4oYpTya1zwMkKyW-ipv7a-PShemePLo%3D%26xsec_source%3Dpc_search%26source%3Dunknown\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ERLOO：比PPO更适合LLM的RLHF方法 - 小红书\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"91k-MusE\"\u003E【17】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fhuggingface.co\u002Fblog\u002Fzh\u002Fputting_rl_back_in_rlhf_with_rloo%3Futm_source%3Dchatgpt.com\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Eputting rl back in rlhf with rloo\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Ue43aV1e\"\u003E【18】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fhijkzzz.notion.site\u002Frlhf-implementation-tricks%3Fv%3D158d9a33ecc98132bf9e000c39227361\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAdvanced Tricks for Training Large Language Models with Proximal Policy Optimization\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"1ahjqAdD\"\u003E【19】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2310.12036\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EA General Theoretical Paradigm to Understand Learning from Human Preferences\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"-oEF62Vt\"\u003E【20】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2402.01306\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EKTO: Model Alignment as Prospect Theoretic Optimization\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"qCX_dRWM\"\u003E【21】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2112.00861\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EA general language assistant as a laboratory for alignment\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"skNFbXma\"\u003E【22】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2310.06452\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EUnderstanding the effects of rlhf on llm generalisation and diversity\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"WysHclQw\"\u003E【23】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.bilibili.com\u002Fvideo\u002FBV1PSDmYkEj1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ERLOO一作讲解RLOO\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"XQDZhMdL\"\u003E【24】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fopenai.com\u002Findex\u002Ffaulty-reward-functions\u002F\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eopenai.com\u002Findex\u002Ffaulty\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E-reward-functions\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"VLBmFsCi\"\u003E【25】\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F675329917\" class=\"internal\" target=\"_blank\"\u003E何枝：【RLHF】RL 究竟是如何与 LLM 做结合的？\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"0Q5PuYak\"\u003E【26】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2501.03262\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EREINFORCE++: A SIMPLE AND EFFICIENT APPROACH FOR ALIGNING LARGE LANGUAGE MODELS\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"CadJs6c4\"\u003E【27】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2503.14476\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDAPO: An Open-Source LLM Reinforcement Learning System at Scale\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"K9uaE6Ru\"\u003E【28】\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F1888687830710084683\" class=\"internal\" target=\"_blank\"\u003E大家好我是爱因：DAPO：GRPO的问题分析及四个改进策略\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"eJFCSR8W\"\u003E【29】\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2503.20783\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EUnderstanding R1-Zero-Like Training: A Critical Perspective\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"uajb5aoW\"\u003E【30】\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F1891404563971564359\" class=\"internal\" target=\"_blank\"\u003E大家好我是爱因：理解 R1-Zero 式训练：一个批判性视角\u003C\u002Fa\u003E\u003C\u002Fp\u003E","contentNeedTruncated":false,"forceLoginWhenClickReadMore":false,"adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F25402720","type":"topic","id":"25402720","name":"大模型"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F27239249","type":"topic","id":"27239249","name":"多模态大模型"}],"voteupCount":335,"voting":0,"heavyUpStatus":"allow_heavy_up","column":{"description":"","canManage":false,"intro":"新技术新算法层出不穷，这不是我们的认知负担，而是我们的认知福利","isFollowing":false,"urlToken":"jackstark","id":"jackstark","articlesCount":298,"acceptSubmission":true,"title":"机器学习小王子","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fjackstark","commentPermission":"all","created":1566551693,"updated":1745991103,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-254f045a8551f0c6d3e3e8445f224e4d_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb.jpg?source=172ae18b","uid":"1019333493377122304","userType":"people","isFollowing":false,"urlToken":"iamein","id":"e03c883e599808250b135d35b0dc9c24","description":"","name":"大家好我是爱因","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002Fe03c883e599808250b135d35b0dc9c24","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":1813,"type":"column","columnType":"normal"},"commentCount":13,"contributions":[{"id":48467980,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"新技术新算法层出不穷，这不是我们的认知负担，而是我们的认知福利","isFollowing":false,"urlToken":"jackstark","id":"jackstark","articlesCount":298,"acceptSubmission":true,"title":"机器学习小王子","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fjackstark","commentPermission":"all","created":1566551693,"updated":1745991103,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-254f045a8551f0c6d3e3e8445f224e4d_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb.jpg?source=172ae18b","uid":"1019333493377122304","userType":"people","isFollowing":false,"urlToken":"iamein","id":"e03c883e599808250b135d35b0dc9c24","description":"","name":"大家好我是爱因","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002Fe03c883e599808250b135d35b0dc9c24","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":1813,"type":"column","columnType":"normal"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":true,"hasPublishingDraft":false,"isFavorited":false,"favlistsCount":588,"isNormal":true,"status":0,"activityToppingInfo":{"state":"untopped"},"shareText":"大模型中的强化学习 - 来自知乎专栏「机器学习小王子」，作者: 大家好我是爱因 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F693582342 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":45,"hasColumn":true,"republishers":[],"isNewLinkCard":true,"emojiReaction":{"likeCount":45,"likeHasSet":false},"abParam":{"qaHiddenVoteup":"","rsInterest1":"","zpZhiStyle":""},"attachedInfo":"kgIkCgkyNDIyNTk2ODcSCTY5MzU4MjM0MhgHIgpJTUFHRV9URVhU","shareGuide":{"hasPositiveBubble":false,"hasTimeBubble":false,"hitShareGuideCluster":false},"settings":{"tableOfContents":{"enabled":true}},"canReference":false,"reactionInstruction":{},"reaction":{"statistics":{"upVoteCount":335,"downVoteCount":4,"likeCount":45,"commentCount":13,"shareCount":0,"playCount":0,"interestPlayCount":0,"favorites":588,"pvCount":0,"bulletCount":0,"applaudCount":0,"questionFollowerCount":0,"questionAnswerCount":0,"plaincontentVoteUpCount":0,"plaincontentLikeCount":0,"imgLikeCount":{"v214032877acc55b3d9d01ab3499f852f5":0,"v21a0520d031941077b31cfc6bfec6b19c":0,"v225e2a31c9d52d9a6c59018346f99c304":1,"v229d4f8b0094b902902649c0fae9f0b63":0,"v236d53576eb29495894221e42cc0654b9":0,"v2638ceea6aa03b5d88366ca081708628e":0,"v26b91567c45c1943e6e35ec7a0da091f3":0,"v2Aa334b22cfcb50fa82608e8d6a4bf0b9":0,"v2Aad2c6070600ba224252b4e055186c61":0,"v2C74f58b19921bf0a52c1d1bd196ea263":0,"v2Ce99342d89a1478e43d433f8a5ced58b":0,"v2Dd930f961bd3709bd5c6950bddb62e7f":0,"v2Df12253f4eb3afa67788e97902f009dd":0,"v2E3dd361d9d10aac42a1e7e5d313ce754":0,"v2Eba10ba1c76228967df531c5ac287f3f":1,"v2Ece21d083fbd4ac15d6ca16d8a787de7":0,"v2Edb79bf548d7d181db49b0a4014a41ec":0,"v2Ee3c677200c094464dcd5e23ebd2c57d":0,"v2Fdd58ea4ecac5e65050979067271b89e":0},"subscribeCount":0,"republishers":[]},"relation":{"isAuthor":false,"vote":"Neutral","liked":false,"imgLiked":{"v214032877acc55b3d9d01ab3499f852f5":false,"v21a0520d031941077b31cfc6bfec6b19c":false,"v225e2a31c9d52d9a6c59018346f99c304":false,"v229d4f8b0094b902902649c0fae9f0b63":false,"v236d53576eb29495894221e42cc0654b9":false,"v2638ceea6aa03b5d88366ca081708628e":false,"v26b91567c45c1943e6e35ec7a0da091f3":false,"v2Aa334b22cfcb50fa82608e8d6a4bf0b9":false,"v2Aad2c6070600ba224252b4e055186c61":false,"v2C74f58b19921bf0a52c1d1bd196ea263":false,"v2Ce99342d89a1478e43d433f8a5ced58b":false,"v2Dd930f961bd3709bd5c6950bddb62e7f":false,"v2Df12253f4eb3afa67788e97902f009dd":false,"v2E3dd361d9d10aac42a1e7e5d313ce754":false,"v2Eba10ba1c76228967df531c5ac287f3f":false,"v2Ece21d083fbd4ac15d6ca16d8a787de7":false,"v2Edb79bf548d7d181db49b0a4014a41ec":false,"v2Ee3c677200c094464dcd5e23ebd2c57d":false,"v2Fdd58ea4ecac5e65050979067271b89e":false},"faved":false,"following":false,"subcribed":false,"isNavigatorVote":false,"currentUserIsNavigator":false,"voteNextStep":""},"imageReactions":{"v214032877acc55b3d9d01ab3499f852f5":{"likeCount":0,"isLiked":false},"v21a0520d031941077b31cfc6bfec6b19c":{"likeCount":0,"isLiked":false},"v225e2a31c9d52d9a6c59018346f99c304":{"likeCount":1,"isLiked":false},"v229d4f8b0094b902902649c0fae9f0b63":{"likeCount":0,"isLiked":false},"v236d53576eb29495894221e42cc0654b9":{"likeCount":0,"isLiked":false},"v2638ceea6aa03b5d88366ca081708628e":{"likeCount":0,"isLiked":false},"v26b91567c45c1943e6e35ec7a0da091f3":{"likeCount":0,"isLiked":false},"v2Aa334b22cfcb50fa82608e8d6a4bf0b9":{"likeCount":0,"isLiked":false},"v2Aad2c6070600ba224252b4e055186c61":{"likeCount":0,"isLiked":false},"v2C74f58b19921bf0a52c1d1bd196ea263":{"likeCount":0,"isLiked":false},"v2Ce99342d89a1478e43d433f8a5ced58b":{"likeCount":0,"isLiked":false},"v2Dd930f961bd3709bd5c6950bddb62e7f":{"likeCount":0,"isLiked":false},"v2Df12253f4eb3afa67788e97902f009dd":{"likeCount":0,"isLiked":false},"v2E3dd361d9d10aac42a1e7e5d313ce754":{"likeCount":0,"isLiked":false},"v2Eba10ba1c76228967df531c5ac287f3f":{"likeCount":1,"isLiked":false},"v2Ece21d083fbd4ac15d6ca16d8a787de7":{"likeCount":0,"isLiked":false},"v2Edb79bf548d7d181db49b0a4014a41ec":{"likeCount":0,"isLiked":false},"v2Ee3c677200c094464dcd5e23ebd2c57d":{"likeCount":0,"isLiked":false},"v2Fdd58ea4ecac5e65050979067271b89e":{"likeCount":0,"isLiked":false}}},"interactionBarPlugins":[{"type":"comment","comment":{"enable":true,"placeholder":"发条带图评论"}}],"barPluginsFlipTime":3000,"podcastAudioEnter":{"text":"1 人听过","textColor":"MapBrand","textSize":13,"actionUrl":"zhihu:\u002F\u002Fpodcast\u002Faudio_player\u002F14459481418?contentId=693582342&contentType=article"},"endorsements":[{"elements":[{"type":"IMAGE","imageKey":"zhicon_icon_24_column_fill","imageColor":{"alpha":1,"group":"GBL01A"},"width":16,"height":16},{"type":"TEXT","content":"收录于 · 机器学习小王子","fontSize":13,"fontColor":{"alpha":1,"group":"GBL01A"},"isBold":false,"maxLine":1},{"type":"IMAGE","imageKey":"zhicon_icon_16_arrow_right","imageColor":{"alpha":1,"group":"GBL01A"},"width":12,"height":12}],"subElements":[],"subElementsType":"DESCRIPTION","backgroundColor":{"alpha":0.08,"group":"GBL01A"},"actionUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fcolumn\u002Fjackstark","za":{"blockText":"Column","type":"text","text":"收录于 · 机器学习小王子"}}],"allowSegmentInteraction":0,"isNavigator":false,"navigatorVote":false,"voteNextStep":"vote"}},"columns":{"jackstark":{"description":"","canManage":false,"intro":"新技术新算法层出不穷，这不是我们的认知负担，而是我们的认知福利","isFollowing":false,"urlToken":"jackstark","id":"jackstark","articlesCount":298,"acceptSubmission":true,"title":"机器学习小王子","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fjackstark","commentPermission":"all","created":1566551693,"updated":1745991103,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-254f045a8551f0c6d3e3e8445f224e4d_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb.jpg?source=172ae18b","uid":"1019333493377122304","userType":"people","isFollowing":false,"urlToken":"iamein","id":"e03c883e599808250b135d35b0dc9c24","description":"","name":"大家好我是爱因","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002Fe03c883e599808250b135d35b0dc9c24","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c5d9627d9e98fd98830eeced564275cb_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":1813,"type":"column","columnType":"normal"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"zvideos":{},"eduCourses":{}},"currentUser":"1f1705dc0d3c88feeb3767a8db29edea","account":{"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{},"profileInfinity":null},"env":{"abV2":{"config":{"paramMap":{"nm_gift_button":{"value":"PC1","abId":"gif_button_PC-1"},"ac_zhida_kb_squre":{"value":"1","abId":"rl-zhida_kb_squre_1-1"},"nm_pc_creator_no1":{"value":"1","abId":"rl-pc_creator_no3-1"},"wm_title_weak":{"value":"1","abId":"qa_pc_title_weak-1"},"nm_post_success":{"value":"1","abId":"post_success_25-1"},"ws_column_square":{"value":"1","abId":"rl-column_square_pc-1"}},"abMap":{"gif_button_PC-1":{"abId":"gif_button_PC-1","layerId":"creation_product_member_web_domain_layer3","diversionType":2},"rl-zhida_kb_squre_1-1":{"abId":"rl-zhida_kb_squre_1-1","layerId":"rl-zhida_kb_squre_1","diversionType":2},"rl-pc_creator_no3-1":{"abId":"rl-pc_creator_no3-1","layerId":"rl-pc_creator_no3","diversionType":2},"qa_pc_title_weak-1":{"abId":"qa_pc_title_weak-1","layerId":"qa_pc_title_weak_layer","diversionType":2},"post_success_25-1":{"abId":"post_success_25-1","layerId":"post_success_25_layer","diversionType":2},"rl-column_square_pc-1":{"abId":"rl-column_square_pc-1","layerId":"rl-column_square_pc"}}},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"HarmonyOS":false,"iOS":false,"isAppleDevice":true,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Quark":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"Zhixuetang":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"OpenHarmony":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F120.0.0.0 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F693582342","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F693582342","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"oppoSearch":false,"baiduSearch":false,"googleSearch":false,"shenma":false,"miniProgram":false,"xiaomi":false,"huaweiSearch":false},"theme":"light","appHeaderTheme":{"current":"normal","disable":true,"normal":{"bgColor":"GBK99A"},"custom":{"bgColor":"GBK99A"}},"enableShortcut":true,"referer":"https:\u002F\u002Fwww.zhihu.com\u002F","xUDId":"04TTmy8AyhqPTmKgvIsDSoovSIb2ZMOeIm4=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"台北市","countryName":"中国","regionName":"台湾","countryCode":"TW"},"logged":true,"query":{},"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0}},"recommend":{"recommendTimes":{}}},"explore":{},"levelUpperLimit":10,"mcn":{},"mcnManage":{},"tasks":{},"announcement":{},"creatorsRecommendInfo":{}},"creators":{"common":{"applyStatus":{},"rightsStatus":{}},"bayesDomains":{"status":{},"options":{"topDomains":null,"allDomains":null,"editable":0},"contents":null},"school":{"tabs":[],"contents":[],"banner":null,"entities":{}},"faq":{"tabs":[],"article":{}},"knowledgeIncome":{},"safeguardRights":{},"analytics":{"all":{},"answer":{},"zvideo":{},"article":{},"pin":{},"singleContent":{}},"account":{"growthLevel":{}},"KMResource":{},"training":{},"ToolsQuestion":{"goodatTopics":[]},"ToolsHotspot":{"domains":[]},"ToolsRecommend":{},"ToolsCustomPromotion":{"itemLists":{},"baseInfo":{}},"ToolsSearchQuestion":{},"editorSetting":{},"MCNManage":{},"knowledgeTasks":{},"incomeAnalysis":{"income":{"aggregation":{}}},"creationManage":{"editModal":{"status":false}},"activity":{},"announcement":{},"home":{"currentCreatorUrlToken":null,"rights":[],"newRights":[],"scoreInfo":{},"menusShowControlByServer":{"bVipRecomend":false,"creationRelationship":false},"newTasks":{"creatorTask":{"tasks":[],"des":[]}},"bannerList":[],"recentlyCreated":[],"homecard":{},"homeData":{}},"videoSupport":{"textBenefit":{}},"videoDistribution":{},"profilePoster":{"creatorPosterConfig":{},"creatorPosterData":{}}},"answers":{"voters":{},"upvoters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"entityWords":{},"paidContent":{},"settings":{},"relationEndorsement":{},"growthCardOrder":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{},"upvoters":{},"relationEndorsement":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotList":[],"hotListHeadZone":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"commercialBanner":{"show":false,"banner":{},"trackData":{}},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false},"ringTab":{"headerData":null,"feedLoading":false,"feedError":false,"feedPaging":{},"feedList":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"jackstark"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[]},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":null,"articleDisclaimers":[],"aiTask":null},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false},"linkCardLimit":0},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaignVideoList":{},"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"qyActivityData":{},"talkActivityData":{},"party2022ActivityData":{},"batchVideos":{},"creationReferences":{},"zvideoCollection":{},"zvideoGrant":{},"collectData":{"isFetching":false,"list":[]},"videoSource":{"isLoaded":false},"paidColumnInfo":{}},"republish":{},"commentPermission":{},"menuIgnoreSet":{},"creatorRightStatus":{"list":[]},"adPromotion":{"answer":{},"article":{}},"paidColumn":{"entities":{}},"followingColumns":{},"hotSpot":{},"contentColumnCard":{},"publishedModal":{"relatedRings":[]}},"fetchHost":"www.zhihu.com","subAppName":"column","spanName":"Post","canaryConfig":{"test_canary":"0","use_new_player":"0","player_vendor":"0","use_hevc":"1","upload_use_signature":"1","use_backdrop_blur":"1","article_title_imagex":"1","play_station":"1","use_cached_supported_countries":"1","pc_favorite_toast":"1","pc_article_page_favorite_toast":"1","comment_v4":"0","segment_interaction":"1"}}</script><script crossorigin="" src="https://static.zhihu.com/heifetz/vendor.899245ae89fd273e9d72.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react@17.0.2/umd/react.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react-dom@17.0.2/umd/react-dom.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react-dom@17.0.2/umd/react-dom-server.browser.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/runtime.app.e1bed6be4bbcef1054f4.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-75fc9c18.app.3db651c252e14ef6658e.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-29107295.app.42d07f814b7b05187671.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-79b5cf47.app.8b6b6bf4b6d894db9b07.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-330004dc.app.57ae5a954ed8b8160f16.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-f3572862.app.2f441c446cc825b7d4c8.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-4b14521a.app.76054a378e7401a203d9.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-0e5ce61e.app.8a1779aad4e7363c911d.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-38cf5c11.app.dd7d331983f4a5e4ea62.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/8911.app.11a4b42982b8a4cda6c6.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.app.57694df23eb1d1b00c0a.js"></script><script defer="" id="ariascripts" loaddata="false" src="https://static.zhihu.com/event/wza/4633_1/aria.js?appid=a3637ace5dc3a347f6863b0bac487599" wapforceoldfixed="false"></script><script async="" src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49"></script><script nonce="1fd6587f-56e3-430f-bfdf-8bac0df9e6f9">{;var x = document.createElement('link');x.rel = 'preload';x.as = 'style';x.href = 'https://unpkg.zhimg.com/@cfe/font-misans@1.1.1/dist/font.min.css';x.crossOrigin = '';x.onload = function() {;x.onload = null;x.rel = 'stylesheet';};(document.body||document.head).appendChild(x);}</script></body></html>
本来以为 RL 强化学习，是一个非常简单的内容，没想到有点复杂，涉及数学理论、算法等。

单独建一个仓库，专门学习下 RL（Reinforcement Learning），强化学习。


基本思路：

1. 简述
2. 数学抽象
3. 几类算法
4. Q-learning
5. Policy Gradient
6. Actor-Critic
7. DQN
8. DDQN
9. TD3
10. PPO
11. TRPO
12. SAC




## 1.简述

一句最简明的定义：

> **强化学习（Reinforcement Learning, RL）** 是一种让「智能体（`agent`）」通过「试错」与「环境（`environment`）」互动，从而`学习`「最佳`行为策略`」的机器学习方法。


### 1.1.实例

想象一只小狗学坐下。

* 你（环境）会观察它的行为。
* 如果它坐下 → 给奖励（比如小饼干）。
* 如果它跳起来 → 没有奖励。

久而久之，小狗会学会「坐下 → 有奖励」这个规律，于是倾向于坐下。

这，就是`强化学习`的基本思路。


### 1.2. 四个核心要素

| 要素                  | 含义          | 类比（小狗例子）      |
| ------------------- | ----------- | ------------- |
| **Agent（智能体）**      | 学习、决策的主体    | 小狗            |
| **Environment（环境）** | 与智能体互动的外部世界 | 主人、房间         |
| **Action（动作）**      | 智能体能做的行为    | 坐下、跳、跑        |
| **Reward（奖励）**      | 环境给予的反馈     | 小饼干（正奖励）或没有奖励 |

目标就是：

* 让 `Agent` 学会一组「策略（`policy`）」：在不同情境下，选择什么动作，能得到**最多的累积奖励**。


### 1.3.和其他机器学习方式的不同

| 类型        | 学习方式            | 示例        |
| --------- | --------------- | --------- |
| **监督学习**  | 已知输入+正确输出 → `学映射` | 猫狗图片分类    |
| **无监督学习** | 只给输入 → 学结构、`分布`   | 聚类        |
| **强化学习**  | 通过互动和反馈 → 学`最优策略` | 自动驾驶、围棋AI |


## 2.数学抽象


### 2.1.核心目标

RL 目标：最大化`长期收益`。


强化学习的核心目标是：

$$
\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big]
$$


解释一下每个部分的意义：

* `argmax`：argument max 求的是`表达式`使达到最大值的`自变量` $\pi$，记作 $\pi^*$。
* $\pi$ ：策略（policy），定义在每个状态 (s) 下，采取动作 (a) 的`概率分布`  $\pi(a|s)$ 。
* $R_t$ ：在第 (t) 步获得的`即时奖励`，只是「当前步骤的即时奖励」，并不包含未来的奖励。
* $\gamma \in (0,1]$：`折扣因子`（discount factor），控制「未来奖励」的重要程度。
* 目标是让策略 $\pi$ 产生的**期望累积奖励**最大化。

> 这就像是一个玩家，不只想每次得分高，而是希望整局游戏的“总得分”最高。




### 2.2. 环境建模：马尔可夫决策过程（MDP）

我们通常假设环境满足 **Markov Decision Process (MDP)**：

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)
$$

* $\mathcal{S}$：状态空间
* $\mathcal{A}$：动作空间
* $P(s'|s,a)$：状态转移概率，表示在状态 $s$ 下，采取动作 $a$ 后，转移到状态 $s'$ 的概率。
* $R(s,a)$：即时奖励函数
* $\gamma$：折扣系数

**马尔可夫性**的含义是：


$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)
$$

—— `未来只依赖当前`状态和动作，`不依赖过去`历史。


### 2.3.价值函数：衡量策略优劣的指标

强化学习的核心是学习 `价值函数`（Value Function）。

两个函数：

* V 函数：只看`状态` $s$ 的好坏（Value）；
* Q 函数：看`状态 + 动作` $(s,a)$ 的好坏（Quality）。


#### 2.3.1.状态价值函数 Value

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0 = s\Big]
$$

→ 表示：在状态 $s$ 下，按策略 $\pi$ 行动，能期望获得的长期收益。

* $R_t$：此刻的`瞬时快乐`；
* $\sum_{t=0}^\infty \gamma^t R_t$：一生的`总幸福`，只是未来的快乐会打个折（$\gamma < 1$）。


所以，在强化学习里我们通常还会定义一个**回报（Return）**：

$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$

→ 也就是 **从时间 (t) 开始往后看** 的总收益。
而价值函数 $V^{\pi}(s_t)$ 的定义，也可写为：

$$
V^{\pi}(s_t) = \mathbb{E}[G_t | s_t]
$$

#### 2.3.2.状态-动作价值函数（Q 函数）

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0=s, a_0=a\Big]
$$

→ 表示：从状态 $s$ 出发，先做动作 $a$，再按策略 $\pi$ 行动的期望收益。 Q 来源于 Quality 质量/价值。

它们之间关系为：

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \cdot Q^{\pi}(s,a)
$$


### 2.4.贝尔曼方程：递归关系

这是 `RL` 理论的核心方程。

$$
Q^{\pi}(s,a) = \mathbb{E}\big[R(s,a) + \gamma \sum_{a'} \pi(a'|s') \cdot Q^{\pi}(s',a') \big]
$$

→ 表示`当前动作`价值等于：

* 当前的`即时奖励`  $R(s,a)$ ；
* 加上 `下一状态的价值` 的`折扣期望`， $\gamma$ 为折扣系数
* 其中： $s'$ 是 $s$ 采取动作 $a$ 后转移到的新状态， $a'$ 是 $s'$ 可能采取的动作。

而对于`最优策略` $\pi^*$，我们得到 **贝尔曼最优方程**：

$$
Q^*(s,a)
$$

$$
=  \mathbb{E}\big[R(s,a) + \gamma \max_{a'} Q^*(s',a') \big]
$$

这就是 `Q-learning` 的理论基础。


## 3.学习方法的两大路线

| 类型         | 代表算法              | 思想                 | 关键点           |             |
| ---------- | ----------------- | ------------------ | ------------- | ----------- |
| **价值函数方法**  | Q-learning, SARSA | 直接逼近 $Q^*(s,a)$    | 通过 TD（时间差分）更新 |             |
| **策略梯度方法** | REINFORCE, PPO    | 直接优化 $\pi_\theta(a \| s)$           | 通过`梯度上升`最大化奖励 |



### 3.1.价值函数（Value-based）

> **核心思想：** 不直接学策略，而是先学“每个状态或动作有多好”，再据此选动作。

这类算法的目标是，逼近`最优` **Q 函数**：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

也就是：每个`状态–动作`对的 **最佳长期回报**。

#### 3.1.1.代表算法

* **Q-learning**
* **Deep Q Network (DQN)**


#### 3.1.2.思路

1. 学一个 **价值函数**（比如  $Q(s,a)$ 或 $V(s)$ ）。
2. 通过与环境交互、获得奖励，不断更新估计：

   $$
   Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
   $$

3. 选择动作时，就选 $Q$ 值最高的那个动作：

   $$
   a^* = \arg\max_a Q(s,a)
   $$

训练结束的标志：**找到目标价值函数（即 $Q^*$）就意味着找到了最优策略**。


> 一旦学到了正确的 $Q^*$，在每个状态选 $Q$ 最大的动作，就是最优决策。


不过要注意两点：

1. 实际中我们不会“精确找到” $Q^*$，而是`逼近`；
2. 收敛标准通常是：Q 值`变化非常小` 或 `策略稳定`不再改进。

训练结束时，收敛结果是：

$$
Q^*(s,a)
$$

$$
\approx r + \gamma \max_{a'} Q^*(s',a')
$$



#### 3.1.3.优点

* 理论扎实，计算高效；
* 不需要显式建模策略。

#### 3.1.4.缺点

* 只能处理**离散动作空间**（连续动作会很难“max”）；
* 不适合直接学习复杂的随机策略。


### 3.2.策略（Policy-based）

> **核心思想：** 直接学习“如何行动”的`策略函数` $\pi_\theta(a|s)$。


#### 3.2.1.代表算法

* **REINFORCE（Policy Gradient）**
* **Proximal Policy Optimization (PPO)**
* **Actor-Critic**
* **PPO / A2C / DDPG / SAC**（现代主流）


#### 3.2.2.思路

1. 策略由参数 $\theta$ 决定（通常是一个`神经网络`）。
2. 目标是最大化期望累计奖励：
   $$
   J(\theta) = \mathbb{E}_{\pi\theta}\Big[\sum_t \gamma^t R_t\Big]
   $$
3. 使用梯度上升（Policy Gradient）更新：
   $$
   \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
   $$


#### 3.2.3.优点

* 能处理**连续动作空间**；
* 可以直接优化期望收益；
* 更灵活（可学到随机策略）。

#### 3.2.4.缺点

* 收敛慢、方差大；
* 通常需要配合价值估计（这就引出了 **Actor-Critic**）。


### 3.3.混合路线：Actor–Critic

> 同时学策略（Actor）和价值（Critic），结合两者优点。

结构：

* **Actor**：学策略 $\pi(a|s)$，告诉你“该怎么做”；
* **Critic**：学价值函数 $V(s)$ 或 $Q(s,a)$，告诉你“做得好不好”。


## 4.Q-learning算法

Q-learning 算法，是`价值函数`类算法的典型实现。

这类算法的目标是，逼近`最优` **Q 函数**：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

也就是：每个`状态–动作`对的 **最佳长期回报**。



### 4.1.训练过程

核心更新公式：表格(`Tabular`)算法形式。

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
$$

含义：

* 观察当前`奖励` $r$；
* 看`下一个`状态的最大潜在价值 $\max_{a'} Q(s',a')$；
* 让当前的 $Q(s,a)$ 向“目标值”靠近。
* $\alpha$ 是`学习率`（步长），只是一个`外部超参`数（hyperparameter），控制每次更新时，“向目标靠近的幅度”
* $r + \gamma \max_{a'} Q(s',a')$ 是目标（`target`）
* $Q(s,a)$ 是当前估计（`prediction`）

这个「目标值」就是：

$$
y = r + \gamma \max_{a'} Q(s',a')
$$

如果每次更新都能让 $Q$ 收敛到真实的 $Q^*$，
就说明策略也收敛到了最优策略（因为最优策略就是选最大 $Q$ 的动作）。


$\alpha$ 的取值策略，一般情况下：

* 训练`初期`希望学得快：$\alpha$ 较`大`（如 0.5）
* 越到`后期`，$\alpha$ 越`小`（如 0.1 或逐步递减）

一种常见做法：

$$
\alpha_t = \frac{1}{1 + \text{visit}(s,a)}
$$

表示该`状态–动作`对被访问得越多，学习率越低（因为它已经稳定）。


### 4.2.理论分析

在表格形式中，差异项：跟最佳Q值的差异。

$$
\delta = r + \gamma \max_{a'} Q(s',a') - Q(s,a)
$$

叫做 **TD误差**（`Temporal Difference error` TD error）、 Bellman 误差。

这个更新，其实就是在做：

$$
Q(s,a) \leftarrow Q(s,a) - \alpha \frac{\partial}{\partial Q} \frac{1}{2}\delta^2
$$

也就是说，它等价于在**最小化一个平方误差损失函数**：
$$
L = \frac{1}{2}\big(r + \gamma \max_{a'} Q(s',a') - Q(s,a)\big)^2
$$

所以 Q-learning 虽然表面上看像是直接更新，但实际上就是梯度下降在减少预测值和目标值的误差。



### 4.3.训练 & 推理

从 输入 -> 更新 -> 输出 视角，关注 Q-learning 的完整过程。


简要总结：

> 1. `Tabular Q-learning` 的输入是`状态–动作–奖励–next状态`四元组
> 2. `Q 表`是算法内部逐步学习，得到的“记忆表”，
> 3. 最终`输出`（模型）就是这张 `Q 表`本身。


#### 4.3.1. 输入：不是 Q 表**

在运行时，算法的每次迭代「输入」其实是**环境交互得到的一个样本**：

$$
(s, a, r, s') = (状态, 动作, 奖励, 下一个状态)
$$

也就是：

* 当前`状态` $s$  ( s )
* 执行`动作` $a$
* 得到的即时`奖励` $r$
* 下一步`状态` $s'$

---

#### 4.3.2. Q 表：是“内部存储结构”

Q 表（Q-table）是算法**内部维护的一张表格**，用来记录「每个状态–动作对」的当前价值估计。

比如：

| 状态 `s` | 动作 `a₁` | 动作 `a₂` | 动作 `a₃` |
| ---- | ----- | ----- | ----- |
| `s₁`   | 0.5   | 0.2   | -0.1  |
| `s₂`   | 0.1   | 0.6   | 0.0   |
| `s₃`   | -0.2  | 0.3   | 0.8   |

每一格 $Q(s,a)$ 表示：在状态 $s$ 下选择动作 $a$ 的预期长期收益。


#### 4.3.3. 算法流程：学习过程

我们可以这样总结它的“输入→更新→输出”逻辑：

1.**初始化**：创建一张全 0 的 Q 表（所有状态–动作对初始化为 0）

2.**交互采样**：从环境得到一组样本 $(s, a, r, s')$

3.**更新表格**：根据更新规则：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)]
$$ 

4.**循环迭代**：不断重复上面的过程，直到 `Q 表`收敛。（训练结束）


#### 4.3.4. 最终输出：推理过程

最终输出的“模型”其实就是这张 `Q 表`。

在`推理`时（即不再训练），智能体只需：

$$
a^* = \arg\max_a Q(s,a)
$$

即可直接根据 Q 表选出最优动作。

### 4.4.实例

我们来用一个**最小化的例子**（只需纸笔就能算）来**完整演示一次训练过程**。


#### 4.4.1. 场景设定：一个 3 状态迷你世界

想象一个简单环境：

$$
S1 → S2 → S3(终点)
$$

规则如下：

* 状态集合：$S = \{S1, S2, S3\}$
* 动作集合：$A = \{Right, Left\}$
* 奖励规则：

  * 从 $S1$ “Right” → $S2$ ，奖励 $+0$
  * 从 $S2$ “Right” → $S3$ ，奖励 $+1$（目标）
  * 其他动作（比如往回走）奖励 $0$
* 折扣因子 $\gamma = 0.9$
* 学习率 $\alpha = 0.5$


#### 4.4.2. 初始化 Q 表

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0     | 0    |
| S2 | 0     | 0    |
| S3 | 0     | 0    |


#### 4.4.3. 第 1 回合（Episode 1）

##### Step 1

* 当前状态：$S1$
* 采取动作：$Right$
* 得到奖励：$r = 0$
* 下一状态：$S2$

更新公式：
$$
Q(S1, Right) ← Q(S1, Right) + α [r + γ \max_{a'} Q(S2, a') - Q(S1, Right)]
$$

代入：

$$
Q(S1, Right) = 0 + 0.5 [0 + 0.9×0 - 0] = 0
$$

（没有即时奖励，也没学到什么）


##### Step 2

* 当前状态：$S2$
* 动作：$Right$
* 奖励：$r = +1$
* 下一状态：$S3$

更新：

$$
Q(S2, Right) = 0 + 0.5 [1 + 0.9×\max Q(S3,·) - 0]
$$

$$
= 0.5 × 1 = 0.5
$$

更新后的 Q 表：

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0     | 0    |
| S2 | 0.5   | 0    |
| S3 | 0     | 0    |


#### 4.4.4. 第 2 回合（Episode 2）

又从 $S1$ 出发：

##### Step 1

* $S1$, 动作 $Right$
* 奖励 $r=0$，下一状态 $S2$

更新：

$$
Q(S1, Right) = 0 + 0.5 [0 + 0.9×\max Q(S2,·) - 0]
$$

$$
= 0.5 × (0.9×0.5)
= 0.225
$$

新表：

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0.225 | 0    |
| S2 | 0.5   | 0    |
| S3 | 0     | 0    |


#### 4.4.5. 第 3 回合（Episode 3）

再跑几次后，$Q(S1,Right)$ 继续更新：

$$
Q(S1, Right) ← 0.225 + 0.5 [0 + 0.9×0.5 - 0.225]
$$

$$
= 0.225 + 0.5(0.225 - 0.225)
= 0.225  （几乎稳定）
$$

$S2→Right$ 的值也会逐渐逼近 1。


#### 4.4.6. 收敛后的结果（直观理解）

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | ≈0.4  | 0    |
| S2 | ≈1.0  | 0    |
| S3 | 0     | 0    |


#### 4.4.7. 模型输出

最终的 Q 表，就是你的模型。

策略：

$$
π^*(s) = \arg\max_a Q(s,a)
$$

得到：

* 在 S1 → 选择 Right
* 在 S2 → 选择 Right
* 到达 S3 → 终止

智能体学会了从 S1 → S2 → S3 的最优路径。


#### 4.4.8. 总结：核心逻辑

| 步骤   | 输入            | 更新目标                            | 输出       |
| ---- | ------------- | ------------------------------- | -------- |
| 每步交互 | $(s, a, r, s′)$ | 使 $Q(s,a) \approx r + \gamma \max_{a'} Q(s',a')$ | 更新一格 Q 值 |
| 重复多轮 | 环境交互样本        | 最小化 Bellman 误差                  | 收敛到 Q*   |
| 推理阶段 | 状态 s          | 取 $\arg\max_a Q(s,a)$              | 最优动作 $a^*$  |











## 5.DQN（Deep Q-Network）

DQN（Deep Q-Network），是`Q-learning`的升级版，主要解决了Q-learning无法处理`连续动作空间`的问题。

当状态空间太大（比如游戏画面）时，不能再存一个表格来记录所有 Q 值。

于是我们用`神经网络`逼近：

$$
Q(s,a;\theta) \approx Q^*(s,a)
$$

更新目标：`损失函数`，当前值与预测值的差异

$$
L(\theta) = \big(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta)\big)^2
$$

其中， $\theta^-$ 是目标网络的参数，用于稳定训练。

* $Q(s,a;\theta)$：当前网络预测值；
* $r + \gamma \max_{a'} Q(s',a';\theta^-)$：目标 Q 值（来自固定目标网络 $\theta^-$）；
* 训练过程：最小化 $L(\theta)$，让网络逼近目标。

有了损失函数，再用 `梯度下降`，就可以更新整个神经网络。细节参考：[AI 系列：反向传播 & 梯度下降](https://ningg.top/ai-series-backward-propagation-gradient-descent-intro/)



| 算法        | 核心思路        | 特点           |
| ---------- | -------------- | ------------- |
| Q-learning | 向最优目标 Q 更新 | Off-policy，稳定 |
| DQN        | 用神经网络逼近 Q  | 能处理高维状态空间 |














非常好的追问 👏，这说明你已经开始从「输入→更新→输出」的角度理解整个算法了。
我们来一点一点拆清楚：

---

### 🧩 1️⃣ **Tabular Q-learning 的输入不是 Q 表**

在运行时，算法的每次迭代「输入」其实是**环境交互得到的一个样本**：

[
(s, a, r, s')
]

也就是：

* 当前状态 ( s )
* 执行动作 ( a )
* 得到的即时奖励 ( r )
* 下一步状态 ( s' )

---

### 🧩 2️⃣ **Q 表是“内部存储结构”**

Q 表（Q-table）是算法**内部维护的一张表格**，
用来记录「每个状态–动作对」的当前价值估计。

比如：

| 状态 s | 动作 a₁ | 动作 a₂ | 动作 a₃ |
| ---- | ----- | ----- | ----- |
| s₁   | 0.5   | 0.2   | -0.1  |
| s₂   | 0.1   | 0.6   | 0.0   |
| s₃   | -0.2  | 0.3   | 0.8   |

每一格 Q(s,a) 表示：在状态 s 下选择动作 a 的预期长期收益。

---

### 🧩 3️⃣ **算法流程**

我们可以这样总结它的“输入→更新→输出”逻辑：

1️⃣ **初始化**：
创建一张全 0 的 Q 表（所有状态–动作对初始化为 0）

2️⃣ **交互采样**：
从环境得到一组样本 (s, a, r, s′)

3️⃣ **更新表格**：
根据更新规则：
[
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)]
]

4️⃣ **循环迭代**：
不断重复上面的过程，直到 Q 表收敛。

---

### 🧩 4️⃣ **最终输出**

最终输出的“模型”其实就是这张 Q 表。
在推理时（即不再训练），智能体只需：

[
a^* = \arg\max_a Q(s,a)
]

即可直接根据 Q 表选出最优动作。

---

✅ **一句话总结**

> Tabular Q-learning 的输入是状态–动作–奖励–下状态四元组，
> Q 表是算法内部逐步学习得到的“记忆表”，
> 最终输出（模型）就是这张 Q 表本身。

---

要不要我接着帮你画一个 Q-learning 的「输入→更新→收敛→推理」流程图？
一张图能让整个过程变得非常直观。



=======================


非常棒 👏，这正是理解 Q-learning 的关键一环。
我们来非常直接、公式化地讲清楚：

---

### 🧩 一句话答案

> **Tabular Q-learning 的最终模型，就是那张收敛后的 Q 表（Q-table）本身。**

---

### 🧠 1️⃣ 训练过程中学的是什么？

Q-learning 在训练时，不是在“学参数权重”，
而是在逐步**填充、修正 Q 表中的每个值**。

公式回顾：

[
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
]

在每次交互中：

* 它用样本 (s, a, r, s′) 来更新一格 Q(s, a)
* 最后所有格子都逼近真实的长期回报

所以，算法的“学习成果”完全体现在那张表的内容里。

---

### 🧩 2️⃣ 最终模型长什么样？

举个例子：

| 状态 s | 动作 a₁ | 动作 a₂ | 动作 a₃ |
| ---- | ----- | ----- | ----- |
| s₁   | 4.7   | 3.1   | 2.9   |
| s₂   | 5.2   | 4.8   | 1.0   |
| s₃   | 2.3   | 6.0   | 5.5   |

这张表中每个数 ( Q^*(s,a) ) 表示：

> 在状态 s 下执行动作 a 后，长期累积期望奖励。

---

### 🧩 3️⃣ 如何用这个模型？

在推理（决策）阶段，不再更新，只要查表选动作：

[
a^* = \arg\max_a Q(s,a)
]

也就是说：

* 当前观察状态 s
* 找出该行中 Q 值最大的动作列
* 执行它

→ 这就是“最优策略”。

---

### 🧩 4️⃣ 小结表格

| 阶段   | 输入            | 输出      | 更新目标          |
| ---- | ------------- | ------- | ------------- |
| 训练阶段 | (s, a, r, s′) | 更新 Q 表  | 减小 Bellman 误差 |
| 推理阶段 | 当前状态 s        | 最优动作 a* | 从 Q 表中查最大值    |

---

✅ **总结一句话：**

> Tabular Q-learning 的“模型”不是网络、不是函数，
> 而是收敛后的 Q 表本身；
> 它在推理时通过查表完成决策。

---

要不要我帮你画一张「训练过程 → 收敛后表格 → 推理阶段」的全流程图？
那张图能让整个 Q-learning 的“模型意义”非常直观。





















本来以为 RL 强化学习，是一个非常简单的内容，没想到有点复杂，涉及数学理论、算法等。

单独建一个仓库，专门学习下 RL（Reinforcement Learning），强化学习。


基本思路：

1. 简述
2. 数学抽象
3. 几类算法：价值函数（Q函数）、梯度策略
4. Q-learning：Q 函数
5. DQN：Deep Q-Network，深度 Q 函数网络
5. Policy Gradient
6. Actor-Critic
8. DDQN
9. TD3
10. PPO
11. TRPO
12. SAC




## 1.简述

一句最简明的定义：

> **强化学习（Reinforcement Learning, RL）** 是一种让「智能体（`agent`）」通过「试错」与「环境（`environment`）」互动，从而`学习`「最佳`行为策略`」的机器学习方法。


### 1.1.实例

想象一只小狗学坐下。

* 你（环境）会观察它的行为。
* 如果它坐下 → 给奖励（比如小饼干）。
* 如果它跳起来 → 没有奖励。

久而久之，小狗会学会「坐下 → 有奖励」这个规律，于是倾向于坐下。

这，就是`强化学习`的基本思路。


### 1.2. 四个核心要素

| 要素                  | 含义          | 类比（小狗例子）      |
| ------------------- | ----------- | ------------- |
| **Agent（智能体）**      | 学习、决策的主体    | 小狗            |
| **Environment（环境）** | 与智能体互动的外部世界 | 主人、房间         |
| **Action（动作）**      | 智能体能做的行为    | 坐下、跳、跑        |
| **Reward（奖励）**      | 环境给予的反馈     | 小饼干（正奖励）或没有奖励 |

目标就是：

* 让 `Agent` 学会一组「策略（`policy`）」：在不同情境下，选择什么动作，能得到**最多的累积奖励**。


### 1.3.和其他机器学习方式的不同

| 类型        | 学习方式            | 示例        |
| --------- | --------------- | --------- |
| **监督学习**  | 已知输入+正确输出 → `学映射` | 猫狗图片分类    |
| **无监督学习** | 只给输入 → 学结构、`分布`   | 聚类        |
| **强化学习**  | 通过互动和反馈 → 学`最优策略` | 自动驾驶、围棋AI |


## 2.数学抽象


### 2.1.核心目标

RL 目标：最大化`长期收益`。


强化学习的核心目标是：

$$
\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big]
$$


解释一下每个部分的意义：

* `argmax`：argument max 求的是`表达式`使达到最大值的`自变量` $\pi$，记作 $\pi^*$。
* $\pi$ ：策略（policy），定义在每个状态 (s) 下，采取动作 (a) 的`概率分布`  $\pi(a|s)$ 。
* $R_t$ ：在第 (t) 步获得的`即时奖励`，只是「当前步骤的即时奖励」，并不包含未来的奖励。
* $\gamma \in (0,1]$：`折扣因子`（discount factor），控制「未来奖励」的重要程度。
* 目标是让策略 $\pi$ 产生的**期望累积奖励**最大化。

> 这就像是一个玩家，不只想每次得分高，而是希望整局游戏的“总得分”最高。




### 2.2. 环境建模：马尔可夫决策过程（MDP）

我们通常假设环境满足 **Markov Decision Process (MDP)**：

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)
$$

* $\mathcal{S}$：状态空间
* $\mathcal{A}$：动作空间
* $P(s'|s,a)$：状态转移概率，表示在状态 $s$ 下，采取动作 $a$ 后，转移到状态 $s'$ 的概率。
* $R(s,a)$：即时奖励函数
* $\gamma$：折扣系数

**马尔可夫性**的含义是：


$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)
$$

—— `未来只依赖当前`状态和动作，`不依赖过去`历史。


### 2.3.价值函数：衡量策略优劣的指标

强化学习的核心是学习 `价值函数`（Value Function）。

两个函数：

* V 函数：只看`状态` $s$ 的好坏（Value）；
* Q 函数：看`状态 + 动作` $(s,a)$ 的好坏（Quality）。


#### 2.3.1.状态价值函数 Value

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0 = s\Big]
$$

→ 表示：在状态 $s$ 下，按策略 $\pi$ 行动，能期望获得的长期收益。

* $R_t$：此刻的`瞬时快乐`；
* $\sum_{t=0}^\infty \gamma^t R_t$：一生的`总幸福`，只是未来的快乐会打个折（ $\gamma < 1$ ）。


所以，在强化学习里我们通常还会定义一个**回报（Return）**：

$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$

→ 也就是 **从时间 (t) 开始往后看** 的总收益。
而价值函数 $V^{\pi}(s_t)$ 的定义，也可写为：

$$
V^{\pi}(s_t) = \mathbb{E}[G_t | s_t]
$$

#### 2.3.2.状态-动作价值函数（Q 函数）

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\Big[\sum_{t=0}^{\infty} \gamma^t R_t \Big| s_0=s, a_0=a\Big]
$$

→ 表示：从状态 $s$ 出发，先做动作 $a$，再按策略 $\pi$ 行动的期望收益。 Q 来源于 Quality 质量/价值。

它们之间关系为： `状态价值V`，等于 `状态-动作价值Q` 的`加权平均`。

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \cdot Q^{\pi}(s,a)
$$


### 2.4.贝尔曼方程：递归关系

这是 `RL` 理论的核心方程。

$$
Q^{\pi}(s,a) 
$$
$$
= \mathbb{E}\big[R(s,a) + \gamma V_{s'} \big]
$$
$$
= \mathbb{E}\big[R(s,a) + \gamma \sum_{a'} \pi(a'|s') \cdot Q^{\pi}(s',a') \big]
$$

→ 表示`当前动作`价值等于：

* 当前的`即时奖励`  $R(s,a)$ ；
* 加上 `下一状态的价值` 的`折扣期望`， $\gamma$ 为折扣系数
* 其中： $s'$ 是 $s$ 采取动作 $a$ 后转移到的新状态， $a'$ 是 $s'$ 可能采取的动作。

> **Tips**: **动作价值**（`Q 函数`），包含了 **及时奖励** + **下一个状态价值** (`V 函数`)的折扣期望.

而对于`最优策略` $\pi^*$，我们得到 **贝尔曼最优方程**：

$$
Q^*(s,a)
$$

$$
=  \mathbb{E}\big[R(s,a) + \gamma \max_{a'} Q^*(s',a') \big]
$$

这就是 `Q-learning` 的理论基础。


## 3.学习方法的两大路线

| 类型     | 代表算法       | 思想      | 关键点      | 
| ---------- | ----------------- | ------------------ | ------------- | 
| **价值函数**方法，Value-based  | Q-learning, SARSA | 直接逼近 $Q^*(s,a)$ ，再决定动作   | 通过 TD（时间差分）更新 | 
| **策略梯度**方法，Policy-based | REINFORCE, PPO    | 直接优化 $\pi_\theta(a \| s)$ 概率分布    | 通过`梯度上升`最大化奖励 |





### 3.1.价值函数（Value-based）

> **核心思想：** 不直接学策略，而是先学“每个状态或动作有多好”，再据此选动作。

这类算法的目标是，逼近`最优` **Q 函数**：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

也就是：每个`状态–动作`对的 **最佳长期回报**。

#### 3.1.1.代表算法

* **Q-learning**
* **Deep Q Network (DQN)**


#### 3.1.2.思路

1. 学一个 **价值函数**（比如  $Q(s,a)$ 或 $V(s)$ ）。
2. 通过与环境交互、获得奖励，不断更新估计：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
$$

3. 推理阶段，选择动作时，就选 $Q$ 值最高的那个动作：

$$
a^* = \arg\max_a Q(s,a)
$$

训练结束的标志：**找到目标价值函数（即 $Q^*$）就意味着找到了最优策略**。


> 一旦学到了正确的 $Q^*$，在每个状态选 $Q$ 最大的动作，就是最优决策、**长期回报最高**。


不过要注意两点：

1. 实际中我们不会“精确找到” $Q^*$，而是`逼近`；
2. 收敛标准通常是：Q 值`变化非常小` 或 `策略稳定`不再改进。

训练结束时，收敛结果是：

$$
Q^*(s,a)
$$

$$
\approx r + \gamma \max_{a'} Q^*(s',a')
$$



#### 3.1.3.优点

* 理论扎实，计算高效；
* 不需要显式建模策略。

#### 3.1.4.缺点

* 只能处理**离散动作空间**（连续动作会很难“max”）；
* 不适合直接学习复杂的随机策略。


### 3.2.策略（Policy-based）

> **核心思想：** 直接学习“如何行动”的`策略函数` $\pi_\theta(a|s)$ 概率分布.


#### 3.2.1.代表算法

* **REINFORCE（Policy Gradient）**
* **Proximal Policy Optimization (PPO)**
* **Actor-Critic**
* **PPO / A2C / DDPG / SAC**（现代主流）


#### 3.2.2.思路

1. 策略由参数 $\theta$ 决定（通常是一个`神经网络`）。
2. 目标是最大化期望累计奖励：

$$
J(\theta) = \mathbb{E}_{\pi\theta}\Big[\sum_t \gamma^t R_t\Big]
$$

3. 使用梯度上升（Policy Gradient）更新：

$$
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
$$


#### 3.2.3.优点

* 能处理**连续动作空间**；
* 可以`直接优化`期望`收益`；
* 更灵活（可学到随机策略）。

#### 3.2.4.缺点

* 收敛慢、方差大；
* 通常需要配合价值估计（这就引出了 **Actor-Critic**）。



### 3.3.混合路线：Actor–Critic

> 同时学策略（Actor）和价值（Critic），结合两者优点。

结构：

* **Actor**：学策略 $\pi(a|s)$，告诉你“该怎么做”；
* **Critic**：学价值函数 $V(s)$ 或 $Q(s,a)$，告诉你“做得好不好”。


## 4.Q-learning算法

Q-learning 算法，是`价值函数`类算法的典型实现。

这类算法的目标是，逼近`最优` **Q 函数**：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

也就是：每个`状态–动作`对的 **最佳长期回报**。



### 4.1.训练过程

核心更新公式：表格(`Tabular`)算法形式。

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
$$

含义：

* 观察当前`奖励` $r$；
* 看`下一个`状态的最大潜在价值 $\max_{a'} Q(s',a')$；
* 让当前的 $Q(s,a)$ 向“目标值”靠近。
* $\alpha$ 是`学习率`（步长），只是一个`外部超参`数（hyperparameter），控制每次更新时，“向目标靠近的幅度”
* $r + \gamma \max_{a'} Q(s',a')$ 是目标（`target`）
* $Q(s,a)$ 是当前估计（`prediction`）

这个「目标值」就是：

$$
y = r + \gamma \max_{a'} Q(s',a')
$$

如果每次更新都能让 $Q$ 收敛到真实的 $Q^*$，
就说明策略也收敛到了最优策略（因为最优策略就是选最大 $Q$ 的动作）。


$\alpha$ 的取值策略，一般情况下：

* 训练`初期`希望学得快： $\alpha$ 较`大`（如 0.5）
* 越到`后期`， $\alpha$ 越`小`（如 0.1 或`逐步递减`）

一种常见做法：

$$
\alpha_t = \frac{1}{1 + \text{visit}(s,a)}
$$

表示该`状态–动作`对被访问得越多，学习率越低（因为它已经稳定）。


### 4.2.理论分析

在表格形式中，差异项：跟最佳Q值的差异。

$$
\delta = r + \gamma \max_{a'} Q(s',a') - Q(s,a)
$$

叫做 **TD误差**（`Temporal Difference error` TD error）、 Bellman 误差。

这个更新，其实就是在做：

$$
Q(s,a) \leftarrow Q(s,a) - \alpha \frac{\partial}{\partial Q} \frac{1}{2}\delta^2
$$

也就是说，它等价于在**最小化一个平方误差损失函数**：

$$
L = \frac{1}{2}\big(r + \gamma \max_{a'} Q(s',a') - Q(s,a)\big)^2
$$

所以 Q-learning 虽然表面上看像是直接更新，但实际上就是梯度下降在减少预测值和目标值的误差。



### 4.3.训练 & 推理

从 `输入 -> 更新 -> 输出` 视角，关注 Q-learning 的完整过程。


简要总结：

> 1. `Tabular Q-learning` 的输入是`状态–动作–奖励–next状态`四元组
> 2. `Q 表`是算法内部逐步学习，得到的“记忆表”，
> 3. 最终`输出`（模型）就是这张 `Q 表`本身。


#### 4.3.1. 输入：不是 Q 表

在运行时，算法的每次迭代「输入」其实是**环境交互得到的一个样本**：

$$
(s, a, r, s') = (状态, 动作, 奖励, 下一个状态)
$$

也就是：

* 当前`状态` $s$ 
* 执行`动作` $a$
* 得到的即时`奖励` $r$
* 下一步`状态` $s'$


#### 4.3.2. Q 表：是“内部存储结构”

Q 表（Q-table）是算法**内部维护的一张表格**，用来记录「每个状态–动作对」的`当前价值`估计。

比如：

| 状态 `s` | 动作 `a₁` | 动作 `a₂` | 动作 `a₃` |
| ---- | ----- | ----- | ----- |
| `s₁`   | 0.5   | 0.2   | -0.1  |
| `s₂`   | 0.1   | 0.6   | 0.0   |
| `s₃`   | -0.2  | 0.3   | 0.8   |

每一格 $Q(s,a)$ 表示：在状态 $s$ 下选择动作 $a$ 的预期长期收益。


#### 4.3.3. 算法流程：学习过程

我们可以这样总结它的“输入→更新→输出”逻辑：

1.**初始化**：创建一张全 0 的 Q 表（所有`状态–动作对`初始化为 0）

2.**交互采样**：从环境得到一组样本 $(s, a, r, s')$

3.**更新表格**：根据更新规则：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)]
$$ 

4.**循环迭代**：不断重复上面的过程，直到 `Q 表`收敛。（训练结束）


#### 4.3.4. 最终输出：推理过程

最终输出的“模型”其实就是这张 `Q 表`。

在`推理`时（即不再训练），智能体只需：

$$
a^* = \arg\max_a Q(s,a)
$$

即可直接根据 Q 表选出最优动作。

### 4.4.实例

我们来用一个**最小化的例子**（只需纸笔就能算）来**完整演示一次训练过程**。


#### 4.4.1. 场景设定：一个 3 状态迷你世界

想象一个简单环境：

$$
S1 → S2 → S3(终点)
$$

规则如下：

* 状态集合： $S = \{S1, S2, S3\}$
* 动作集合： $A = \{Right, Left\}$
* 奖励规则：
  * 从 $S1$ “Right” → $S2$ ，奖励 $+0$
  * 从 $S2$ “Right” → $S3$ ，奖励 $+1$（目标）
  * 其他动作（比如往回走）奖励 $0$
* 折扣因子 $\gamma = 0.9$
* 学习率 $\alpha = 0.5$


#### 4.4.2. 初始化 Q 表

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0     | 0    |
| S2 | 0     | 0    |
| S3 | 0     | 0    |


#### 4.4.3. 第 1 回合（Episode 1）

##### Step 1

* 当前状态： $S1$
* 采取动作： $Right$
* 得到奖励： $r = 0$
* 下一状态： $S2$

更新公式：

$$
Q(S1, Right) ← Q(S1, Right) + α [r + γ \max_{a'} Q(S2, a') - Q(S1, Right)]
$$

代入：

$$
Q(S1, Right) = 0 + 0.5 [0 + 0.9×0 - 0] = 0
$$

（没有即时奖励，也没学到什么）


##### Step 2

* 当前状态： $S2$
* 动作： $Right$
* 奖励： $r = +1$
* 下一状态： $S3$

更新：

$$
Q(S2, Right) = 0 + 0.5 [1 + 0.9×\max Q(S3,·) - 0]
$$

$$
= 0.5 × 1 = 0.5
$$

更新后的 Q 表：

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0     | 0    |
| S2 | 0.5   | 0    |
| S3 | 0     | 0    |


#### 4.4.4. 第 2 回合（Episode 2）

又从 $S1$ 出发：

##### Step 1

* $S1$, 动作 $Right$
* 奖励 $r=0$，下一状态 $S2$

更新：

$$
Q(S1, Right) = 0 + 0.5 [0 + 0.9×\max Q(S2,·) - 0]
$$

$$
= 0.5 × (0.9×0.5)
= 0.225
$$

新表：

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | 0.225 | 0    |
| S2 | 0.5   | 0    |
| S3 | 0     | 0    |


#### 4.4.5. 第 3 回合（Episode 3）

再跑几次后， $Q(S1,Right)$ 继续更新：

$$
Q(S1, Right) ← 0.225 + 0.5 [0 + 0.9×0.5 - 0.225]
$$

$$
= 0.225 + 0.5(0.225 - 0.225)
= 0.225  （几乎稳定）
$$

$S2→Right$ 的值也会逐渐逼近 1。


#### 4.4.6. 收敛后的结果（直观理解）

| 状态 | Right | Left |
| -- | ----- | ---- |
| S1 | ≈0.225  | 0    |
| S2 | ≈1.0  | 0    |
| S3 | 0     | 0    |


#### 4.4.7. 模型输出

最终的 Q 表，就是你的模型。

策略：

$$
π^*(s) = \arg\max_a Q(s,a)
$$

得到：

* 在 S1 → 选择 Right
* 在 S2 → 选择 Right
* 到达 S3 → 终止

智能体学会了从 $S1 → S2 → S3$ 的最优路径。


#### 4.4.8. 总结：核心逻辑

| 步骤   | 输入            | 更新目标                            | 输出       |
| ---- | ------------- | ------------------------------- | -------- |
| 每步交互 | $(s, a, r, s′)$ | 使 $Q(s,a) \approx r + \gamma \max_{a'} Q(s',a')$ | 更新一格 Q 值 |
| 重复多轮 | 环境交互样本        | 最小化 Bellman 误差                  | 收敛到 Q*   |
| 推理阶段 | 状态 s          | 取 $\arg\max_a Q(s,a)$              | 最优动作 $a^*$  |











## 5.DQN（Deep Q-Network）

DQN（Deep Q-Network），是`Q-learning`的升级版，主要解决了Q-learning无法处理`连续动作空间`的问题。

当状态空间太大（比如游戏画面）时，不能再存一个表格来记录所有 Q 值。

于是我们用`神经网络`逼近：

$$
Q(s,a;\theta) \approx Q^*(s,a)
$$

更新目标：`损失函数`，当前值与预测值的差异

$$
L(\theta) = \big(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta)\big)^2
$$

其中， $\theta^-$ 是目标网络的参数，用于稳定训练。

* $Q(s,a;\theta)$：当前网络预测值；
* $r + \gamma \max_{a'} Q(s',a';\theta^-)$：目标 Q 值（来自固定目标网络 $\theta^-$）；
* 训练过程：最小化 $L(\theta)$，让网络逼近目标。

有了损失函数，再用 `梯度下降`，就可以更新整个神经网络。细节参考：[AI 系列：反向传播 & 梯度下降](https://ningg.top/ai-series-backward-propagation-gradient-descent-intro/)



| 算法        | 核心思路        | 特点           |
| ---------- | -------------- | ------------- |
| Q-learning | 向最优目标 Q 更新 | Off-policy，稳定 |
| DQN        | 用神经网络逼近 Q  | 能处理高维状态空间 |





## 6. REINFORCE 算法：Policy Gradient

REINFORCE 算法，是`策略梯度`类算法的典型实现。不会考虑`价值函数`，直接学 `策略函数` $\pi_\theta(a|s)$ ，即`动作的概率分布`。


REINFORCE 通过 `增加`带来`高奖励`的**动作的概率**，来学习策略。


数学上，就是：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a|s) \cdot G_t ]
$$

其中：

* $J(\theta)$：策略的`总期望奖励`
* $\pi_\theta(a|s)$：策略（参数化的动作分布）
* $G_t$：从时间步 t 开始的**累积回报**（discounted sum of rewards）

这个式子叫做 **策略梯度定理**，它说明：

我们应该沿着能“提高带来`高奖励动作`的概率”的方向去更新参数 $\theta$。



### 6.1.策略梯度定理：数学推导（简化版）


定义通用轨迹 $\tau = (s_0, a_0, r_1, s_1, \dots)$ 。

**目标函数**是：从策略 $\pi_\theta$ 生成的整个`轨迹` $\tau$ 的奖励期望 $R(\tau)$ 。

$$
J(\theta) = \mathbb{E}_{\pi_\theta}[R(\tau)]
$$


为了最大化它，我们求梯度：（*这一步章节 6.2 中独立推导*）

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\pi_\theta} [ R(\tau) \nabla_\theta \log P(\tau|\theta) ]
$$

 

然后拆分 $P(\tau|\theta)$（整条轨迹的概率）：复合函数的`链式规则`

$$
P(\tau|\theta) = \prod_t \pi_\theta(a_t|s_t) P(s_{t+1}|s_t,a_t)
$$

其中`环境转移概率`不依赖 $\theta$，于是只剩：

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\pi_\theta} [ R(\tau) \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) ]
$$

→ 这就是 REINFORCE 更新的来源！


1. 初始化参数 θ
2. **重复**：

   * 采样一条轨迹 $\tau = (s_0, a_0, r_1, s_1, \dots)$
   * 对每个时间步 $t$：

     * 计算 $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$
     * 更新参数： $$ \theta \leftarrow \theta + \alpha \cdot G_t \cdot \nabla_\theta \log \pi_\theta(a_t|s_t) $$
   * （高奖励 → 概率上升；低奖励 → 概率下降）



### 6.2.策略梯度定理：数学推导（补充）

轨迹 $\tau$ 的期望回报：

$$
J(\theta) = \mathbb{E}_{\pi_\theta}[R(\tau)]
$$


为了最大化它，我们求梯度：

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\pi_\theta} [ R(\tau) \nabla_\theta \log P(\tau|\theta) ]
$$

为什么是这个公式？其中的 $\nabla_\theta \log P(\tau|\theta)$ 怎么来的？

这一步其实是整个 REINFORCE 推导中最关键、但也最容易迷惑的地方。我们一点点拆解。


#### 6.2.1. 我们从原始目标开始：

$$
J(\theta) = \mathbb{E}_{\tau \sim P(\tau|\theta)} [R(\tau)]
$$

也就是「轨迹的期望回报」。

展开期望（连续数值求期望就是积分），就是积分形式：

$$
J(\theta) = \int P(\tau|\theta) R(\tau) d\tau
$$


#### 6.2.2. 接下来：我们要求它对 $\theta$ 的梯度

直接求导：

$$
\nabla_\theta J(\theta) = \nabla_\theta \int P(\tau|\theta) R(\tau) d\tau
$$

$R(\tau)$ 与 $\theta$ 无关，因此梯度只作用在 $P(\tau|\theta)$ 上，只作用在 `环境转移概率` 上：

$$
= \int \nabla_\theta P(\tau|\theta) R(\tau) d\tau
$$


#### 6.2.3. 接下来是关键技巧：**log-derivative trick**

> 使用log-derivative trick 是一种常用的技巧，用于在不可微分的采样过程中估计梯度。

我们把导数项改写成下面这个形式：

$$
\nabla_\theta P(\tau|\theta) = P(\tau|\theta) \nabla_\theta \log P(\tau|\theta)
$$

这是因为对任意正数 $x$，都有 $\nabla x = x \nabla \log x$。

> 简单验证： $\nabla \log x = \frac{1}{x}\nabla x  \implies \nabla x = x \nabla \log x$。




#### 6.2.4. 代回去：

$$
\nabla_\theta J(\theta) = \int P(\tau|\theta) \nabla_\theta \log P(\tau|\theta) R(\tau) d\tau
$$

然后用积分的概率形式重新写作期望：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim P(\tau|\theta)} [ R(\tau) \cdot \nabla_\theta \log P(\tau|\theta) ]
$$



#### 6.2.5. 总结

“多出来的 $\log P(\tau|\theta)$” 其实不是“凭空出现的”，
而是从 $\nabla_\theta P(\tau|\theta)$ 用 **log-derivative trick** 转换出来的。
它的好处是：可以把求导从`概率密度函数`本身（难）变成对 $\log$ `概率的梯度`（容易、数值更稳定）。

































